from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Text,
    DECIMAL,
    TIMESTAMP,
    ForeignKey,
    LargeBinary,
    ARRAY,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from sqlalchemy.dialects.postgresql import JSONB
from datetime import datetime
import os

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šè¨­å®š
DATABASE_URL = f"postgresql://{os.getenv('DB_USER', 'user')}:{os.getenv('DB_PASSWORD', 'password')}@{os.getenv('DB_HOST', 'db')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME', 'analysis')}"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()


class AnalysisSession(Base):
    __tablename__ = "analysis_sessions"

    id = Column(Integer, primary_key=True, index=True)
    session_name = Column(String(255), nullable=False)
    original_filename = Column(String(255), nullable=False)
    file_size = Column(Integer)
    upload_timestamp = Column(TIMESTAMP, default=datetime.utcnow)
    analysis_timestamp = Column(TIMESTAMP, default=datetime.utcnow)
    description = Column(Text)
    tags = Column(ARRAY(String))
    user_id = Column(String(100))

    # ğŸ†• åˆ†ææ‰‹æ³•ã®ç¨®é¡ã‚’è¿½åŠ 
    analysis_type = Column(String(50), default="correspondence", nullable=False)

    # åˆ†æçµæœã®çµ±è¨ˆæƒ…å ±
    total_inertia = Column(DECIMAL(10, 8))
    chi2_value = Column(DECIMAL(15, 4))
    degrees_of_freedom = Column(Integer)
    row_count = Column(Integer)
    column_count = Column(Integer)

    # æ¬¡å…ƒæ•°ã¨å¯„ä¸ç‡
    dimensions_count = Column(Integer, default=2)
    dimension_1_contribution = Column(DECIMAL(8, 6))
    dimension_2_contribution = Column(DECIMAL(8, 6))

    # åˆ†æè¨­å®š
    analysis_parameters = Column(JSONB)

    created_at = Column(TIMESTAMP, default=datetime.utcnow)
    updated_at = Column(TIMESTAMP, default=datetime.utcnow)

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    original_data = relationship(
        "OriginalData", back_populates="session", cascade="all, delete-orphan"
    )
    coordinates = relationship(
        "CoordinatesData", back_populates="session", cascade="all, delete-orphan"
    )
    visualizations = relationship(
        "VisualizationData", back_populates="session", cascade="all, delete-orphan"
    )
    eigenvalues = relationship(
        "EigenvalueData", back_populates="session", cascade="all, delete-orphan"
    )
    # ğŸ†• æ–°ã—ã„ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    metadata_entries = relationship(
        "AnalysisMetadata", back_populates="session", cascade="all, delete-orphan"
    )


class OriginalData(Base):
    __tablename__ = "original_data"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("analysis_sessions.id"), nullable=False)
    csv_data = Column(Text, nullable=False)
    row_names = Column(ARRAY(String))
    column_names = Column(ARRAY(String))
    data_matrix = Column(JSONB)

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    session = relationship("AnalysisSession", back_populates="original_data")


class CoordinatesData(Base):
    __tablename__ = "coordinates_data"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("analysis_sessions.id"), nullable=False)
    # ğŸ†• æ‹¡å¼µã•ã‚ŒãŸpoint_type: 'row', 'column', 'observation', 'variable', 'factor'
    point_type = Column(String(20), nullable=False)
    point_name = Column(String(255), nullable=False)
    dimension_1 = Column(DECIMAL(12, 8))
    dimension_2 = Column(DECIMAL(12, 8))
    contribution_dim1 = Column(DECIMAL(8, 6))
    contribution_dim2 = Column(DECIMAL(8, 6))
    quality_representation = Column(DECIMAL(8, 6))

    # è¿½åŠ ã®æ¬¡å…ƒ
    dimension_3 = Column(DECIMAL(12, 8))
    dimension_4 = Column(DECIMAL(12, 8))

    # ğŸ†• ä¸€æ„åˆ¶ç´„ã®åå‰ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    __table_args__ = (
        UniqueConstraint(
            "session_id",
            "point_type",
            "point_name",
            name="uq_coordinates_session_type_name",
        ),
    )

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    session = relationship("AnalysisSession", back_populates="coordinates")


class VisualizationData(Base):
    __tablename__ = "visualization_data"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("analysis_sessions.id"), nullable=False)
    # ğŸ†• æ‹¡å¼µã•ã‚ŒãŸimage_type
    image_type = Column(String(50), default="correspondence_plot")
    image_data = Column(LargeBinary, nullable=True)
    image_base64 = Column(Text)
    image_size = Column(Integer)
    width = Column(Integer)
    height = Column(Integer)
    dpi = Column(Integer, default=300)
    created_at = Column(TIMESTAMP, default=datetime.utcnow)

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    session = relationship("AnalysisSession", back_populates="visualizations")


class EigenvalueData(Base):
    __tablename__ = "eigenvalue_data"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("analysis_sessions.id"), nullable=False)
    dimension_number = Column(Integer, nullable=False)
    eigenvalue = Column(DECIMAL(12, 8))
    explained_inertia = Column(DECIMAL(8, 6))
    cumulative_inertia = Column(DECIMAL(8, 6))

    # ğŸ†• ä¸€æ„åˆ¶ç´„ã®åå‰ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    __table_args__ = (
        UniqueConstraint(
            "session_id", "dimension_number", name="uq_eigenvalue_session_dimension"
        ),
    )

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    session = relationship("AnalysisSession", back_populates="eigenvalues")


# ğŸ†• æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«: åˆ†ææ‰‹æ³•å›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
class AnalysisMetadata(Base):
    __tablename__ = "analysis_metadata"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("analysis_sessions.id"), nullable=False)
    metadata_type = Column(
        String(50), nullable=False
    )  # 'pca_loadings', 'factor_loadings', etc.
    metadata_content = Column(JSONB, nullable=False)
    created_at = Column(TIMESTAMP, default=datetime.utcnow)

    # ğŸ†• ä¸€æ„åˆ¶ç´„
    __table_args__ = (
        UniqueConstraint(
            "session_id", "metadata_type", name="uq_metadata_session_type"
        ),
    )

    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    session = relationship("AnalysisSession", back_populates="metadata_entries")


# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¾å­˜æ€§æ³¨å…¥ç”¨
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
def create_tables():
    Base.metadata.create_all(bind=engine)


# ğŸ†• åˆ†ææ‰‹æ³•åˆ¥ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def get_sessions_by_analysis_type(
    db, analysis_type: str, user_id: str = None, limit: int = 50, offset: int = 0
):
    """åˆ†ææ‰‹æ³•åˆ¥ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    query = db.query(AnalysisSession).filter(
        AnalysisSession.analysis_type == analysis_type
    )

    if user_id:
        query = query.filter(AnalysisSession.user_id == user_id)

    return (
        query.order_by(AnalysisSession.analysis_timestamp.desc())
        .offset(offset)
        .limit(limit)
        .all()
    )


def get_analysis_summary_stats(db):
    """åˆ†ææ‰‹æ³•åˆ¥ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    from sqlalchemy import func

    stats = (
        db.query(
            AnalysisSession.analysis_type,
            func.count(AnalysisSession.id).label("count"),
            func.max(AnalysisSession.analysis_timestamp).label("latest_analysis"),
        )
        .group_by(AnalysisSession.analysis_type)
        .all()
    )

    return [
        {
            "analysis_type": stat.analysis_type,
            "count": stat.count,
            "latest_analysis": (
                stat.latest_analysis.isoformat() if stat.latest_analysis else None
            ),
        }
        for stat in stats
    ]


# ğŸ†• åˆ†ææ‰‹æ³•ã®å®šæ•°
class AnalysisTypes:
    CORRESPONDENCE = "correspondence"
    PCA = "pca"
    FACTOR = "factor"
    CLUSTER = "cluster"

    @classmethod
    def all(cls):
        return [cls.CORRESPONDENCE, cls.PCA, cls.FACTOR, cls.CLUSTER]

    @classmethod
    def is_valid(cls, analysis_type: str):
        return analysis_type in cls.all()


# ğŸ†• ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®å®šæ•°
class MetadataTypes:
    # ä¸»æˆåˆ†åˆ†æç”¨
    PCA_LOADINGS = "pca_loadings"
    PCA_SCORES = "pca_scores"
    PCA_VARIANCE_EXPLAINED = "pca_variance_explained"

    # å› å­åˆ†æç”¨
    FACTOR_LOADINGS = "factor_loadings"
    FACTOR_SCORES = "factor_scores"
    FACTOR_ROTATION_MATRIX = "factor_rotation_matrix"

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æç”¨
    CLUSTER_CENTERS = "cluster_centers"
    CLUSTER_ASSIGNMENTS = "cluster_assignments"
    CLUSTER_METRICS = "cluster_metrics"

    @classmethod
    def get_types_for_analysis(cls, analysis_type: str):
        """åˆ†ææ‰‹æ³•ã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’å–å¾—"""
        type_mapping = {
            AnalysisTypes.PCA: [
                cls.PCA_LOADINGS,
                cls.PCA_SCORES,
                cls.PCA_VARIANCE_EXPLAINED,
            ],
            AnalysisTypes.FACTOR: [
                cls.FACTOR_LOADINGS,
                cls.FACTOR_SCORES,
                cls.FACTOR_ROTATION_MATRIX,
            ],
            AnalysisTypes.CLUSTER: [
                cls.CLUSTER_CENTERS,
                cls.CLUSTER_ASSIGNMENTS,
                cls.CLUSTER_METRICS,
            ],
        }
        return type_mapping.get(analysis_type, [])
