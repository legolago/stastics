from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, Query, Form
from fastapi.responses import JSONResponse, StreamingResponse
from sqlalchemy.orm import Session
import pandas as pd
import numpy as np
import io
import csv
from typing import Optional, List

from models import get_db
from analysis.timeseries import TimeSeriesAnalyzer

# LightGBMã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
try:
    import lightgbm as lgb

    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

router = APIRouter(prefix="/timeseries", tags=["timeseries"])


@router.post("/analyze")
async def analyze_timeseries(
    file: UploadFile = File(...),
    session_name: str = Form(..., description="åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³å"),
    description: Optional[str] = Form(None, description="åˆ†æã®èª¬æ˜"),
    tags: Optional[str] = Form(None, description="ã‚¿ã‚°ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"),
    user_id: str = Form("default", description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    target_column: str = Form(..., description="ç›®çš„å¤‰æ•°åˆ—å"),
    date_column: Optional[str] = Form(None, description="æ—¥ä»˜åˆ—å"),
    feature_columns: Optional[str] = Form(
        None, description="èª¬æ˜å¤‰æ•°åˆ—åï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"
    ),
    forecast_periods: int = Form(30, description="äºˆæ¸¬æœŸé–“æ•°"),
    test_size: float = Form(0.2, description="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ"),
    db: Session = Depends(get_db),
):
    """æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        print(f"=== æ™‚ç³»åˆ—åˆ†æAPIå‘¼ã³å‡ºã—é–‹å§‹ ===")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {file.filename}")
        print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³: {session_name}")
        print(f"ç›®çš„å¤‰æ•°: {target_column}, æ—¥ä»˜åˆ—: {date_column}")
        print(f"äºˆæ¸¬æœŸé–“: {forecast_periods}, ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {test_size}")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not file.filename.endswith(".csv"):
            raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        contents = await file.read()
        try:
            csv_text = contents.decode("utf-8")
        except UnicodeDecodeError:
            csv_text = contents.decode("shift_jis")

        print(f"CSVãƒ†ã‚­ã‚¹ãƒˆ:\n{csv_text[:500]}...")

        df = pd.read_csv(io.StringIO(csv_text))
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")

        if df.empty:
            raise HTTPException(status_code=400, detail="ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")

        # ç›®çš„å¤‰æ•°ã®å­˜åœ¨ç¢ºèª
        if target_column not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"ç›®çš„å¤‰æ•° '{target_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(df.columns)}",
            )

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        if target_column not in df.select_dtypes(include=[np.number]).columns:
            raise HTTPException(
                status_code=400,
                detail=f"ç›®çš„å¤‰æ•° '{target_column}' ã¯æ•°å€¤å‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™",
            )

        # æ—¥ä»˜åˆ—ã®ç¢ºèª
        if date_column and date_column not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"æ—¥ä»˜åˆ— '{date_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(df.columns)}",
            )

        # ç‰¹å¾´é‡åˆ—ã®å‡¦ç†
        feature_list = None
        if feature_columns:
            feature_list = [col.strip() for col in feature_columns.split(",")]
            missing_features = [col for col in feature_list if col not in df.columns]
            if missing_features:
                raise HTTPException(
                    status_code=400,
                    detail=f"ç‰¹å¾´é‡ {missing_features} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                )

        # æ¬ æå€¤ã®ç¢ºèª
        if df[target_column].isnull().all():
            raise HTTPException(
                status_code=400, detail=f"ç›®çš„å¤‰æ•° '{target_column}' ãŒã™ã¹ã¦æ¬ æå€¤ã§ã™"
            )

        # ã‚¿ã‚°å‡¦ç†
        tag_list = [tag.strip() for tag in tags.split(",")] if tags else []

        # åˆ†æå®Ÿè¡Œ
        analyzer = TimeSeriesAnalyzer()
        response_data = analyzer.run_full_analysis(
            df=df,
            db=db,
            session_name=session_name,
            description=description,
            tags=tag_list,
            user_id=user_id,
            file=file,
            csv_text=csv_text,
            target_column=target_column,
            date_column=date_column,
            feature_columns=feature_list,
            forecast_periods=forecast_periods,
            test_size=test_size,
        )

        print("=== æ™‚ç³»åˆ—åˆ†æAPIå‡¦ç†å®Œäº† ===")
        return JSONResponse(content=response_data)

    except HTTPException:
        raise
    except Exception as e:
        print(f"=== æ™‚ç³»åˆ—åˆ†æAPIå‡¦ç†ã‚¨ãƒ©ãƒ¼ ===")
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")

        raise HTTPException(
            status_code=500, detail=f"æ™‚ç³»åˆ—åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )


@router.get("/methods")
async def get_timeseries_methods():
    """æ™‚ç³»åˆ—åˆ†æã§åˆ©ç”¨å¯èƒ½ãªæ‰‹æ³•ä¸€è¦§ã‚’å–å¾—"""
    methods = {
        "models": [
            {
                "id": "lightgbm",
                "name": "LightGBM",
                "description": "å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«",
                "available": LIGHTGBM_AVAILABLE,
                "recommended": True,
            },
            {
                "id": "linear_regression",
                "name": "ç·šå½¢å›å¸°",
                "description": "ä»£æ›¿æ‰‹æ³•ï¼ˆLightGBMåˆ©ç”¨ä¸å¯æ™‚ï¼‰",
                "available": True,
                "recommended": False,
            },
        ],
        "features": {
            "lag_features": "ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆ1, 3, 7, 14æœŸé–“å‰ã®å€¤ï¼‰",
            "moving_averages": "ç§»å‹•å¹³å‡ç‰¹å¾´é‡ï¼ˆ3, 7, 14æœŸé–“ï¼‰",
            "time_features": "æ™‚é–“ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆæœˆã€å››åŠæœŸã€æ›œæ—¥ãªã©ï¼‰",
            "custom_features": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šç‰¹å¾´é‡",
        },
        "evaluation_metrics": [
            {"name": "RMSE", "description": "äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®"},
            {"name": "MAE", "description": "å¹³å‡çµ¶å¯¾èª¤å·®"},
            {"name": "RÂ²", "description": "æ±ºå®šä¿‚æ•°"},
            {"name": "MAPE", "description": "å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®"},
        ],
        "guidelines": {
            "minimum_samples": "æœ€ä½30ã‚µãƒ³ãƒ—ãƒ«æ¨å¥¨",
            "test_size_range": "0.1-0.3ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‰²åˆï¼‰",
            "forecast_periods": "å…ƒãƒ‡ãƒ¼ã‚¿ã®10-30%ç¨‹åº¦æ¨å¥¨",
            "required_columns": {
                "target": "äºˆæ¸¬å¯¾è±¡ã®æ•°å€¤åˆ—ï¼ˆå¿…é ˆï¼‰",
                "date": "æ—¥ä»˜åˆ—ï¼ˆæ¨å¥¨ã€è‡ªå‹•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–å¯èƒ½ï¼‰",
                "features": "èª¬æ˜å¤‰æ•°ï¼ˆä»»æ„ã€è‡ªå‹•ç‰¹å¾´é‡ç”Ÿæˆã‚‚å¯èƒ½ï¼‰",
            },
        },
        "library_status": {
            "lightgbm": LIGHTGBM_AVAILABLE,
            "sklearn_alternative": True,
            "pandas": True,
            "numpy": True,
        },
    }

    return methods


@router.get("/parameters/validate")
async def validate_timeseries_parameters(
    target_column: str = Query(..., description="ç›®çš„å¤‰æ•°åˆ—å"),
    date_column: Optional[str] = Query(None, description="æ—¥ä»˜åˆ—å"),
    feature_columns: Optional[str] = Query(None, description="ç‰¹å¾´é‡åˆ—å"),
    forecast_periods: int = Query(30, description="äºˆæ¸¬æœŸé–“æ•°"),
    test_size: float = Query(0.2, description="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‰²åˆ"),
):
    """æ™‚ç³»åˆ—åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    validation_result = {"valid": True, "warnings": [], "errors": []}

    # ç›®çš„å¤‰æ•°ã®æ¤œè¨¼
    if not target_column or not target_column.strip():
        validation_result["errors"].append("ç›®çš„å¤‰æ•°åˆ—åã¯å¿…é ˆã§ã™")
        validation_result["valid"] = False

    # äºˆæ¸¬æœŸé–“ã®æ¤œè¨¼
    if forecast_periods < 1:
        validation_result["errors"].append("äºˆæ¸¬æœŸé–“ã¯1ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        validation_result["valid"] = False
    elif forecast_periods > 365:
        validation_result["warnings"].append(
            "äºˆæ¸¬æœŸé–“ãŒ365ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
        )

    # ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã®æ¤œè¨¼
    if test_size <= 0 or test_size >= 1:
        validation_result["errors"].append(
            "ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã¯0ã‚ˆã‚Šå¤§ãã1ã‚ˆã‚Šå°ã•ã„å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        )
        validation_result["valid"] = False
    elif test_size < 0.1 or test_size > 0.4:
        validation_result["warnings"].append(
            "ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã¯0.1-0.4ã®ç¯„å›²ãŒæ¨å¥¨ã•ã‚Œã¾ã™"
        )

    # ç‰¹å¾´é‡åˆ—ã®æ¤œè¨¼
    if feature_columns:
        feature_list = [col.strip() for col in feature_columns.split(",")]
        if len(feature_list) > 50:
            validation_result["warnings"].append(
                "ç‰¹å¾´é‡ãŒå¤šã™ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘æ€§ãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            )

    return validation_result


@router.get("/interpretation")
async def get_interpretation_guide():
    """æ™‚ç³»åˆ—åˆ†æçµæœã®è§£é‡ˆã‚¬ã‚¤ãƒ‰ã‚’å–å¾—"""
    return {
        "model_metrics": {
            "rmse": {
                "description": "äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·® - äºˆæ¸¬èª¤å·®ã®å¤§ãã•",
                "interpretation": "å€¤ãŒå°ã•ã„ã»ã©è‰¯å¥½ã€‚ç›®çš„å¤‰æ•°ã®æ¨™æº–åå·®ã¨æ¯”è¼ƒ",
            },
            "mae": {
                "description": "å¹³å‡çµ¶å¯¾èª¤å·® - å¹³å‡çš„ãªäºˆæ¸¬èª¤å·®",
                "interpretation": "å®Ÿéš›ã®å€¤ã®å˜ä½ã§è§£é‡ˆã—ã‚„ã™ã„èª¤å·®æŒ‡æ¨™",
            },
            "r2_score": {
                "description": "æ±ºå®šä¿‚æ•° - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜åŠ›",
                "ranges": {
                    "0.9ä»¥ä¸Š": "éå¸¸ã«è‰¯å¥½",
                    "0.7-0.9": "è‰¯å¥½",
                    "0.5-0.7": "ä¸­ç¨‹åº¦",
                    "0.5æœªæº€": "è¦æ”¹å–„",
                },
            },
            "mape": {
                "description": "å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®",
                "ranges": {
                    "5%æœªæº€": "éå¸¸ã«è‰¯å¥½",
                    "5-10%": "è‰¯å¥½",
                    "10-25%": "ä¸­ç¨‹åº¦",
                    "25%ä»¥ä¸Š": "è¦æ”¹å–„",
                },
            },
        },
        "feature_importance": {
            "description": "å„ç‰¹å¾´é‡ã®äºˆæ¸¬ã¸ã®å¯„ä¸åº¦",
            "interpretation": {
                "lag_features": "éå»ã®å€¤ã®å½±éŸ¿åº¦",
                "moving_averages": "ãƒˆãƒ¬ãƒ³ãƒ‰ã®å½±éŸ¿åº¦",
                "time_features": "å­£ç¯€æ€§ãƒ»å‘¨æœŸæ€§ã®å½±éŸ¿åº¦",
                "external_features": "å¤–éƒ¨è¦å› ã®å½±éŸ¿åº¦",
            },
        },
        "residual_analysis": {
            "description": "æ®‹å·®ï¼ˆå®Ÿæ¸¬å€¤ - äºˆæ¸¬å€¤ï¼‰ã®åˆ†æ",
            "good_signs": [
                "æ®‹å·®ãŒ0å‘¨è¾ºã«ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å¸ƒ",
                "æ®‹å·®ã«æ˜ç¢ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãªã„",
                "æ®‹å·®ã®åˆ†æ•£ãŒä¸€å®š",
            ],
            "warning_signs": [
                "æ®‹å·®ã«å‘¨æœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³",
                "æ®‹å·®ã®åˆ†æ•£ãŒæ™‚é–“ã§å¤‰åŒ–",
                "å¤–ã‚Œå€¤ãŒå¤šæ•°å­˜åœ¨",
            ],
        },
        "forecast_reliability": {
            "description": "äºˆæ¸¬ã®ä¿¡é ¼æ€§è©•ä¾¡",
            "factors": [
                "ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æŒ‡æ¨™ï¼ˆRÂ²ã€RMSEç­‰ï¼‰",
                "äºˆæ¸¬æœŸé–“ã®é•·ã•ï¼ˆçŸ­æœŸã»ã©ä¿¡é ¼æ€§é«˜ï¼‰",
                "ãƒ‡ãƒ¼ã‚¿ã®å®‰å®šæ€§ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å­£ç¯€æ€§ï¼‰",
                "å¤–éƒ¨è¦å› ã®å¤‰åŒ–å¯èƒ½æ€§",
            ],
        },
    }


@router.get("/sessions/{session_id}")
async def get_timeseries_session_detail(
    session_id: int,
    db: Session = Depends(get_db),
):
    """æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—"""
    try:
        print(f"ğŸ“Š æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

        # TimeSeriesAnalyzerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        analyzer = TimeSeriesAnalyzer()

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—
        session_detail = analyzer.get_session_detail(db, session_id)

        print(f"ğŸ” å–å¾—ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°: {session_detail.get('success', False)}")

        if not session_detail or not session_detail.get("success"):
            error_msg = (
                session_detail.get("error", f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                if session_detail
                else f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
            raise HTTPException(status_code=404, detail=error_msg)

        return JSONResponse(content=session_detail)

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")

        raise HTTPException(
            status_code=500,
            detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


@router.get("/download/{session_id}/details")
async def download_timeseries_details(session_id: int, db: Session = Depends(get_db)):
    """æ™‚ç³»åˆ—åˆ†æçµæœè©³ç´°ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from models import (
            AnalysisSession,
            AnalysisMetadata,
            CoordinatesData,
        )

        print(f"Starting timeseries details download for session: {session_id}")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )
        if not session:
            raise HTTPException(status_code=404, detail="ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if session.analysis_type != "timeseries":
            raise HTTPException(
                status_code=400, detail="æ™‚ç³»åˆ—åˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
            )

        print(f"Session found: {session.session_name}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        metadata_entries = (
            db.query(AnalysisMetadata)
            .filter(AnalysisMetadata.session_id == session_id)
            .all()
        )

        # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        coordinates_data = (
            db.query(CoordinatesData)
            .filter(CoordinatesData.session_id == session_id)
            .all()
        )

        print(
            f"Found {len(metadata_entries)} metadata entries, {len(coordinates_data)} coordinates"
        )

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        output = io.StringIO()
        writer = csv.writer(output)

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
        writer.writerow(["ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±"])
        writer.writerow(["é …ç›®", "å€¤"])
        writer.writerow(["ã‚»ãƒƒã‚·ãƒ§ãƒ³å", session.session_name])
        writer.writerow(["ãƒ•ã‚¡ã‚¤ãƒ«å", session.original_filename])
        writer.writerow(["åˆ†ææ‰‹æ³•", "æ™‚ç³»åˆ—åˆ†æ"])
        writer.writerow(
            ["åˆ†ææ—¥æ™‚", session.analysis_timestamp.strftime("%Y-%m-%d %H:%M:%S")]
        )
        writer.writerow(["ã‚µãƒ³ãƒ—ãƒ«æ•°", session.row_count])
        writer.writerow(["å¤‰æ•°æ•°", session.column_count])
        writer.writerow([])

        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™
        model_metrics = {}
        feature_importance = []
        for meta in metadata_entries:
            if meta.metadata_type == "timeseries_metrics":
                content = meta.metadata_content
                model_metrics = content.get("metrics", {})
                feature_importance = content.get("feature_importance", [])
                break

        if model_metrics:
            writer.writerow(["ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™"])
            writer.writerow(["æŒ‡æ¨™", "è¨“ç·´ãƒ‡ãƒ¼ã‚¿", "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"])

            train_metrics = model_metrics.get("train", {})
            test_metrics = model_metrics.get("test", {})

            for metric in ["rmse", "mae", "r2", "mape"]:
                train_val = train_metrics.get(metric, 0)
                test_val = test_metrics.get(metric, 0)
                writer.writerow([metric.upper(), f"{train_val:.6f}", f"{test_val:.6f}"])
            writer.writerow([])

        # ç‰¹å¾´é‡é‡è¦åº¦
        if feature_importance:
            writer.writerow(["ç‰¹å¾´é‡é‡è¦åº¦"])
            writer.writerow(["ç‰¹å¾´é‡å", "é‡è¦åº¦"])
            for feature_name, importance in feature_importance[:20]:  # ä¸Šä½20å€‹
                writer.writerow([feature_name, f"{importance:.6f}"])
            writer.writerow([])

        # å®Ÿæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿
        actual_coords = [
            coord for coord in coordinates_data if coord.point_type == "train"
        ]
        if actual_coords:
            writer.writerow(["å®Ÿæ¸¬å€¤ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰"])
            writer.writerow(["ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—", "å€¤"])
            for coord in sorted(actual_coords, key=lambda x: x.dimension_4 or 0):
                writer.writerow(
                    [
                        coord.point_name,
                        f"{coord.dimension_1:.6f}" if coord.dimension_1 else "0.000000",
                    ]
                )
            writer.writerow([])

        # äºˆæ¸¬å€¤ã¨æ®‹å·®
        pred_coords = [
            coord for coord in coordinates_data if coord.point_type == "test"
        ]
        if pred_coords:
            writer.writerow(["äºˆæ¸¬çµæœï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰"])
            writer.writerow(["ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—", "äºˆæ¸¬å€¤", "å®Ÿæ¸¬å€¤", "æ®‹å·®"])
            for coord in sorted(pred_coords, key=lambda x: x.dimension_4 or 0):
                writer.writerow(
                    [
                        coord.point_name,
                        f"{coord.dimension_1:.6f}" if coord.dimension_1 else "0.000000",
                        f"{coord.dimension_3:.6f}" if coord.dimension_3 else "0.000000",
                        f"{coord.dimension_2:.6f}" if coord.dimension_2 else "0.000000",
                    ]
                )
            writer.writerow([])

        # æœªæ¥äºˆæ¸¬å€¤
        forecast_coords = [
            coord for coord in coordinates_data if coord.point_type == "variable"
        ]
        if forecast_coords:
            writer.writerow(["æœªæ¥äºˆæ¸¬"])
            writer.writerow(["ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—", "äºˆæ¸¬å€¤"])
            for coord in sorted(forecast_coords, key=lambda x: x.dimension_4 or 0):
                writer.writerow(
                    [
                        coord.point_name,
                        f"{coord.dimension_1:.6f}" if coord.dimension_1 else "0.000000",
                    ]
                )
            writer.writerow([])

        # CSVå†…å®¹ã‚’å–å¾—
        csv_content = output.getvalue()
        output.close()

        print(f"Generated CSV content length: {len(csv_content)} characters")

        # ãƒ•ã‚¡ã‚¤ãƒ«åè¨­å®š
        filename = f"timeseries_details_{session_id}.csv"

        # Responseã‚’ä½œæˆ
        return StreamingResponse(
            io.StringIO(csv_content),
            media_type="text/csv",
            headers={"Content-Disposition": f'attachment; filename="{filename}"'},
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"è©³ç´°CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail=f"è©³ç´°CSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )


@router.get("/download/{session_id}/predictions")
async def download_timeseries_predictions(
    session_id: int, db: Session = Depends(get_db)
):
    """äºˆæ¸¬çµæœã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from models import AnalysisSession, CoordinatesData

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )
        if not session:
            raise HTTPException(status_code=404, detail="ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if session.analysis_type != "timeseries":
            raise HTTPException(
                status_code=400, detail="æ™‚ç³»åˆ—åˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
            )

        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        predictions = (
            db.query(CoordinatesData)
            .filter(
                CoordinatesData.session_id == session_id,
                CoordinatesData.point_type == "test",
            )
            .order_by(CoordinatesData.dimension_4)
            .all()
        )

        if not predictions:
            raise HTTPException(status_code=404, detail="äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        output = io.StringIO()
        writer = csv.writer(output)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        writer.writerow(["timestamp", "predicted_value", "actual_value", "residual"])

        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for pred in predictions:
            writer.writerow(
                [
                    pred.point_name,
                    pred.dimension_1 if pred.dimension_1 is not None else 0.0,
                    pred.dimension_3 if pred.dimension_3 is not None else 0.0,
                    pred.dimension_2 if pred.dimension_2 is not None else 0.0,
                ]
            )

        output.seek(0)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        return StreamingResponse(
            io.StringIO(output.getvalue()),
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename=timeseries_predictions_{session_id}.csv"
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"äºˆæ¸¬CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"äºˆæ¸¬CSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


@router.get("/download/{session_id}/forecast")
async def download_timeseries_forecast(session_id: int, db: Session = Depends(get_db)):
    """æœªæ¥äºˆæ¸¬ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from models import AnalysisSession, CoordinatesData

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )
        if not session:
            raise HTTPException(status_code=404, detail="ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if session.analysis_type != "timeseries":
            raise HTTPException(
                status_code=400, detail="æ™‚ç³»åˆ—åˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
            )

        # æœªæ¥äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        forecasts = (
            db.query(CoordinatesData)
            .filter(
                CoordinatesData.session_id == session_id,
                CoordinatesData.point_type == "variable",
            )
            .order_by(CoordinatesData.dimension_4)
            .all()
        )

        if not forecasts:
            raise HTTPException(
                status_code=404, detail="æœªæ¥äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        output = io.StringIO()
        writer = csv.writer(output)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        writer.writerow(["timestamp", "predicted_value"])

        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for forecast in forecasts:
            writer.writerow(
                [
                    forecast.point_name,
                    forecast.dimension_1 if forecast.dimension_1 is not None else 0.0,
                ]
            )

        output.seek(0)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        return StreamingResponse(
            io.StringIO(output.getvalue()),
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename=timeseries_forecast_{session_id}.csv"
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"æœªæ¥äºˆæ¸¬CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"æœªæ¥äºˆæ¸¬CSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


@router.get("/download/{session_id}/feature_importance")
async def download_feature_importance(session_id: int, db: Session = Depends(get_db)):
    """ç‰¹å¾´é‡é‡è¦åº¦ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from models import AnalysisSession, AnalysisMetadata

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )
        if not session:
            raise HTTPException(status_code=404, detail="ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if session.analysis_type != "timeseries":
            raise HTTPException(
                status_code=400, detail="æ™‚ç³»åˆ—åˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
            )

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
        metadata = (
            db.query(AnalysisMetadata)
            .filter(
                AnalysisMetadata.session_id == session_id,
                AnalysisMetadata.metadata_type == "timeseries_metrics",
            )
            .first()
        )

        if not metadata:
            raise HTTPException(status_code=404, detail="ç‰¹å¾´é‡é‡è¦åº¦ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        feature_importance = metadata.metadata_content.get("feature_importance", [])
        if not feature_importance:
            raise HTTPException(status_code=404, detail="ç‰¹å¾´é‡é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        output = io.StringIO()
        writer = csv.writer(output)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        writer.writerow(["feature_name", "importance"])

        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for feature_name, importance in feature_importance:
            writer.writerow([feature_name, importance])

        output.seek(0)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        return StreamingResponse(
            io.StringIO(output.getvalue()),
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename=feature_importance_{session_id}.csv"
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"ç‰¹å¾´é‡é‡è¦åº¦CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ç‰¹å¾´é‡é‡è¦åº¦CSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


# timeseries.pyï¼ˆFastAPIãƒ«ãƒ¼ã‚¿ãƒ¼ï¼‰ã«è¿½åŠ 


@router.get("/sessions")
async def get_timeseries_sessions(
    user_id: str = Query("default", description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    limit: int = Query(50, description="å–å¾—ä»¶æ•°"),
    offset: int = Query(0, description="ã‚ªãƒ•ã‚»ãƒƒãƒˆ"),
    search: Optional[str] = Query(None, description="æ¤œç´¢ã‚¯ã‚¨ãƒª"),
    db: Session = Depends(get_db),
):
    """æ™‚ç³»åˆ—åˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—"""
    try:
        from models import AnalysisSession
        from sqlalchemy import or_, and_, text

        print(f"ğŸ” æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—é–‹å§‹")
        print(
            f"Parameters: user_id={user_id}, limit={limit}, offset={offset}, search={search}"
        )

        # ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªï¼ˆæ™‚ç³»åˆ—åˆ†æã®ã¿ï¼‰
        query = db.query(AnalysisSession).filter(
            and_(
                AnalysisSession.user_id == user_id,
                AnalysisSession.analysis_type == "timeseries",
            )
        )

        # æ¤œç´¢æ¡ä»¶ã‚’è¿½åŠ 
        if search:
            search_filter = or_(
                AnalysisSession.session_name.ilike(f"%{search}%"),
                AnalysisSession.original_filename.ilike(f"%{search}%"),
                AnalysisSession.description.ilike(f"%{search}%"),
            )
            query = query.filter(search_filter)

        # çµæœå–å¾—
        sessions = (
            query.order_by(AnalysisSession.analysis_timestamp.desc())
            .offset(offset)
            .limit(limit)
            .all()
        )

        print(f"ğŸ“Š å–å¾—ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(sessions)}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«å¤‰æ›
        sessions_data = []
        for session in sessions:
            # ã‚¿ã‚°æƒ…å ±ã‚’å–å¾—ï¼ˆARRAYå‹ã®tagsã‚«ãƒ©ãƒ ã‚’å„ªå…ˆä½¿ç”¨ï¼‰
            tags = []
            try:
                # analysis_sessionsãƒ†ãƒ¼ãƒ–ãƒ«ã®tagsã‚«ãƒ©ãƒ ï¼ˆARRAYå‹ï¼‰ã‚’ä½¿ç”¨
                if hasattr(session, "tags") and session.tags:
                    tags = list(session.tags)  # PostgreSQL ARRAYã‚’Pythonãƒªã‚¹ãƒˆã«å¤‰æ›

                # ã‚‚ã—ARRAYå‹ãŒç©ºãªã‚‰ã€session_tagsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚‚ç¢ºèª
                if not tags:
                    tag_result = db.execute(
                        text(
                            "SELECT tag FROM session_tags WHERE session_id = :session_id"
                        ),
                        {"session_id": session.id},
                    ).fetchall()
                    tags = [row[0] for row in tag_result if row[0]]

            except Exception as tag_error:
                print(
                    f"âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³{session.id}ã®ã‚¿ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {tag_error}"
                )
                tags = []

            session_data = {
                "session_id": session.id,
                "session_name": session.session_name,
                "description": getattr(session, "description", "") or "",
                "filename": session.original_filename,
                "analysis_type": session.analysis_type,
                "analysis_timestamp": (
                    session.analysis_timestamp.isoformat()
                    if hasattr(session, "analysis_timestamp")
                    and session.analysis_timestamp
                    else None
                ),
                "user_id": session.user_id,
                "row_count": getattr(session, "row_count", 0) or 0,
                "column_count": getattr(session, "column_count", 0) or 0,
                "tags": tags,
                # æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                "dimension_1_contribution": float(
                    getattr(session, "dimension_1_contribution", 0) or 0
                ),
                "dimensions_count": getattr(session, "dimensions_count", 1) or 1,
            }
            sessions_data.append(session_data)

        response_data = {
            "success": True,
            "data": sessions_data,
            "total": len(sessions_data),
            "offset": offset,
            "limit": limit,
        }

        print(f"âœ… æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—å®Œäº†: {len(sessions_data)}ä»¶")
        return response_data

    except Exception as e:
        print(f"âŒ æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")

        return {"success": False, "error": str(e), "data": []}
