from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, Query
from fastapi.responses import JSONResponse, StreamingResponse, Response
from sqlalchemy.orm import Session
import pandas as pd
import numpy as np
import io
import csv
from typing import Optional, List
from datetime import datetime

from models import get_db
from analysis.rfm import RFMAnalysisAnalyzer

router = APIRouter(prefix="/rfm", tags=["rfm"])


@router.post("/analyze")
async def analyze_rfm(
    file: UploadFile = File(...),
    session_name: str = Query(..., description="åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³å"),
    description: Optional[str] = Query(None, description="åˆ†æã®èª¬æ˜"),
    tags: Optional[str] = Query(None, description="ã‚¿ã‚°ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"),
    user_id: str = Query("default", description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    customer_id_col: str = Query("id", description="é¡§å®¢IDåˆ—å"),
    date_col: str = Query("date", description="æ—¥ä»˜åˆ—å"),
    amount_col: str = Query("price", description="é‡‘é¡åˆ—å"),
    analysis_date: Optional[str] = Query(None, description="åˆ†æåŸºæº–æ—¥ (YYYY-MM-DD)"),
    rfm_divisions: int = Query(3, description="RFMåˆ†å‰²æ•°", ge=3, le=5),
    use_monetary_4_divisions: bool = Query(False, description="Monetaryã‚’4åˆ†å‰²ã™ã‚‹ã‹"),
    db: Session = Depends(get_db),
):
    """RFMåˆ†æã‚’å®Ÿè¡Œ"""
    try:
        print(f"=== RFMåˆ†æAPIå‘¼ã³å‡ºã—é–‹å§‹ ===")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {file.filename}")
        print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³: {session_name}")
        print(f"é¡§å®¢IDåˆ—: {customer_id_col}, æ—¥ä»˜åˆ—: {date_col}, é‡‘é¡åˆ—: {amount_col}")
        print(f"åˆ†æåŸºæº–æ—¥: {analysis_date}, RFMåˆ†å‰²æ•°: {rfm_divisions}")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not file.filename.endswith(".csv"):
            raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        contents = await file.read()
        try:
            csv_text = contents.decode("utf-8")
        except UnicodeDecodeError:
            try:
                csv_text = contents.decode("shift_jis")
            except UnicodeDecodeError:
                csv_text = contents.decode("iso-8859-1")

        print(f"CSVãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:\n{csv_text[:500]}")

        # CSVã‚’DataFrameã«å¤‰æ›
        df = pd.read_csv(io.StringIO(csv_text))
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ :\n{df.head()}")
        print(f"åˆ—å: {list(df.columns)}")

        if df.empty:
            raise HTTPException(status_code=400, detail="ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")

        # å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèª
        required_cols = [customer_id_col, date_col, amount_col]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise HTTPException(
                status_code=400,
                detail=f"å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_cols}. åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(df.columns)}",
            )

        # åˆ†æåŸºæº–æ—¥ã®æ¤œè¨¼
        if analysis_date:
            try:
                datetime.strptime(analysis_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail="åˆ†æåŸºæº–æ—¥ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYY-MM-DDå½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
                )

        # ã‚¿ã‚°å‡¦ç†
        tag_list = [tag.strip() for tag in tags.split(",")] if tags else []

        # åˆ†æå®Ÿè¡Œ
        analyzer = RFMAnalysisAnalyzer()
        response_data = analyzer.run_full_analysis(
            df=df,
            db=db,
            session_name=session_name,
            description=description,
            tags=tag_list,
            user_id=user_id,
            file=file,
            csv_text=csv_text,
            customer_id_col=customer_id_col,
            date_col=date_col,
            amount_col=amount_col,
            analysis_date=analysis_date,
            rfm_divisions=rfm_divisions,
            use_monetary_4_divisions=use_monetary_4_divisions,
        )

        print("=== RFMåˆ†æAPIå‡¦ç†å®Œäº† ===")
        return JSONResponse(content=response_data)

    except HTTPException:
        raise
    except Exception as e:
        print(f"=== RFMåˆ†æAPIå‡¦ç†ã‚¨ãƒ©ãƒ¼ ===")
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")

        raise HTTPException(
            status_code=500, detail=f"RFMåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )


@router.get("/methods")
async def get_rfm_methods():
    """RFMåˆ†æã§åˆ©ç”¨å¯èƒ½ãªæ‰‹æ³•ä¸€è¦§ã‚’å–å¾—"""
    methods = {
        "rfm_divisions": [
            {
                "value": 3,
                "name": "3åˆ†å‰²",
                "description": "ä½ãƒ»ä¸­ãƒ»é«˜ã®3æ®µéšã§RFMå€¤ã‚’åˆ†å‰²",
            },
            {
                "value": 4,
                "name": "4åˆ†å‰²",
                "description": "ã‚ˆã‚Šç´°ã‹ã„4æ®µéšã§RFMå€¤ã‚’åˆ†å‰²",
            },
            {
                "value": 5,
                "name": "5åˆ†å‰²",
                "description": "æœ€ã‚‚ç´°ã‹ã„5æ®µéšã§RFMå€¤ã‚’åˆ†å‰²",
            },
        ],
        "segment_definitions": {
            "VIPé¡§å®¢": "æœ€è¿‘è³¼å…¥ã—ã€é »ç¹ã«è³¼å…¥ã—ã€é«˜é¡ãªé¡§å®¢",
            "å„ªè‰¯é¡§å®¢": "æœ€è¿‘è³¼å…¥ã—ã€é©åº¦ã«è³¼å…¥ã—ã€ã‚ã‚‹ç¨‹åº¦ã®é‡‘é¡ã‚’ä½¿ã†é¡§å®¢",
            "æ–°è¦é¡§å®¢": "æœ€è¿‘è³¼å…¥ã—ãŸãŒã€ã¾ã é »åº¦ã‚„é‡‘é¡ãŒå°‘ãªã„é¡§å®¢",
            "è¦æ³¨æ„ãƒ˜ãƒ“ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼": "è³¼å…¥é »åº¦ãƒ»é‡‘é¡ã¯é«˜ã„ãŒã€æœ€è¿‘è³¼å…¥ã—ã¦ã„ãªã„é¡§å®¢",
            "å®‰å®šé¡§å®¢": "å®šæœŸçš„ã«è³¼å…¥ã—ã¦ã„ã‚‹é¡§å®¢",
            "è¦‹è¾¼ã¿é¡§å®¢": "ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãŒã‚ã‚‹é¡§å®¢",
            "é›¢è„±ã—ãŸå„ªè‰¯é¡§å®¢": "éå»ã¯å„ªè‰¯ã ã£ãŸãŒã€æœ€è¿‘è³¼å…¥ã—ã¦ã„ãªã„é¡§å®¢",
            "é›¢è„±ã—ã¤ã¤ã‚ã‚‹é¡§å®¢": "è³¼å…¥ãŒæ¸›ã£ã¦ã„ã‚‹é¡§å®¢",
            "é›¢è„±é¡§å®¢": "è³¼å…¥ã—ãªããªã£ãŸé¡§å®¢",
        },
        "required_columns": {
            "customer_id": "é¡§å®¢ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®åˆ—ï¼ˆä¾‹: id, customer_idï¼‰",
            "date": "è³¼å…¥æ—¥ä»˜ã®åˆ—ï¼ˆä¾‹: date, purchase_dateï¼‰",
            "amount": "è³¼å…¥é‡‘é¡ã®åˆ—ï¼ˆä¾‹: price, amount, totalï¼‰",
        },
        "guidelines": {
            "minimum_data_period": "æœ€ä½3ãƒ¶æœˆé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¨å¥¨",
            "minimum_customers": "æœ€ä½50äººã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’æ¨å¥¨",
            "date_format": "æ—¥ä»˜ã¯ YYYY-MM-DD, YYYY/MM/DD ãªã©ã®å½¢å¼ã«å¯¾å¿œ",
            "analysis_date": "åˆ†æåŸºæº–æ—¥ã‚’æŒ‡å®šã—ãªã„å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚æ—¥ã®ç¿Œæ—¥ã‚’ä½¿ç”¨",
        },
    }

    return methods


@router.get("/parameters/validate")
async def validate_rfm_parameters(
    customer_id_col: str = Query("id", description="é¡§å®¢IDåˆ—å"),
    date_col: str = Query("date", description="æ—¥ä»˜åˆ—å"),
    amount_col: str = Query("price", description="é‡‘é¡åˆ—å"),
    analysis_date: Optional[str] = Query(None, description="åˆ†æåŸºæº–æ—¥"),
    rfm_divisions: int = Query(3, description="RFMåˆ†å‰²æ•°"),
):
    """RFMåˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    validation_result = {"valid": True, "warnings": [], "errors": []}

    # åˆ—åã®æ¤œè¨¼
    if not customer_id_col.strip():
        validation_result["errors"].append("é¡§å®¢IDåˆ—åã¯å¿…é ˆã§ã™")
        validation_result["valid"] = False

    if not date_col.strip():
        validation_result["errors"].append("æ—¥ä»˜åˆ—åã¯å¿…é ˆã§ã™")
        validation_result["valid"] = False

    if not amount_col.strip():
        validation_result["errors"].append("é‡‘é¡åˆ—åã¯å¿…é ˆã§ã™")
        validation_result["valid"] = False

    # åˆ†æåŸºæº–æ—¥ã®æ¤œè¨¼
    if analysis_date:
        try:
            datetime.strptime(analysis_date, "%Y-%m-%d")
        except ValueError:
            validation_result["errors"].append(
                "åˆ†æåŸºæº–æ—¥ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYY-MM-DDå½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
            )
            validation_result["valid"] = False

    # RFMåˆ†å‰²æ•°ã®æ¤œè¨¼
    if rfm_divisions < 3:
        validation_result["errors"].append("RFMåˆ†å‰²æ•°ã¯3ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        validation_result["valid"] = False
    elif rfm_divisions > 5:
        validation_result["warnings"].append(
            "RFMåˆ†å‰²æ•°ãŒå¤šã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆæ¨å¥¨: 3-5ï¼‰"
        )

    return validation_result


@router.get("/interpretation")
async def get_interpretation_guide():
    """RFMåˆ†æçµæœã®è§£é‡ˆã‚¬ã‚¤ãƒ‰ã‚’å–å¾—"""
    return {
        "rfm_metrics": {
            "recency": {
                "description": "æœ€çµ‚è³¼å…¥ã‹ã‚‰ã®çµŒéæ—¥æ•°",
                "interpretation": "å€¤ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼ˆæœ€è¿‘è³¼å…¥ã—ã¦ã„ã‚‹ï¼‰",
            },
            "frequency": {
                "description": "è³¼å…¥å›æ•°",
                "interpretation": "å€¤ãŒå¤§ãã„ã»ã©è‰¯ã„ï¼ˆé »ç¹ã«è³¼å…¥ã—ã¦ã„ã‚‹ï¼‰",
            },
            "monetary": {
                "description": "è³¼å…¥é‡‘é¡åˆè¨ˆ",
                "interpretation": "å€¤ãŒå¤§ãã„ã»ã©è‰¯ã„ï¼ˆå¤šãè³¼å…¥ã—ã¦ã„ã‚‹ï¼‰",
            },
        },
        "segments": {
            "VIPé¡§å®¢": {
                "characteristics": "R=é«˜, F=é«˜, M=é«˜",
                "action": "é–¢ä¿‚ç¶­æŒã€ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚µãƒ¼ãƒ“ã‚¹æä¾›",
            },
            "å„ªè‰¯é¡§å®¢": {
                "characteristics": "R=é«˜, F=ä¸­-é«˜, M=ä¸­-é«˜",
                "action": "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ã€ã‚¯ãƒ­ã‚¹ã‚»ãƒ«",
            },
            "æ–°è¦é¡§å®¢": {
                "characteristics": "R=é«˜, F=ä½, M=ä½",
                "action": "ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¼·åŒ–",
            },
            "è¦æ³¨æ„ãƒ˜ãƒ“ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼": {
                "characteristics": "R=ä½, F=é«˜, M=é«˜",
                "action": "ç·Šæ€¥ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ–½ç­–",
            },
            "å®‰å®šé¡§å®¢": {
                "characteristics": "R=ä¸­, F=ä¸­, M=ä¸­",
                "action": "å®šæœŸçš„ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
            },
            "é›¢è„±é¡§å®¢": {
                "characteristics": "R=ä½, F=ä½, M=ä½",
                "action": "ã‚¦ã‚£ãƒ³ãƒãƒƒã‚¯æ–½ç­–",
            },
        },
        "score_interpretation": {
            "high_scores": "9ä»¥ä¸Š: æœ€å„ªè‰¯é¡§å®¢",
            "medium_scores": "6-8: ä¸€èˆ¬é¡§å®¢",
            "low_scores": "3-5: è¦æ”¹å–„é¡§å®¢",
        },
    }


@router.get("/sessions/{session_id}")
async def get_rfm_session_detail(
    session_id: int,
    db: Session = Depends(get_db),
):
    """RFMåˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—"""
    try:
        print(f"ğŸ“Š RFMåˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

        # RFMAnalysisAnalyzerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        analyzer = RFMAnalysisAnalyzer()

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—
        session_detail = await analyzer.get_session_detail_async(session_id, db)

        print(f"ğŸ” å–å¾—ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°: {session_detail.get('success', False)}")

        if not session_detail or not session_detail.get("success"):
            error_msg = (
                session_detail.get("error", f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                if session_detail
                else f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
            raise HTTPException(status_code=404, detail=error_msg)

        return JSONResponse(content=session_detail)

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ RFMåˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")

        raise HTTPException(
            status_code=500,
            detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


@router.get("/download/{session_id}/details")
async def download_rfm_details(session_id: int, db: Session = Depends(get_db)):
    """RFMåˆ†æçµæœè©³ç´°ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from models import AnalysisSession, AnalysisMetadata, CoordinatesData

        print(f"Starting RFM details download for session: {session_id}")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )
        if not session:
            raise HTTPException(status_code=404, detail="ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if session.analysis_type != "rfm":
            raise HTTPException(
                status_code=400, detail="RFMåˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
            )

        print(f"Session found: {session.session_name}")

        # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        customer_data = (
            db.query(CoordinatesData)
            .filter(
                CoordinatesData.session_id == session_id,
                CoordinatesData.point_type == "customer",
            )
            .all()
        )

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        metadata_entries = (
            db.query(AnalysisMetadata)
            .filter(AnalysisMetadata.session_id == session_id)
            .all()
        )

        print(
            f"Found {len(customer_data)} customers, {len(metadata_entries)} metadata entries"
        )

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        output = io.StringIO()
        writer = csv.writer(output)

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
        writer.writerow(["ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±"])
        writer.writerow(["é …ç›®", "å€¤"])
        writer.writerow(["ã‚»ãƒƒã‚·ãƒ§ãƒ³å", session.session_name])
        writer.writerow(["ãƒ•ã‚¡ã‚¤ãƒ«å", session.original_filename])
        writer.writerow(["åˆ†ææ‰‹æ³•", "RFMåˆ†æ"])
        writer.writerow(
            ["åˆ†ææ—¥æ™‚", session.analysis_timestamp.strftime("%Y-%m-%d %H:%M:%S")]
        )
        writer.writerow(["ãƒ‡ãƒ¼ã‚¿è¡Œæ•°", session.row_count])
        writer.writerow(["ç·é¡§å®¢æ•°", len(customer_data)])
        writer.writerow([])

        # RFMçµ±è¨ˆæƒ…å ±
        rfm_stats = None
        for meta in metadata_entries:
            if meta.metadata_type == "rfm_statistics":
                rfm_stats = meta.metadata_content
                break

        if rfm_stats:
            writer.writerow(["RFMçµ±è¨ˆæƒ…å ±"])

            # Recencyçµ±è¨ˆ
            writer.writerow(["Recencyçµ±è¨ˆ"])
            writer.writerow(["é …ç›®", "å€¤"])
            r_stats = rfm_stats.get("rfm_stats", {}).get("recency", {})
            writer.writerow(["å¹³å‡", f"{r_stats.get('mean', 0):.2f}"])
            writer.writerow(["æ¨™æº–åå·®", f"{r_stats.get('std', 0):.2f}"])
            writer.writerow(["æœ€å°å€¤", f"{r_stats.get('min', 0):.2f}"])
            writer.writerow(["æœ€å¤§å€¤", f"{r_stats.get('max', 0):.2f}"])
            writer.writerow([])

            # Frequencyçµ±è¨ˆ
            writer.writerow(["Frequencyçµ±è¨ˆ"])
            writer.writerow(["é …ç›®", "å€¤"])
            f_stats = rfm_stats.get("rfm_stats", {}).get("frequency", {})
            writer.writerow(["å¹³å‡", f"{f_stats.get('mean', 0):.2f}"])
            writer.writerow(["æ¨™æº–åå·®", f"{f_stats.get('std', 0):.2f}"])
            writer.writerow(["æœ€å°å€¤", f"{f_stats.get('min', 0):.2f}"])
            writer.writerow(["æœ€å¤§å€¤", f"{f_stats.get('max', 0):.2f}"])
            writer.writerow([])

            # Monetaryçµ±è¨ˆ
            writer.writerow(["Monetaryçµ±è¨ˆ"])
            writer.writerow(["é …ç›®", "å€¤"])
            m_stats = rfm_stats.get("rfm_stats", {}).get("monetary", {})
            writer.writerow(["å¹³å‡", f"{m_stats.get('mean', 0):.2f}"])
            writer.writerow(["æ¨™æº–åå·®", f"{m_stats.get('std', 0):.2f}"])
            writer.writerow(["æœ€å°å€¤", f"{m_stats.get('min', 0):.2f}"])
            writer.writerow(["æœ€å¤§å€¤", f"{m_stats.get('max', 0):.2f}"])
            writer.writerow([])

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å¸ƒ
            segment_counts = rfm_stats.get("segment_counts", {})
            if segment_counts:
                writer.writerow(["ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å¸ƒ"])
                writer.writerow(["ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", "é¡§å®¢æ•°", "å‰²åˆ(%)"])
                total_customers = sum(segment_counts.values())
                for segment, count in segment_counts.items():
                    percentage = (
                        (count / total_customers * 100) if total_customers > 0 else 0
                    )
                    writer.writerow([segment, count, f"{percentage:.1f}"])
                writer.writerow([])

        # é¡§å®¢è©³ç´°ãƒ‡ãƒ¼ã‚¿
        writer.writerow(["é¡§å®¢è©³ç´°ãƒ‡ãƒ¼ã‚¿"])
        writer.writerow(
            [
                "é¡§å®¢ID",
                "Recency",
                "Frequency",
                "Monetary",
                "RFMã‚¹ã‚³ã‚¢",
                "Rã‚¹ã‚³ã‚¢",
                "Fã‚¹ã‚³ã‚¢",
                "Mã‚¹ã‚³ã‚¢",
                "ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ",
            ]
        )

        for customer in customer_data:
            metadata = customer.metadata_json or {}
            writer.writerow(
                [
                    customer.point_name,
                    f"{customer.dimension_1:.2f}" if customer.dimension_1 else "0.00",
                    f"{customer.dimension_2:.2f}" if customer.dimension_2 else "0.00",
                    f"{customer.dimension_3:.2f}" if customer.dimension_3 else "0.00",
                    f"{customer.dimension_4:.2f}" if customer.dimension_4 else "0.00",
                    metadata.get("r_score", ""),
                    metadata.get("f_score", ""),
                    metadata.get("m_score", ""),
                    metadata.get("segment", ""),
                ]
            )

        # CSVå†…å®¹ã‚’å–å¾—
        csv_content = output.getvalue()
        output.close()

        print(f"Generated CSV content length: {len(csv_content)} characters")

        # ãƒ•ã‚¡ã‚¤ãƒ«åè¨­å®š
        filename = f"rfm_details_{session_id}.csv"

        # Responseã‚’ä½œæˆ
        return Response(
            content=csv_content.encode("utf-8-sig"),
            media_type="text/csv",
            headers={"Content-Disposition": f'attachment; filename="{filename}"'},
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"è©³ç´°CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail=f"è©³ç´°CSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )


@router.get("/download/{session_id}/customers")
async def download_rfm_customers(session_id: int, db: Session = Depends(get_db)):
    """é¡§å®¢RFMãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        from models import AnalysisSession, CoordinatesData
        import pandas as pd

        print(f"=== é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ===")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )

        if not session:
            print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise HTTPException(
                status_code=404, detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        if session.analysis_type != "rfm":
            print(f"âŒ åˆ†æã‚¿ã‚¤ãƒ—ãŒé•ã„ã¾ã™: {session.analysis_type}")
            raise HTTPException(
                status_code=400,
                detail=f"RFMåˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {session.analysis_type}",
            )

        print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºèª: {session.session_name}")

        # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        customers = (
            db.query(CoordinatesData)
            .filter(
                CoordinatesData.session_id == session_id,
                CoordinatesData.point_type == "customer",
            )
            .order_by(CoordinatesData.point_name)
            .all()
        )

        print(f"å–å¾—ã—ãŸé¡§å®¢ãƒ‡ãƒ¼ã‚¿æ•°: {len(customers)}")

        if not customers:
            print("âŒ é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            all_data = (
                db.query(CoordinatesData)
                .filter(CoordinatesData.session_id == session_id)
                .all()
            )
            print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ã®å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_data)}")
            for data in all_data[:5]:  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                print(f"  - {data.point_name} ({data.point_type})")

            raise HTTPException(
                status_code=404,
                detail=f"é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ã«ã¯ {len(all_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ãŒã€é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
            )

        # Pandasã‚’ä½¿ç”¨ã—ã¦CSVä½œæˆ
        data_list = []
        for customer in customers:
            metadata = customer.metadata_json or {}
            data_list.append(
                {
                    "customer_id": customer.point_name,
                    "recency": (
                        customer.dimension_1
                        if customer.dimension_1 is not None
                        else 0.0
                    ),
                    "frequency": (
                        customer.dimension_2
                        if customer.dimension_2 is not None
                        else 0.0
                    ),
                    "monetary": (
                        customer.dimension_3
                        if customer.dimension_3 is not None
                        else 0.0
                    ),
                    "rfm_score": (
                        customer.dimension_4
                        if customer.dimension_4 is not None
                        else 0.0
                    ),
                    "r_score": metadata.get("r_score", ""),
                    "f_score": metadata.get("f_score", ""),
                    "m_score": metadata.get("m_score", ""),
                    "segment": metadata.get("segment", ""),
                }
            )

        # DataFrameã«å¤‰æ›ã—ã¦CSVå‡ºåŠ›
        df = pd.DataFrame(data_list)

        # CSVæ–‡å­—åˆ—ã¨ã—ã¦å‡ºåŠ›
        csv_content = df.to_csv(index=False, encoding="utf-8")
        print(f"CSVç”Ÿæˆå®Œäº†: {len(csv_content)} æ–‡å­—, {len(df)} è¡Œ")

        # ãƒ•ã‚¡ã‚¤ãƒ«åè¨­å®š
        filename = f"rfm_customers_{session_id}.csv"

        # BOMä»˜ãUTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        csv_bytes = csv_content.encode("utf-8-sig")

        return Response(
            content=csv_bytes,
            media_type="text/csv; charset=utf-8",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "Content-Type": "text/csv; charset=utf-8",
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ é¡§å®¢ãƒ‡ãƒ¼ã‚¿CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"é¡§å®¢ãƒ‡ãƒ¼ã‚¿CSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


@router.get("/download/{session_id}/segments")
async def download_rfm_segments(session_id: int, db: Session = Depends(get_db)):
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥çµ±è¨ˆã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        from models import AnalysisSession, CoordinatesData
        import pandas as pd

        print(f"=== ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±è¨ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ===")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        session = (
            db.query(AnalysisSession).filter(AnalysisSession.id == session_id).first()
        )

        if not session:
            raise HTTPException(
                status_code=404, detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        if session.analysis_type != "rfm":
            raise HTTPException(
                status_code=400,
                detail=f"RFMåˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {session.analysis_type}",
            )

        # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        customers = (
            db.query(CoordinatesData)
            .filter(
                CoordinatesData.session_id == session_id,
                CoordinatesData.point_type == "customer",
            )
            .all()
        )

        if not customers:
            raise HTTPException(status_code=404, detail="é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # DataFrameã«å¤‰æ›
        data_list = []
        for customer in customers:
            metadata = customer.metadata_json or {}
            data_list.append(
                {
                    "customer_id": customer.point_name,
                    "recency": customer.dimension_1 or 0,
                    "frequency": customer.dimension_2 or 0,
                    "monetary": customer.dimension_3 or 0,
                    "rfm_score": customer.dimension_4 or 0,
                    "segment": metadata.get("segment", "ä¸æ˜"),
                }
            )

        df = pd.DataFrame(data_list)

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥çµ±è¨ˆã‚’è¨ˆç®—
        segment_stats = (
            df.groupby("segment")
            .agg(
                {
                    "customer_id": "count",
                    "recency": "mean",
                    "frequency": "mean",
                    "monetary": "mean",
                    "rfm_score": "mean",
                }
            )
            .round(2)
        )

        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¿½åŠ 
        total_customers = len(df)
        segment_stats["percentage"] = (
            segment_stats["customer_id"] / total_customers * 100
        ).round(1)

        # åˆ—åã‚’æ—¥æœ¬èªã«å¤‰æ›´
        segment_stats.columns = [
            "customer_count",
            "avg_recency",
            "avg_frequency",
            "avg_monetary",
            "avg_rfm_score",
            "percentage",
        ]

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ—ã«å¤‰æ›
        segment_stats = segment_stats.reset_index()
        segment_stats.columns = [
            "segment",
            "customer_count",
            "avg_recency",
            "avg_frequency",
            "avg_monetary",
            "avg_rfm_score",
            "percentage",
        ]

        # CSVã¨ã—ã¦å‡ºåŠ›
        csv_content = segment_stats.to_csv(index=False, encoding="utf-8")
        print(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±è¨ˆCSVç”Ÿæˆå®Œäº†: {len(segment_stats)} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")

        filename = f"rfm_segments_{session_id}.csv"
        csv_bytes = csv_content.encode("utf-8-sig")

        return Response(
            content=csv_bytes,
            media_type="text/csv; charset=utf-8",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "Content-Type": "text/csv; charset=utf-8",
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±è¨ˆCSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ±è¨ˆCSVå‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )


@router.get("/debug/session/{session_id}/data")
async def debug_session_coordinates(session_id: int, db: Session = Depends(get_db)):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«è¡¨ç¤º"""
    try:
        from models import CoordinatesData

        coordinates = (
            db.query(CoordinatesData)
            .filter(CoordinatesData.session_id == session_id)
            .limit(10)
            .all()
        )  # æœ€åˆã®10ä»¶ã®ã¿

        result = []
        for coord in coordinates:
            result.append(
                {
                    "point_name": coord.point_name,
                    "point_type": coord.point_type,
                    "dimensions": [
                        coord.dimension_1,
                        coord.dimension_2,
                        coord.dimension_3,
                        coord.dimension_4,
                    ],
                    "metadata": coord.metadata_json,
                }
            )

        return {
            "session_id": session_id,
            "total_count": db.query(CoordinatesData)
            .filter(CoordinatesData.session_id == session_id)
            .count(),
            "customer_count": db.query(CoordinatesData)
            .filter(
                CoordinatesData.session_id == session_id,
                CoordinatesData.point_type == "customer",
            )
            .count(),
            "sample_data": result,
        }

    except Exception as e:
        return {"error": str(e)}


# routers/rfm.py ã«è¿½åŠ ã™ã‚‹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ


@router.post("/admin/fix-decimal-precision")
async def fix_decimal_precision(db: Session = Depends(get_db)):
    """DECIMALç²¾åº¦å•é¡Œã‚’ä¿®æ­£"""
    try:
        from sqlalchemy import text

        migration_steps = []

        # Step 1: metadata_jsonã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆã¾ã å­˜åœ¨ã—ãªã„å ´åˆï¼‰
        try:
            add_metadata_sql = text(
                """
                ALTER TABLE coordinates_data 
                ADD COLUMN IF NOT EXISTS metadata_json JSONB
            """
            )
            db.execute(add_metadata_sql)
            migration_steps.append("metadata_jsonã‚«ãƒ©ãƒ è¿½åŠ ")
        except Exception as e:
            migration_steps.append(f"metadata_jsonã‚«ãƒ©ãƒ è¿½åŠ ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")

        # Step 2: DECIMALç²¾åº¦ã‚’ä¿®æ­£
        decimal_fixes = [
            ("dimension_1", "DECIMAL(10,2)", "Recencyç”¨"),
            ("dimension_2", "DECIMAL(10,2)", "Frequencyç”¨"),
            ("dimension_3", "DECIMAL(15,2)", "Monetaryç”¨ï¼ˆé«˜ç²¾åº¦ï¼‰"),
            ("dimension_4", "DECIMAL(10,2)", "RFMã‚¹ã‚³ã‚¢ç”¨"),
        ]

        for column, new_type, description in decimal_fixes:
            try:
                alter_sql = text(
                    f"""
                    ALTER TABLE coordinates_data 
                    ALTER COLUMN {column} TYPE {new_type}
                """
                )
                db.execute(alter_sql)
                migration_steps.append(f"{column}ã‚’{new_type}ã«å¤‰æ›´ ({description})")
            except Exception as e:
                migration_steps.append(f"{column}å¤‰æ›´ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # Step 3: å¤±æ•—ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        try:
            cleanup_sql = text(
                """
                DELETE FROM coordinates_data 
                WHERE session_id IN (
                    SELECT DISTINCT session_id 
                    FROM coordinates_data 
                    WHERE session_id >= 315
                )
            """
            )
            result = db.execute(cleanup_sql)
            migration_steps.append(
                f"å¤±æ•—ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤: {result.rowcount}ä»¶"
            )
        except Exception as e:
            migration_steps.append(f"ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

        db.commit()

        # Step 4: ç¾åœ¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ç¢ºèª
        verify_sql = text(
            """
            SELECT column_name, data_type, numeric_precision, numeric_scale
            FROM information_schema.columns 
            WHERE table_name = 'coordinates_data' 
            AND column_name LIKE 'dimension_%'
            ORDER BY column_name
        """
        )

        columns = db.execute(verify_sql).fetchall()
        column_info = [
            {"column": col[0], "type": col[1], "precision": col[2], "scale": col[3]}
            for col in columns
        ]

        return {
            "success": True,
            "message": "DECIMALç²¾åº¦ä¿®æ­£å®Œäº†",
            "migration_steps": migration_steps,
            "current_columns": column_info,
        }

    except Exception as e:
        db.rollback()
        import traceback

        return {"success": False, "error": str(e), "traceback": traceback.format_exc()}


@router.get("/admin/check-table-structure")
async def check_table_structure(db: Session = Depends(get_db)):
    """ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ç¢ºèª"""
    try:
        from sqlalchemy import text

        # coordinates_dataãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’ç¢ºèª
        structure_sql = text(
            """
            SELECT 
                column_name, 
                data_type, 
                numeric_precision, 
                numeric_scale,
                is_nullable,
                column_default
            FROM information_schema.columns 
            WHERE table_name = 'coordinates_data' 
            ORDER BY ordinal_position
        """
        )

        columns = db.execute(structure_sql).fetchall()

        table_structure = []
        for col in columns:
            col_info = {
                "column_name": col[0],
                "data_type": col[1],
                "is_nullable": col[4],
                "column_default": col[5],
            }

            # æ•°å€¤å‹ã®å ´åˆã¯ç²¾åº¦æƒ…å ±ã‚’è¿½åŠ 
            if col[2] is not None:
                col_info["numeric_precision"] = col[2]
                col_info["numeric_scale"] = col[3]

            table_structure.append(col_info)

        # ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã‚‚ç¢ºèª
        count_sql = text(
            """
            SELECT 
                session_id,
                point_type,
                COUNT(*) as count
            FROM coordinates_data 
            GROUP BY session_id, point_type 
            ORDER BY session_id DESC 
            LIMIT 10
        """
        )

        data_counts = db.execute(count_sql).fetchall()
        count_info = [
            {"session_id": row[0], "point_type": row[1], "count": row[2]}
            for row in data_counts
        ]

        return {
            "success": True,
            "table_structure": table_structure,
            "data_counts": count_info,
        }

    except Exception as e:
        import traceback

        return {"success": False, "error": str(e), "traceback": traceback.format_exc()}


# routers/rfm.py ã«è¿½åŠ ã™ã‚‹ä¿®æ­£ç‰ˆãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³


@router.post("/admin/fix-decimal-with-view-handling")
async def fix_decimal_with_view_handling(db: Session = Depends(get_db)):
    """VIEWã‚’å‡¦ç†ã—ã¦ã‹ã‚‰DECIMALç²¾åº¦ã‚’ä¿®æ­£"""
    try:
        from sqlalchemy import text

        migration_steps = []

        # Step 1: æ—¢å­˜ã®VIEWã‚’ç¢ºèª
        check_views_sql = text(
            """
            SELECT schemaname, viewname, definition
            FROM pg_views 
            WHERE viewname LIKE '%rfm%' OR viewname LIKE '%analysis%'
        """
        )

        views = db.execute(check_views_sql).fetchall()
        migration_steps.append(f"ç™ºè¦‹ã•ã‚ŒãŸVIEWæ•°: {len(views)}")

        view_definitions = {}

        # Step 2: é–¢é€£ã™ã‚‹VIEWã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦å‰Šé™¤
        for view in views:
            view_name = view[1]
            view_definition = view[2]
            view_definitions[view_name] = view_definition
            migration_steps.append(f"VIEWãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {view_name}")

            try:
                drop_view_sql = text(f"DROP VIEW IF EXISTS {view_name} CASCADE")
                db.execute(drop_view_sql)
                migration_steps.append(f"VIEWå‰Šé™¤æˆåŠŸ: {view_name}")
            except Exception as e:
                migration_steps.append(f"VIEWå‰Šé™¤ã‚¨ãƒ©ãƒ¼ {view_name}: {str(e)}")

        # Step 3: DECIMALç²¾åº¦ã‚’ä¿®æ­£
        decimal_fixes = [
            ("dimension_1", "DECIMAL(10,2)", "Recencyç”¨"),
            ("dimension_2", "DECIMAL(10,2)", "Frequencyç”¨"),
            ("dimension_3", "DECIMAL(15,2)", "Monetaryç”¨ï¼ˆé«˜ç²¾åº¦ï¼‰"),
            ("dimension_4", "DECIMAL(10,2)", "RFMã‚¹ã‚³ã‚¢ç”¨"),
        ]

        for column, new_type, description in decimal_fixes:
            try:
                alter_sql = text(
                    f"""
                    ALTER TABLE coordinates_data 
                    ALTER COLUMN {column} TYPE {new_type}
                """
                )
                db.execute(alter_sql)
                migration_steps.append(f"âœ… {column}ã‚’{new_type}ã«å¤‰æ›´ ({description})")
            except Exception as e:
                migration_steps.append(f"âŒ {column}å¤‰æ›´ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # Step 4: å¤±æ•—ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        try:
            cleanup_sql = text(
                """
                DELETE FROM coordinates_data 
                WHERE session_id >= 315
            """
            )
            result = db.execute(cleanup_sql)
            migration_steps.append(
                f"âœ… å¤±æ•—ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤: {result.rowcount}ä»¶"
            )
        except Exception as e:
            migration_steps.append(f"âŒ ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

        db.commit()

        # Step 5: VIEWã‚’å†ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        # æ³¨æ„: VIEWå®šç¾©ã‚’æ‰‹å‹•ã§ç¢ºèªã—ã¦å¿…è¦ãªã‚‚ã®ã®ã¿å†ä½œæˆ
        recreated_views = []
        for view_name, definition in view_definitions.items():
            try:
                # VIEWå®šç¾©ã‚’ä¿®æ­£ã—ã¦å†ä½œæˆã™ã‚‹å ´åˆ
                # create_view_sql = text(f"CREATE VIEW {view_name} AS {definition}")
                # db.execute(create_view_sql)
                # recreated_views.append(view_name)
                migration_steps.append(
                    f"VIEWå®šç¾©ä¿å­˜: {view_name} (æ‰‹å‹•ã§å†ä½œæˆãŒå¿…è¦)"
                )
            except Exception as e:
                migration_steps.append(f"VIEWå†ä½œæˆã‚¨ãƒ©ãƒ¼ {view_name}: {str(e)}")

        # Step 6: ç¾åœ¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ç¢ºèª
        verify_sql = text(
            """
            SELECT column_name, data_type, numeric_precision, numeric_scale
            FROM information_schema.columns 
            WHERE table_name = 'coordinates_data' 
            AND column_name LIKE 'dimension_%'
            ORDER BY column_name
        """
        )

        columns = db.execute(verify_sql).fetchall()
        column_info = [
            {"column": col[0], "type": col[1], "precision": col[2], "scale": col[3]}
            for col in columns
        ]

        return {
            "success": True,
            "message": "DECIMALç²¾åº¦ä¿®æ­£å®Œäº†ï¼ˆVIEWå‡¦ç†æ¸ˆã¿ï¼‰",
            "migration_steps": migration_steps,
            "current_columns": column_info,
            "backed_up_views": list(view_definitions.keys()),
            "view_definitions": view_definitions,
        }

    except Exception as e:
        db.rollback()
        import traceback

        return {"success": False, "error": str(e), "traceback": traceback.format_exc()}


@router.get("/admin/check-views")
async def check_views(db: Session = Depends(get_db)):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®VIEWã‚’ç¢ºèª"""
    try:
        from sqlalchemy import text

        # å…¨VIEWã‚’ç¢ºèª
        views_sql = text(
            """
            SELECT schemaname, viewname, definition
            FROM pg_views 
            WHERE schemaname = 'public'
            ORDER BY viewname
        """
        )

        views = db.execute(views_sql).fetchall()

        view_list = []
        for view in views:
            view_list.append(
                {
                    "schema": view[0],
                    "name": view[1],
                    "definition": (
                        view[2][:200] + "..." if len(view[2]) > 200 else view[2]
                    ),
                }
            )

        # coordinates_dataãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¾å­˜ã™ã‚‹VIEWã‚’ç‰¹å®š
        dependent_views_sql = text(
            """
            SELECT DISTINCT 
                v.schemaname, 
                v.viewname,
                v.definition
            FROM pg_views v
            WHERE v.definition ILIKE '%coordinates_data%'
            OR v.definition ILIKE '%dimension_%'
        """
        )

        dependent_views = db.execute(dependent_views_sql).fetchall()

        dependent_list = []
        for view in dependent_views:
            dependent_list.append(
                {"schema": view[0], "name": view[1], "definition": view[2]}
            )

        return {
            "success": True,
            "all_views_count": len(view_list),
            "all_views": view_list,
            "dependent_views_count": len(dependent_list),
            "dependent_views": dependent_list,
        }

    except Exception as e:
        import traceback

        return {"success": False, "error": str(e), "traceback": traceback.format_exc()}


# ç·Šæ€¥å›é¿ï¼šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ¶ç´„ã‚’ç„¡è¦–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã§çµæœã‚’ä¿å­˜
@router.post("/analyze-file-based")
async def analyze_rfm_file_based(
    file: UploadFile = File(...),
    session_name: str = Query(...),
    customer_id_col: str = Query("id"),
    date_col: str = Query("date"),
    amount_col: str = Query("price"),
    rfm_divisions: int = Query(3),
):
    """RFMåˆ†æï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ä¿å­˜ç‰ˆãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰"""
    try:
        print(f"=== ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹RFMåˆ†æé–‹å§‹ ===")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {file.filename}")

        # CSVèª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šï¼‰
        contents = await file.read()

        # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
        csv_text = None
        encodings = [
            "utf-8",
            "shift_jis",
            "cp932",
            "euc-jp",
            "iso-2022-jp",
            "utf-8-sig",
        ]

        for encoding in encodings:
            try:
                csv_text = contents.decode(encoding)
                print(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ¤å®šæˆåŠŸ: {encoding}")
                break
            except UnicodeDecodeError:
                print(f"âŒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¤±æ•—: {encoding}")
                continue

        if csv_text is None:
            raise ValueError("å¯¾å¿œã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # CSVãƒ†ã‚­ã‚¹ãƒˆã®ç¢ºèª
        print(f"CSVãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰:\n{csv_text[:200]}")

        # DataFrameã«å¤‰æ›
        import io

        df = pd.read_csv(io.StringIO(csv_text))
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")
        print(f"æœ€åˆã®3è¡Œ:\n{df.head(3)}")

        # RFMåˆ†æå®Ÿè¡Œ
        from analysis.rfm import RFMAnalysisAnalyzer

        analyzer = RFMAnalysisAnalyzer()

        results = analyzer.analyze(
            df=df,
            customer_id_col=customer_id_col,
            date_col=date_col,
            amount_col=amount_col,
            rfm_divisions=rfm_divisions,
        )

        print(f"RFMåˆ†æçµæœ: é¡§å®¢æ•°={results['total_customers']}")

        # ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
        plot_base64 = analyzer.create_plot(results, df)
        print(f"ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†: {len(plot_base64)}æ–‡å­—")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        import json
        import os
        from datetime import datetime

        save_dir = "/tmp/rfm_results"
        os.makedirs(save_dir, exist_ok=True)

        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        result_data = {
            "session_name": session_name,
            "analysis_type": "rfm",
            "timestamp": datetime.now().isoformat(),
            "total_customers": results["total_customers"],
            "analysis_date": results["analysis_date"],
            "date_range": results["date_range"],
            "rfm_divisions": results["rfm_divisions"],
            # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ä»¶ä¿å­˜ï¼‰
            "customer_data": results["customer_rfm_data"],
            # çµ±è¨ˆæƒ…å ±
            "segment_counts": results["segment_counts"],
            "rfm_stats": results["rfm_stats"],
            "segment_stats": results["segment_stats"],
            # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒï¼ˆBase64ï¼‰
            "plot_image": plot_base64,
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "metadata": {
                "filename": file.filename,
                "encoding_used": encoding,
                "rows": df.shape[0],
                "columns": df.shape[1],
            },
        }

        # JSONå½¢å¼ã§ä¿å­˜
        result_file = f"{save_dir}/{session_name}_complete.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)

        # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’CSVå½¢å¼ã§ã‚‚ä¿å­˜
        customer_csv_file = f"{save_dir}/{session_name}_customers.csv"
        with open(customer_csv_file, "w", encoding="utf-8-sig", newline="") as f:
            import csv

            writer = csv.writer(f)

            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow(
                [
                    "customer_id",
                    "recency",
                    "frequency",
                    "monetary",
                    "rfm_score",
                    "r_score",
                    "f_score",
                    "m_score",
                    "segment",
                ]
            )

            # ãƒ‡ãƒ¼ã‚¿è¡Œ
            for customer in results["customer_rfm_data"]:
                writer.writerow(
                    [
                        customer["customer_id"],
                        customer["recency"],
                        customer["frequency"],
                        customer["monetary"],
                        customer["rfm_score"],
                        customer["r_score"],
                        customer["f_score"],
                        customer["m_score"],
                        customer["segment"],
                    ]
                )

        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†:")
        print(f"  - JSON: {result_file}")
        print(f"  - CSV:  {customer_csv_file}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå®Ÿéš›ã®RFMåˆ†æçµæœã‚’è¿”ã™ï¼‰
        return {
            "success": True,
            "session_name": session_name,
            "analysis_type": "rfm",
            "data": {
                "total_customers": results["total_customers"],
                "analysis_date": results["analysis_date"],
                "date_range": results["date_range"],
                "rfm_divisions": results["rfm_divisions"],
                # é¡§å®¢ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®100ä»¶ã®ã¿ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã‚ã‚‹ï¼‰
                "customer_data": results["customer_rfm_data"][:100],
                # çµ±è¨ˆæƒ…å ±
                "segment_counts": results["segment_counts"],
                "rfm_stats": results["rfm_stats"],
                "segment_definitions": results["segment_definitions"],
                # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒ
                "plot_image": plot_base64,
            },
            "metadata": {
                "filename": file.filename,
                "encoding_used": encoding,
                "result_files": {
                    "json": result_file,
                    "csv": customer_csv_file,
                },
                "download_urls": {
                    "csv": f"/api/rfm/download-file/{session_name}",
                    "json": f"/api/rfm/download-json/{session_name}",
                },
            },
        }

    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback

        print(f"è©³ç´°:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/download-file/{session_name}")
async def download_rfm_csv(session_name: str):
    """RFMã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        from fastapi import Response

        customer_csv_file = f"/tmp/rfm_results/{session_name}_customers.csv"

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(customer_csv_file):
            raise HTTPException(
                status_code=404,
                detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_name} ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
            )

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(customer_csv_file, "r", encoding="utf-8-sig") as f:
            csv_content = f.read()

        return Response(
            content=csv_content.encode("utf-8-sig"),
            media_type="text/csv",
            headers={
                "Content-Disposition": f'attachment; filename="rfm_customers_{session_name}.csv"'
            },
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/download-file/{session_name}")
async def download_rfm_csv(session_name: str):
    """RFMã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    try:
        import os
        from fastapi import Response
        
        print(f"=== CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {session_name} ===")
        
        customer_csv_file = f"/tmp/rfm_results/{session_name}_customers.csv"
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {customer_csv_file}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(customer_csv_file):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {customer_csv_file}")
            # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            try:
                files = os.listdir("/tmp/rfm_results/")
                print(f"åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«: {files}")
            except:
                print("çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            
            raise HTTPException(status_code=404, detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_name} ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(customer_csv_file)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size} bytes")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        try:
            with open(customer_csv_file, 'r', encoding='utf-8-sig') as f:
                csv_content = f.read()
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {len(csv_content)} æ–‡å­—")
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # UTF-8ã§èª­ã¿è¾¼ã¿å†è©¦è¡Œ
            try:
                with open(customer_csv_file, 'r', encoding='utf-8') as f:
                    csv_content = f.read()
                print(f"UTF-8ã§èª­ã¿è¾¼ã¿æˆåŠŸ: {len(csv_content)} æ–‡å­—")
            except Exception as e2:
                print(f"âŒ UTF-8èª­ã¿è¾¼ã¿ã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")
                raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e2}")
        
        # CSVã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å…ˆé ­ç¢ºèª
        print(f"CSVå…ˆé ­200æ–‡å­—:\n{csv_content[:200]}")
        
        return Response(
            content=csv_content.encode('utf-8-sig'),
            media_type="text/csv; charset=utf-8",
            headers={
                "Content-Disposition": f'attachment; filename="rfm_customers_{session_name}.csv"',
                "Content-Length": str(len(csv_content.encode('utf-8-sig')))
            },
        )
        
    except HTTPException:
        raise  # HTTPExceptionã¯å†ç™ºç”Ÿ
    except Exception as e:
        print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(f"è©³ç´°:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")


@router.get("/download-json/{session_name}")
async def download_rfm_json(session_name: str):
    """RFMã®åˆ†æçµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    try:
        import os
        from fastapi import Response
        
        print(f"=== JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {session_name} ===")
        
        result_file = f"/tmp/rfm_results/{session_name}_complete.json"
        
        if not os.path.exists(result_file):
            print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {result_file}")
            raise HTTPException(status_code=404, detail=f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_name} ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        with open(result_file, 'r', encoding='utf-8') as f:
            json_content = f.read()
        
        print(f"JSONèª­ã¿è¾¼ã¿æˆåŠŸ: {len(json_content)} æ–‡å­—")
        
        return Response(
            content=json_content.encode('utf-8'),
            media_type="application/json; charset=utf-8",
            headers={
                "Content-Disposition": f'attachment; filename="rfm_analysis_{session_name}.json"',
                "Content-Length": str(len(json_content.encode('utf-8')))
            },
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/files")
async def debug_files():
    """ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±"""
    try:
        import os
        
        save_dir = "/tmp/rfm_results"
        
        if not os.path.exists(save_dir):
            return {"error": "çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“", "path": save_dir}
        
        files_info = []
        for filename in os.listdir(save_dir):
            file_path = os.path.join(save_dir, filename)
            stat = os.stat(file_path)
            
            files_info.append({
                "filename": filename,
                "size": stat.st_size,
                "created": stat.st_ctime,
                "modified": stat.st_mtime,
            })
        
        return {
            "directory": save_dir,
            "files": files_info,
            "total_files": len(files_info)
        }
        
    except Exception as e:
        return {"error": str(e)}


# ç·Šæ€¥å›é¿ï¼šç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’è¿”ã™ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@router.get("/download-direct/{session_name}")
async def download_direct_csv(session_name: str):
    """ç›´æ¥CSVã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
    try:
        import os
        
        customer_csv_file = f"/tmp/rfm_results/{session_name}_customers.csv"
        
        if not os.path.exists(customer_csv_file):
            return {"error": "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "path": customer_csv_file}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
        with open(customer_csv_file, 'r', encoding='utf-8-sig') as f:
            lines = f.readlines()
        
        return {
            "success": True,
            "filename": customer_csv_file,
            "total_lines": len(lines),
            "header": lines[0].strip() if lines else "",
            "first_5_lines": [line.strip() for line in lines[:5]],
            "last_2_lines": [line.strip() for line in lines[-2:]] if len(lines) > 2 else [],
        }
        
    except Exception as e:
        return {"error": str(e)}