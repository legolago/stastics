from typing import Dict, Any, Optional, List
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patheffects as pe
import io
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sqlalchemy.orm import Session  # â† ã“ã®è¡Œã‚’è¿½åŠ 
from .base import BaseAnalyzer


class PCAAnalyzer(BaseAnalyzer):
    """ä¸»æˆåˆ†åˆ†æã‚¯ãƒ©ã‚¹"""

    def get_analysis_type(self) -> str:
        return "pca"

    # æ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯å¤‰æ›´ãªã—
    def analyze(
        self,
        df: pd.DataFrame,
        n_components: int = 2,
        standardize: bool = True,
        **kwargs,
    ) -> Dict[str, Any]:
        """ä¸»æˆåˆ†åˆ†æã‚’å®Ÿè¡Œ"""
        try:
            print(f"=== PCAåˆ†æé–‹å§‹ ===")
            print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:\n{df.head()}")
            print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
            print(f"æ¨™æº–åŒ–: {standardize}")

            # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
            df_processed = self._preprocess_pca_data(df)
            print(f"å‰å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_processed.shape}")

            # PCAåˆ†æã®å®Ÿè¡Œ
            results = self._compute_pca(df_processed, n_components, standardize)

            print(f"PCAåˆ†æçµæœ: {list(results.keys())}")
            return results

        except Exception as e:
            print(f"PCAåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
            raise

    def _preprocess_pca_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """PCAç”¨ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        df_clean = self.preprocess_data(df)  # åŸºåº•ã‚¯ãƒ©ã‚¹ã®å‰å‡¦ç†ã‚’ä½¿ç”¨

        # å®šæ•°åˆ—ã®é™¤å»
        for col in df_clean.columns:
            if df_clean[col].std() == 0:
                print(f"è­¦å‘Š: å®šæ•°åˆ— '{col}' ã‚’é™¤å»ã—ã¾ã™")
                df_clean = df_clean.drop(columns=[col])

        if df_clean.empty or df_clean.shape[1] < 2:
            raise ValueError(
                "æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½2åˆ—ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰"
            )

        print(f"å‰å‡¦ç†: {df.shape} -> {df_clean.shape}")
        return df_clean

    def _compute_pca(
        self, df: pd.DataFrame, n_components: int, standardize: bool
    ) -> Dict[str, Any]:
        """ä¸»æˆåˆ†åˆ†æã®è¨ˆç®—"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            X = df.values
            feature_names = df.columns.tolist()
            sample_names = df.index.tolist()

            # æ¨™æº–åŒ–
            scaler = None
            if standardize:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                print("ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã—ã¾ã—ãŸ")
            else:
                X_scaled = X
                print("æ¨™æº–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

            # æ¬¡å…ƒæ•°ã®èª¿æ•´
            max_components = min(X_scaled.shape[0], X_scaled.shape[1])
            n_components = min(n_components, max_components)
            print(f"ä½¿ç”¨ã™ã‚‹ä¸»æˆåˆ†æ•°: {n_components}")

            # PCAå®Ÿè¡Œ
            pca = PCA(n_components=n_components)
            X_pca = pca.fit_transform(X_scaled)

            # å¯„ä¸ç‡ã®è¨ˆç®—
            explained_variance_ratio = pca.explained_variance_ratio_
            cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

            # ä¸»æˆåˆ†å¾—ç‚¹
            component_scores = X_pca

            # ä¸»æˆåˆ†è² è·é‡
            if standardize:
                # æ¨™æº–åŒ–ã—ãŸå ´åˆã®è² è·é‡
                loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
            else:
                # æ¨™æº–åŒ–ã—ãªã„å ´åˆã®è² è·é‡
                loadings = pca.components_.T

            # çµ±è¨ˆçš„æ¤œå®šï¼ˆBartlettçƒé¢æ€§æ¤œå®šã®è¿‘ä¼¼ï¼‰
            correlation_matrix = np.corrcoef(X_scaled.T)
            det_corr = np.linalg.det(correlation_matrix)

            # Kaiser-Meyer-Olkin (KMO) æ¨™æœ¬å¦¥å½“æ€§ã®æ¸¬åº¦ã®ç°¡æ˜“è¨ˆç®—
            kmo_value = self._calculate_kmo(correlation_matrix)

            # æ¬¡å…ƒãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if n_components == 1:
                component_scores = np.column_stack(
                    [component_scores, np.zeros(component_scores.shape[0])]
                )
                loadings = np.column_stack([loadings, np.zeros(loadings.shape[0])])

            results = {
                "n_components": n_components,
                "n_samples": X_scaled.shape[0],
                "n_features": X_scaled.shape[1],
                "standardized": standardize,
                "explained_variance_ratio": explained_variance_ratio.tolist(),
                "cumulative_variance_ratio": cumulative_variance_ratio.tolist(),
                "eigenvalues": pca.explained_variance_.tolist(),
                "component_scores": component_scores,
                "loadings": loadings,
                "feature_names": feature_names,
                "sample_names": sample_names,
                "kmo": float(kmo_value),
                "determinant": float(det_corr),
                "total_inertia": float(cumulative_variance_ratio[-1]),  # ç·å¯„ä¸ç‡
            }

            return results

        except Exception as e:
            print(f"PCAè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            raise

    def _calculate_kmo(self, correlation_matrix):
        """KMOæ¨™æœ¬å¦¥å½“æ€§ã®æ¸¬åº¦ã‚’è¨ˆç®—"""
        try:
            corr_inv = np.linalg.inv(correlation_matrix)
            numer_sum = 0
            denom_sum = 0

            for i in range(correlation_matrix.shape[0]):
                for j in range(correlation_matrix.shape[1]):
                    if i != j:
                        numer_sum += correlation_matrix[i, j] ** 2
                        denom_sum += correlation_matrix[i, j] ** 2 + corr_inv[i, j] ** 2

            if denom_sum == 0:
                return 0.5

            kmo = numer_sum / denom_sum
            return max(0, min(1, kmo))  # 0-1ã®ç¯„å›²ã«åˆ¶é™
        except:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def create_plot(self, results: Dict[str, Any], df: pd.DataFrame) -> str:
        """PCAãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ"""
        try:
            print("=== PCAãƒ—ãƒ­ãƒƒãƒˆä½œæˆé–‹å§‹ ===")

            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
            self.setup_japanese_font()

            component_scores = results["component_scores"]
            loadings = results["loadings"]
            explained_variance_ratio = results["explained_variance_ratio"]
            feature_names = results["feature_names"]
            sample_names = results["sample_names"]

            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            fig.patch.set_facecolor("white")

            # 1. ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç¬¬1-2ä¸»æˆåˆ†ï¼‰
            scatter = ax1.scatter(
                component_scores[:, 0],
                component_scores[:, 1],
                c="#2E86AB",
                s=80,
                alpha=0.7,
                edgecolors="white",
                linewidths=0.5,
            )

            # ã‚µãƒ³ãƒ—ãƒ«ãƒ©ãƒ™ãƒ«
            for i, name in enumerate(sample_names):
                ax1.annotate(
                    str(name),
                    (component_scores[i, 0], component_scores[i, 1]),
                    xytext=(3, 3),
                    textcoords="offset points",
                    fontsize=9,
                    alpha=0.8,
                    fontfamily=[
                        "IPAexGothic",
                        "IPAGothic",
                        "DejaVu Sans",
                        "sans-serif",
                    ],
                )

            ax1.axhline(y=0, color="gray", linestyle="--", alpha=0.5)
            ax1.axvline(x=0, color="gray", linestyle="--", alpha=0.5)
            ax1.set_xlabel(
                f"ç¬¬1ä¸»æˆåˆ† ({explained_variance_ratio[0]*100:.1f}%)",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax1.set_ylabel(
                f"ç¬¬2ä¸»æˆåˆ† ({explained_variance_ratio[1]*100:.1f}%)",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax1.set_title(
                "ä¸»æˆåˆ†å¾—ç‚¹ãƒ—ãƒ­ãƒƒãƒˆ",
                fontweight="bold",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax1.grid(True, alpha=0.3)

            # 2. ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ—ãƒ­ãƒƒãƒˆ
            if loadings.shape[0] >= 2:  # ç‰¹å¾´é‡ãŒ2ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆ
                for i, name in enumerate(feature_names):
                    ax2.arrow(
                        0,
                        0,
                        loadings[i, 0],
                        loadings[i, 1],
                        head_width=0.03,
                        head_length=0.03,
                        fc="#A23B72",
                        ec="#A23B72",
                    )
                    ax2.text(
                        loadings[i, 0] * 1.1,
                        loadings[i, 1] * 1.1,
                        name,
                        fontsize=10,
                        ha="center",
                        va="center",
                        bbox=dict(
                            boxstyle="round,pad=0.3", facecolor="white", alpha=0.8
                        ),
                        fontfamily=[
                            "IPAexGothic",
                            "IPAGothic",
                            "DejaVu Sans",
                            "sans-serif",
                        ],
                    )

            ax2.axhline(y=0, color="gray", linestyle="--", alpha=0.5)
            ax2.axvline(x=0, color="gray", linestyle="--", alpha=0.5)
            ax2.set_xlabel(
                f"ç¬¬1ä¸»æˆåˆ†è² è·é‡",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax2.set_ylabel(
                f"ç¬¬2ä¸»æˆåˆ†è² è·é‡",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax2.set_title(
                "ä¸»æˆåˆ†è² è·é‡ãƒ—ãƒ­ãƒƒãƒˆ",
                fontweight="bold",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax2.grid(True, alpha=0.3)

            # è»¸ã®ç¯„å›²ã‚’èª¿æ•´
            max_loading = (
                np.max(np.abs(loadings[:, :2])) if loadings.shape[0] > 0 else 1
            )
            ax2.set_xlim(-max_loading * 1.2, max_loading * 1.2)
            ax2.set_ylim(-max_loading * 1.2, max_loading * 1.2)

            # 3. å¯„ä¸ç‡ãƒ—ãƒ­ãƒƒãƒˆ
            pc_numbers = range(1, len(explained_variance_ratio) + 1)
            bars = ax3.bar(
                pc_numbers,
                np.array(explained_variance_ratio) * 100,
                color="#2E86AB",
                alpha=0.7,
                edgecolor="white",
                linewidth=0.5,
            )

            # ç´¯ç©å¯„ä¸ç‡ã®ç·šã‚°ãƒ©ãƒ•
            ax3_twin = ax3.twinx()
            ax3_twin.plot(
                pc_numbers,
                np.array(results["cumulative_variance_ratio"]) * 100,
                "o-",
                color="#A23B72",
                linewidth=2,
                markersize=6,
            )
            ax3_twin.set_ylabel(
                "ç´¯ç©å¯„ä¸ç‡ (%)",
                color="#A23B72",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax3_twin.tick_params(axis="y", labelcolor="#A23B72")

            ax3.set_xlabel(
                "ä¸»æˆåˆ†",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax3.set_ylabel(
                "å¯„ä¸ç‡ (%)",
                color="#2E86AB",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax3.set_title(
                "ä¸»æˆåˆ†ã®å¯„ä¸ç‡",
                fontweight="bold",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )
            ax3.tick_params(axis="y", labelcolor="#2E86AB")
            ax3.grid(True, alpha=0.3)

            # ãƒãƒ¼ã®ä¸Šã«æ•°å€¤ã‚’è¡¨ç¤º
            for i, (pc, var_ratio) in enumerate(
                zip(pc_numbers, explained_variance_ratio)
            ):
                ax3.text(
                    pc,
                    var_ratio * 100 + 1,
                    f"{var_ratio*100:.1f}%",
                    ha="center",
                    va="bottom",
                    fontsize=9,
                    fontfamily=[
                        "IPAexGothic",
                        "IPAGothic",
                        "DejaVu Sans",
                        "sans-serif",
                    ],
                )

            # 4. æƒ…å ±ãƒ‘ãƒãƒ«
            ax4.axis("off")
            info_text = f"""ä¸»æˆåˆ†åˆ†æçµæœã‚µãƒãƒªãƒ¼

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {results['n_samples']} Ã— {results['n_features']}
ä½¿ç”¨ä¸»æˆåˆ†æ•°: {results['n_components']}
æ¨™æº–åŒ–: {'ã‚ã‚Š' if results['standardized'] else 'ãªã—'}

ç¬¬1-2ä¸»æˆåˆ†ç´¯ç©å¯„ä¸ç‡: {results['cumulative_variance_ratio'][1]*100:.1f}%
KMOæ¨™æœ¬å¦¥å½“æ€§: {results['kmo']:.3f}
ç›¸é–¢è¡Œåˆ—å¼: {results['determinant']:.6f}

å›ºæœ‰å€¤:
""" + "\n".join(
                [
                    f"ç¬¬{i+1}ä¸»æˆåˆ†: {ev:.3f}"
                    for i, ev in enumerate(results["eigenvalues"])
                ]
            )

            ax4.text(
                0.05,
                0.95,
                info_text,
                transform=ax4.transAxes,
                fontsize=11,
                verticalalignment="top",
                bbox=dict(boxstyle="round,pad=0.5", facecolor="#f8f9fa", alpha=0.8),
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )

            # KMOåˆ¤å®šåŸºæº–ã®è¿½åŠ 
            kmo_interpretation = ""
            kmo = results["kmo"]
            if kmo >= 0.9:
                kmo_interpretation = "éå¸¸ã«è‰¯ã„"
            elif kmo >= 0.8:
                kmo_interpretation = "è‰¯ã„"
            elif kmo >= 0.7:
                kmo_interpretation = "ã¾ã‚ã¾ã‚"
            elif kmo >= 0.6:
                kmo_interpretation = "å¹³å‡¡"
            else:
                kmo_interpretation = "æ‚ªã„"

            ax4.text(
                0.05,
                0.3,
                f"KMOåˆ¤å®š: {kmo_interpretation}",
                transform=ax4.transAxes,
                fontsize=12,
                fontweight="bold",
                color="green" if kmo >= 0.7 else "orange" if kmo >= 0.6 else "red",
                fontfamily=["IPAexGothic", "IPAGothic", "DejaVu Sans", "sans-serif"],
            )

            plt.tight_layout()

            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            plot_base64 = self.save_plot_as_base64(fig)
            print(f"PCAãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
            return plot_base64

        except Exception as e:
            print(f"ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return ""

    def _save_coordinates_data(
        self, db, session_id: int, df: pd.DataFrame, results: Dict[str, Any]
    ):
        """PCAç‰¹æœ‰ã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        from models import CoordinatesData

        try:
            component_scores = results.get("component_scores")
            loadings = results.get("loadings")
            sample_names = results.get("sample_names", df.index.tolist())
            feature_names = results.get("feature_names", df.columns.tolist())

            # ä¸»æˆåˆ†å¾—ç‚¹ã®ä¿å­˜ï¼ˆè¦³æ¸¬å€¤ã¨ã—ã¦ï¼‰
            if component_scores is not None:
                for i, name in enumerate(sample_names):
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(name),
                        point_type="observation",  # PCAç”¨ã®è¦³æ¸¬å€¤
                        dimension_1=(
                            float(component_scores[i, 0])
                            if component_scores.shape[1] > 0
                            else 0.0
                        ),
                        dimension_2=(
                            float(component_scores[i, 1])
                            if component_scores.shape[1] > 1
                            else 0.0
                        ),
                    )
                    db.add(coord_data)

            # ä¸»æˆåˆ†è² è·é‡ã®ä¿å­˜ï¼ˆå¤‰æ•°ã¨ã—ã¦ï¼‰
            if loadings is not None:
                for i, name in enumerate(feature_names):
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(name),
                        point_type="variable",  # PCAç”¨ã®å¤‰æ•°
                        dimension_1=(
                            float(loadings[i, 0]) if loadings.shape[1] > 0 else 0.0
                        ),
                        dimension_2=(
                            float(loadings[i, 1]) if loadings.shape[1] > 1 else 0.0
                        ),
                    )
                    db.add(coord_data)
        except Exception as e:
            print(f"PCAåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def create_response(
        self,
        results: Dict[str, Any],
        df: pd.DataFrame,
        session_id: int,
        session_name: str,
        file,
        plot_base64: str,
    ) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆJSON serializableå¯¾å¿œç‰ˆï¼‰"""
        try:
            component_scores = results["component_scores"]
            loadings = results["loadings"]

            # numpyé…åˆ—ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°
            def ensure_serializable(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, list):
                    return [ensure_serializable(item) for item in obj]
                elif isinstance(obj, dict):
                    return {k: ensure_serializable(v) for k, v in obj.items()}
                else:
                    return obj

            # ä¸»æˆåˆ†å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆï¼ˆserializableç‰ˆï¼‰
            component_scores_data = []
            for i, (sample_name, scores) in enumerate(zip(df.index, component_scores)):
                if isinstance(scores, (list, np.ndarray)) and len(scores) > 0:
                    scores_list = ensure_serializable(scores)
                    component_scores_data.append(
                        {
                            "name": str(sample_name),
                            "sample_name": str(sample_name),
                            "pc_1": (
                                float(scores_list[0]) if len(scores_list) > 0 else 0.0
                            ),
                            "pc_2": (
                                float(scores_list[1]) if len(scores_list) > 1 else 0.0
                            ),
                            "pc_3": (
                                float(scores_list[2]) if len(scores_list) > 2 else 0.0
                            ),
                            "dimension_1": (
                                float(scores_list[0]) if len(scores_list) > 0 else 0.0
                            ),
                            "dimension_2": (
                                float(scores_list[1]) if len(scores_list) > 1 else 0.0
                            ),
                            "dimension_3": (
                                float(scores_list[2]) if len(scores_list) > 2 else 0.0
                            ),
                        }
                    )

            # ä¸»æˆåˆ†è² è·é‡ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆï¼ˆserializableç‰ˆï¼‰
            component_loadings_data = []
            for i, (feature_name, loading_vals) in enumerate(zip(df.columns, loadings)):
                if (
                    isinstance(loading_vals, (list, np.ndarray))
                    and len(loading_vals) > 0
                ):
                    loading_list = ensure_serializable(loading_vals)
                    component_loadings_data.append(
                        {
                            "name": str(feature_name),
                            "variable_name": str(feature_name),
                            "pc_1": (
                                float(loading_list[0]) if len(loading_list) > 0 else 0.0
                            ),
                            "pc_2": (
                                float(loading_list[1]) if len(loading_list) > 1 else 0.0
                            ),
                            "pc_3": (
                                float(loading_list[2]) if len(loading_list) > 2 else 0.0
                            ),
                            "dimension_1": (
                                float(loading_list[0]) if len(loading_list) > 0 else 0.0
                            ),
                            "dimension_2": (
                                float(loading_list[1]) if len(loading_list) > 1 else 0.0
                            ),
                            "dimension_3": (
                                float(loading_list[2]) if len(loading_list) > 2 else 0.0
                            ),
                        }
                    )

            # å…¨ã¦ã®numpyé…åˆ—ã‚’serializableå½¢å¼ã«å¤‰æ›
            serializable_results = ensure_serializable(results)

            return {
                "success": True,
                "session_id": int(session_id),
                "analysis_type": self.get_analysis_type(),
                "data": {
                    "n_components": int(serializable_results["n_components"]),
                    "standardized": bool(serializable_results["standardized"]),
                    # å¾“æ¥å½¢å¼ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰- serializableç‰ˆ
                    "loadings": ensure_serializable(loadings),
                    "eigenvalues": ensure_serializable(
                        serializable_results["eigenvalues"]
                    ),
                    "explained_variance_ratio": ensure_serializable(
                        serializable_results["explained_variance_ratio"]
                    ),
                    "cumulative_variance_ratio": ensure_serializable(
                        serializable_results["cumulative_variance_ratio"]
                    ),
                    "component_scores": ensure_serializable(component_scores),
                    # æ–°å½¢å¼ï¼ˆåº§æ¨™ãƒ‡ãƒ¼ã‚¿ï¼‰
                    "component_scores_data": component_scores_data,
                    "component_loadings_data": component_loadings_data,
                    # ä¸»æˆåˆ†è² è·é‡è¡Œåˆ—ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ï¼‰
                    "loadings_matrix": ensure_serializable(loadings),
                    # å¤‰æ•°ãƒ»ã‚µãƒ³ãƒ—ãƒ«å
                    "feature_names": [
                        str(x) for x in serializable_results["feature_names"]
                    ],
                    "sample_names": [
                        str(x) for x in serializable_results["sample_names"]
                    ],
                    # çµ±è¨ˆå€¤
                    "kmo": float(serializable_results.get("kmo", 0.0)),
                    "determinant": float(serializable_results.get("determinant", 0.0)),
                    # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒ
                    "plot_image": str(plot_base64),
                    # åº§æ¨™å½¢å¼ï¼ˆå¾“æ¥äº’æ›ï¼‰- serializableç‰ˆ
                    "coordinates": {
                        "scores": [
                            {
                                "name": str(name),
                                "dimension_1": (
                                    float(ensure_serializable(component_scores[i])[0])
                                    if len(ensure_serializable(component_scores[i])) > 0
                                    else 0.0
                                ),
                                "dimension_2": (
                                    float(ensure_serializable(component_scores[i])[1])
                                    if len(ensure_serializable(component_scores[i])) > 1
                                    else 0.0
                                ),
                            }
                            for i, name in enumerate(df.index)
                        ],
                        "loadings": [
                            {
                                "name": str(name),
                                "dimension_1": (
                                    float(ensure_serializable(loadings[i])[0])
                                    if len(ensure_serializable(loadings[i])) > 0
                                    else 0.0
                                ),
                                "dimension_2": (
                                    float(ensure_serializable(loadings[i])[1])
                                    if len(ensure_serializable(loadings[i])) > 1
                                    else 0.0
                                ),
                            }
                            for i, name in enumerate(df.columns)
                        ],
                    },
                },
                "metadata": {
                    "session_name": str(session_name),
                    "filename": str(file.filename),
                    "rows": int(df.shape[0]),
                    "columns": int(df.shape[1]),
                    "feature_names": [
                        str(x) for x in serializable_results["feature_names"]
                    ],
                    "sample_names": [
                        str(x) for x in serializable_results["sample_names"]
                    ],
                },
            }

        except Exception as e:
            print(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            raise

    def save_to_database(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å‡¦ç†ï¼ˆfactorå½¢å¼ã«åˆã‚ã›ã¦ä¿®æ­£ï¼‰"""
        try:
            print("=== PCA ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–‹å§‹ ===")

            # åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
            if hasattr(super(), "save_to_database"):
                session_id = super().save_to_database(
                    db,
                    session_name,
                    description,
                    tags,
                    user_id,
                    file,
                    csv_text,
                    df,
                    results,
                    plot_base64,
                )
            else:
                # åŸºåº•ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ç›´æ¥ä¿å­˜
                session_id = self._save_session_directly(
                    db,
                    session_name,
                    description,
                    tags,
                    user_id,
                    file,
                    csv_text,
                    df,
                    results,
                    plot_base64,
                )

            # PCAç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¿å­˜
            self._save_pca_specific_data(db, session_id, results)

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            self._save_coordinates_data(db, session_id, df, results)

            return session_id

        except Exception as e:
            print(f"âŒ PCA ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return 0

    def _save_pca_specific_data(
        self, db: Session, session_id: int, results: Dict[str, Any]
    ):
        """PCAç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from models import AnalysisMetadata

            # ä¸»æˆåˆ†è² è·é‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if "loadings" in results:
                pca_metadata = {
                    "loadings": results["loadings"],
                    "feature_names": results.get("feature_names", []),
                    "sample_names": results.get("sample_names", []),
                    "n_components": results.get("n_components", 0),
                    "standardized": results.get("standardized", True),
                }

                metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="pca_loadings",
                    metadata_content=pca_metadata,
                )
                db.add(metadata)

            # KMOçµ±è¨ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if "kmo" in results:
                kmo_metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="pca_statistics",
                    metadata_content={
                        "kmo": results["kmo"],
                        "determinant": results.get("determinant", 0),
                        "total_inertia": results.get("total_inertia", 0),
                    },
                )
                db.add(kmo_metadata)

            db.commit()

        except Exception as e:
            print(f"PCAç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_session_directly(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        """åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ç›´æ¥ä¿å­˜"""
        try:
            from models import AnalysisSession, VisualizationData, SessionTag

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜
            session = AnalysisSession(
                session_name=session_name,
                description=description,
                analysis_type=self.get_analysis_type(),
                filename=file.filename,
                csv_content=csv_text,
                user_id=user_id,
                row_count=df.shape[0],
                column_count=df.shape[1],
                # PCAç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                dimensions_count=results.get("n_components", 0),
                dimension_1_contribution=(
                    results.get("explained_variance_ratio", [0])[0]
                    if results.get("explained_variance_ratio")
                    else 0
                ),
                dimension_2_contribution=(
                    results.get("explained_variance_ratio", [0, 0])[1]
                    if len(results.get("explained_variance_ratio", [])) > 1
                    else 0
                ),
                standardized=results.get("standardized", False),
            )

            db.add(session)
            db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚
            session_id = session.session_id

            # ã‚¿ã‚°ã‚’ä¿å­˜
            if tags:
                for tag in tags:
                    if tag.strip():
                        session_tag = SessionTag(
                            session_id=session_id, tag_name=tag.strip()
                        )
                        db.add(session_tag)

            # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ä¿å­˜
            if plot_base64:
                visualization = VisualizationData(
                    session_id=session_id,
                    plot_image=plot_base64,
                    plot_width=1600,
                    plot_height=1200,
                )
                db.add(visualization)

            db.commit()
            return session_id

        except Exception as e:
            print(f"âŒ ç›´æ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            db.rollback()
            return 0

    async def get_session_detail(self, session_id: int, db: Session):
        """PCAã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—"""
        try:
            print(f"ğŸ“Š PCA ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

            # åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if hasattr(super(), "get_session_detail"):
                try:
                    base_detail = super().get_session_detail(db, session_id)
                except Exception as e:
                    print(f"âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã®get_session_detailã‚¨ãƒ©ãƒ¼: {e}")
                    base_detail = await self._get_session_detail_directly(
                        db, session_id
                    )
            else:
                print("âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã«get_session_detailãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                base_detail = await self._get_session_detail_directly(db, session_id)

            return base_detail

        except Exception as e:
            print(f"âŒ PCA ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"success": False, "error": str(e)}

    async def _get_session_detail_directly(self, db: Session, session_id: int):
        """PCAã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’ç›´æ¥å–å¾—"""
        try:
            from models import (
                AnalysisSession,
                VisualizationData,
                CoordinatesData,
                EigenvalueData,
                AnalysisMetadata,
            )

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’å–å¾—
            session = (
                db.query(AnalysisSession)
                .filter(AnalysisSession.id == session_id)
                .first()
            )

            if not session:
                return {
                    "success": False,
                    "error": f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                }

            # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            visualization = (
                db.query(VisualizationData)
                .filter(VisualizationData.session_id == session_id)
                .first()
            )

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            coordinates = (
                db.query(CoordinatesData)
                .filter(CoordinatesData.session_id == session_id)
                .all()
            )

            # ä¸»æˆåˆ†å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆobservationï¼‰ã¨ä¸»æˆåˆ†è² è·é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆvariableï¼‰
            component_scores = []
            component_loadings = []
            sample_names = []
            feature_names = []

            for coord in coordinates:
                coord_data = {
                    "name": coord.point_name,
                    "point_name": coord.point_name,
                    "dimension_1": (
                        float(coord.dimension_1)
                        if coord.dimension_1 is not None
                        else 0.0
                    ),
                    "dimension_2": (
                        float(coord.dimension_2)
                        if coord.dimension_2 is not None
                        else 0.0
                    ),
                    "dimension_3": (
                        float(coord.dimension_3)
                        if coord.dimension_3 is not None
                        else 0.0
                    ),
                    "dimension_4": (
                        float(coord.dimension_4)
                        if coord.dimension_4 is not None
                        else 0.0
                    ),
                    "pc_1": (
                        float(coord.dimension_1)
                        if coord.dimension_1 is not None
                        else 0.0
                    ),
                    "pc_2": (
                        float(coord.dimension_2)
                        if coord.dimension_2 is not None
                        else 0.0
                    ),
                    "pc_3": (
                        float(coord.dimension_3)
                        if coord.dimension_3 is not None
                        else 0.0
                    ),
                }

                if coord.point_type == "observation":
                    coord_data["sample_name"] = coord.point_name
                    coord_data["order_index"] = len(component_scores)
                    component_scores.append(coord_data)
                    sample_names.append(coord.point_name)

                elif coord.point_type == "variable":
                    coord_data["variable_name"] = coord.point_name
                    coord_data["order_index"] = len(component_loadings)
                    component_loadings.append(coord_data)
                    feature_names.append(coord.point_name)

            # å›ºæœ‰å€¤ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            eigenvalue_data = (
                db.query(EigenvalueData)
                .filter(EigenvalueData.session_id == session_id)
                .order_by(EigenvalueData.dimension_number)
                .all()
            )

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—
            metadata_entries = (
                db.query(AnalysisMetadata)
                .filter(AnalysisMetadata.session_id == session_id)
                .all()
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’æ§‹ç¯‰
            response_data = {
                "success": True,
                "data": {
                    "session_info": {
                        "session_id": session.id,
                        "session_name": session.session_name,
                        "description": getattr(session, "description", "") or "",
                        "filename": session.original_filename,
                        "row_count": getattr(
                            session, "row_count", len(component_scores)
                        )
                        or len(component_scores),
                        "column_count": getattr(
                            session, "column_count", len(component_loadings)
                        )
                        or len(component_loadings),
                        "dimensions_count": getattr(session, "dimensions_count", 2)
                        or 2,
                        "dimension_1_contribution": (
                            float(getattr(session, "dimension_1_contribution", 0))
                            if getattr(session, "dimension_1_contribution", None)
                            else 0.0
                        ),
                        "dimension_2_contribution": (
                            float(getattr(session, "dimension_2_contribution", 0))
                            if getattr(session, "dimension_2_contribution", None)
                            else 0.0
                        ),
                        "standardized": getattr(session, "standardized", True),
                        "analysis_timestamp": (
                            session.analysis_timestamp.isoformat()
                            if hasattr(session, "analysis_timestamp")
                            and session.analysis_timestamp
                            else None
                        ),
                    },
                    "metadata": {
                        "filename": session.original_filename,
                        "rows": getattr(session, "row_count", len(component_scores))
                        or len(component_scores),
                        "columns": getattr(
                            session, "column_count", len(component_loadings)
                        )
                        or len(component_loadings),
                        "sample_names": sample_names,
                        "feature_names": feature_names,
                    },
                    "analysis_data": {
                        "component_scores": component_scores,
                        "component_loadings": component_loadings,
                    },
                    "coordinates_data": component_scores
                    + component_loadings,  # çµ±åˆã•ã‚ŒãŸåº§æ¨™ãƒ‡ãƒ¼ã‚¿
                    "eigenvalue_data": [
                        {
                            "dimension_number": ev.dimension_number,
                            "eigenvalue": (
                                float(ev.eigenvalue) if ev.eigenvalue else 0.0
                            ),
                            "explained_inertia": (
                                float(ev.explained_inertia)
                                if ev.explained_inertia
                                else 0.0
                            ),
                            "cumulative_inertia": (
                                float(ev.cumulative_inertia)
                                if ev.cumulative_inertia
                                else 0.0
                            ),
                        }
                        for ev in eigenvalue_data
                    ],
                    "metadata_entries": [
                        {
                            "metadata_type": meta.metadata_type,
                            "metadata_content": meta.metadata_content,
                        }
                        for meta in metadata_entries
                    ],
                    "visualization": {
                        "plot_image": (
                            visualization.image_base64 if visualization else None
                        ),
                        "width": visualization.width if visualization else 1600,
                        "height": visualization.height if visualization else 1200,
                    },
                },
            }

            print(f"âœ… PCA ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—å®Œäº†")
            return response_data

        except Exception as e:
            print(f"âŒ PCA ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return {"success": False, "error": str(e)}
