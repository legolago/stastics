from fastapi import APIRouter, File, UploadFile, Form, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from models import (
    get_db,
    AnalysisSession,
    CoordinatesData,
    AnalysisMetadata,
    OriginalData,
)
import pandas as pd
import numpy as np
import io
import csv
from sklearn.preprocessing import StandardScaler

# LightGBMã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import lightgbm as lgb

    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

<<<<<<< HEAD
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

router = APIRouter()
=======
                # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                self._save_coordinates_data(db, session_id, df, results)
            else:
                print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ã«å¤±æ•—: session_id = {session_id}")

            return session_id

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return 0

    def _save_timeseries_specific_data(
        self, db: Session, session_id: int, results: Dict[str, Any]
    ):
        """æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from models import AnalysisMetadata

            print(f"=== æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹ ===")
            print(f"Session ID: {session_id}")

            # session_idãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
            if not session_id or session_id == 0:
                print(f"âŒ ç„¡åŠ¹ãªsession_id: {session_id}")
                return

            # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if "model_metrics" in results:
                metrics_metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="timeseries_metrics",
                    metadata_content={
                        "metrics": results["model_metrics"],
                        "feature_importance": results.get("feature_importance", []),
                        "forecast_parameters": results.get("forecast_parameters", {}),
                        "model_type": results.get("model_type", "lightgbm"),
                        "data_info": results.get("data_info", {}),
                    },
                )
                db.add(metrics_metadata)

            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿è©³ç´°
            if "timeseries_info" in results:
                ts_info_metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="timeseries_info",
                    metadata_content=results["timeseries_info"],
                )
                db.add(ts_info_metadata)

            # ã‚³ãƒŸãƒƒãƒˆå‰ã«flushã§ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            db.flush()
            db.commit()
            print(f"âœ… æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        except Exception as e:
            print(f"æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            try:
                db.rollback()
                print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã—ãŸ")
            except Exception as rollback_error:
                print(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {rollback_error}")

    def _save_session_directly(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        """åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ç›´æ¥ä¿å­˜"""
        try:
            from models import AnalysisSession, VisualizationData, SessionTag

            print(f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³ç›´æ¥ä¿å­˜é–‹å§‹")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜
            session = AnalysisSession(
                session_name=session_name,
                description=description,
                analysis_type=self.get_analysis_type(),
                original_filename=file.filename,
                user_id=user_id,
                row_count=df.shape[0],
                column_count=df.shape[1],
                # æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                dimensions_count=1,  # äºˆæ¸¬å€¤
                dimension_1_contribution=(
                    results.get("model_metrics", {}).get("r2_score", 0)
                    if results.get("model_metrics")
                    else 0
                ),
            )

            db.add(session)
            db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚flush
            session_id = session.id
            print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å®Œäº†: session_id = {session_id}")

            # ã‚¿ã‚°ã‚’ä¿å­˜
            if tags:
                for tag in tags:
                    if tag.strip():
                        session_tag = SessionTag(
                            session_id=session_id, tag_name=tag.strip()
                        )
                        db.add(session_tag)

            # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ä¿å­˜ï¼ˆä¸€æ™‚çš„ã«ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if plot_base64:
                try:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ä¿å­˜ã‚’è©¦è¡Œ
                    visualization = VisualizationData(
                        session_id=session_id,
                        image_base64=plot_base64,
                        image_size=len(plot_base64),
                        width=1400,
                        height=1100,
                    )
                    db.add(visualization)
                    db.flush()  # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã‚’è©¦è¡Œ
                    print("âœ… å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
                except Exception as viz_error:
                    print(f"âš ï¸ å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {viz_error}")
                    # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¦ã‚‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ä¿å­˜ã™ã‚‹
                    # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã«è©³ç´°ã‚’å‡ºåŠ›
                    import traceback

                    print(f"å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:\n{traceback.format_exc()}")
            else:
                print("ãƒ—ãƒ­ãƒƒãƒˆç”»åƒãªã—")

            db.commit()
            print(f"âœ… å…¨ä½“ã®ã‚³ãƒŸãƒƒãƒˆå®Œäº†: session_id = {session_id}")
            return session_id

        except Exception as e:
            print(f"âŒ ç›´æ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            try:
                db.rollback()
                print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã—ãŸ")
            except Exception as rollback_error:
                print(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {rollback_error}")
            return 0

    def _save_coordinates_data(
        self, db: Session, session_id: int, df: pd.DataFrame, results: Dict[str, Any]
    ):
        """æ™‚ç³»åˆ—åˆ†æã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆå®Ÿæ¸¬å€¤ãƒ»äºˆæ¸¬å€¤ãƒ»æ®‹å·®ï¼‰"""
        try:
            from models import CoordinatesData

            print(f"=== æ™‚ç³»åˆ—åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹ ===")
            print(f"Session ID: {session_id}")

            # session_idãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
            if not session_id or session_id == 0:
                print(f"âŒ ç„¡åŠ¹ãªsession_id: {session_id}")
                return

            # å®Ÿæ¸¬å€¤ã‚’ä¿å­˜ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ï¼‰
            actual_values = results.get("actual_values", [])
            if actual_values:
                print(f"å®Ÿæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(actual_values)}ä»¶")
                for i, (timestamp, value) in enumerate(actual_values):
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(timestamp),
                        point_type="train",  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
                        dimension_1=float(value),
                        dimension_2=0.0,  # å®Ÿæ¸¬å€¤ãªã®ã§äºˆæ¸¬èª¤å·®ã¯0
                        dimension_4=float(i),  # æ™‚ç³»åˆ—ã®é †åº
                    )
                    db.add(coord_data)

            # äºˆæ¸¬å€¤ã‚’ä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ï¼‰
            predictions = results.get("predictions", [])
            if predictions:
                print(f"äºˆæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(predictions)}ä»¶")
                for i, (timestamp, pred_value, actual_value) in enumerate(predictions):
                    residual = (
                        actual_value - pred_value if actual_value is not None else 0.0
                    )
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(timestamp),
                        point_type="test",  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
                        dimension_1=float(pred_value),
                        dimension_2=float(residual),
                        dimension_3=(
                            float(actual_value) if actual_value is not None else None
                        ),
                        dimension_4=float(i),  # æ™‚ç³»åˆ—ã®é †åº
                    )
                    db.add(coord_data)

            # æœªæ¥äºˆæ¸¬å€¤ã‚’ä¿å­˜ï¼ˆå¤‰æ•°ã¨ã—ã¦ï¼‰
            future_predictions = results.get("future_predictions", [])
            if future_predictions:
                print(f"æœªæ¥äºˆæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(future_predictions)}ä»¶")
                for i, (timestamp, pred_value) in enumerate(future_predictions):
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(timestamp),
                        point_type="variable",  # å¤‰æ•°ã¨ã—ã¦ä¿å­˜
                        dimension_1=float(pred_value),
                        dimension_2=0.0,  # æœªæ¥ãªã®ã§æ®‹å·®ã¯ä¸æ˜
                        dimension_4=float(len(predictions) + i),  # æ™‚ç³»åˆ—ã®é †åº
                    )
                    db.add(coord_data)

            # ã‚³ãƒŸãƒƒãƒˆå‰ã«flushã§ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            db.flush()
            db.commit()
            print(f"âœ… æ™‚ç³»åˆ—åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            try:
                db.rollback()
                print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã—ãŸ")
            except Exception as rollback_error:
                print(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {rollback_error}")

    def analyze(
        self,
        df: pd.DataFrame,
        target_column: str,
        date_column: Optional[str] = None,
        feature_columns: Optional[List[str]] = None,
        forecast_periods: int = 30,
        test_size: float = 0.2,
        **kwargs,
    ) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œ"""
        try:
            print(f"=== æ™‚ç³»åˆ—åˆ†æé–‹å§‹ ===")
            print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:\n{df}")
            print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
            print(f"ç›®çš„å¤‰æ•°: {target_column}, æ—¥ä»˜åˆ—: {date_column}")
            print(f"äºˆæ¸¬æœŸé–“: {forecast_periods}, ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {test_size}")

            # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å‰å‡¦ç†
            df_processed = self._preprocess_timeseries_data(
                df, target_column, date_column, feature_columns
            )
            print(f"å‰å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿:\n{df_processed}")
>>>>>>> parent of 4e9bd04... 20250612_1355


<<<<<<< HEAD
class SimpleTimeSeriesAnalyzer:
    """ç°¡ç•¥åŒ–ã•ã‚ŒãŸæ™‚ç³»åˆ—åˆ†æã‚¯ãƒ©ã‚¹"""
=======
            print(f"åˆ†æçµæœ: {list(results.keys())}")
            return results

        except Exception as e:
            print(f"æ™‚ç³»åˆ—åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
            raise

    def _preprocess_timeseries_data(
        self,
        df: pd.DataFrame,
        target_column: str,
        date_column: Optional[str],
        feature_columns: Optional[List[str]],
    ) -> pd.DataFrame:
        """æ™‚ç³»åˆ—åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        df_clean = df.copy()

        # æ—¥ä»˜åˆ—ã®å‡¦ç†
        if date_column and date_column in df_clean.columns:
            df_clean[date_column] = pd.to_datetime(df_clean[date_column])
            df_clean = df_clean.sort_values(date_column)
            df_clean.set_index(date_column, inplace=True)
        else:
            # æ—¥ä»˜åˆ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨
            print("æ—¥ä»˜åˆ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ™‚ç³»åˆ—ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚")
>>>>>>> parent of 4e9bd04... 20250612_1355

    def _create_features(self, df, target_column, date_column, feature_columns=None):
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        print("=== ç‰¹å¾´é‡ä½œæˆé–‹å§‹ ===")

        df_features = df.copy()

        # æ—¥ä»˜åˆ—ã‚’ datetime ã«å¤‰æ›
        if date_column in df_features.columns:
            df_features[date_column] = pd.to_datetime(df_features[date_column])
            df_features = df_features.sort_values(date_column)
            df_features = df_features.reset_index(drop=True)

<<<<<<< HEAD
        # å…ƒã®ç‰¹å¾´é‡ã‚’è¿½åŠ 
        feature_cols = []
        if feature_columns:
            for col in feature_columns:
                if col in df_features.columns and col != target_column:
                    feature_cols.append(col)
                    print(f"å…ƒã®ç‰¹å¾´é‡ã‚’è¿½åŠ : {col}")

        # ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ
        for lag in [1, 3, 7]:
            if len(df_features) > lag:
                lag_col = f"{target_column}_lag_{lag}"
                df_features[lag_col] = df_features[target_column].shift(lag)
                feature_cols.append(lag_col)
                print(f"ãƒ©ã‚°ç‰¹å¾´é‡ä½œæˆ: {lag_col}")

        # ç§»å‹•å¹³å‡ç‰¹å¾´é‡ã‚’ä½œæˆ
        for window in [3, 7]:
            if len(df_features) >= window:
                ma_col = f"{target_column}_ma_{window}"
                df_features[ma_col] = (
                    df_features[target_column]
                    .rolling(window=window, min_periods=1)
                    .mean()
=======
        # ç‰¹å¾´é‡ã®é¸æŠ
        if feature_columns:
            available_features = [
                col
                for col in feature_columns
                if col in numeric_columns and col != target_column
            ]
        else:
            available_features = [
                col for col in numeric_columns if col != target_column
            ]

        # æ¬ æå€¤ã®å‡¦ç†
        df_clean = df_clean.dropna(subset=[target_column])
        if df_clean.empty:
            raise ValueError("ç›®çš„å¤‰æ•°ã®æ¬ æå€¤ã‚’é™¤å»ã—ãŸçµæœã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸ")

        print(f"å‰å‡¦ç†å®Œäº†: {df.shape} -> {df_clean.shape}")
        print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {available_features}")
        return df_clean

    def _compute_timeseries_analysis(
        self,
        df: pd.DataFrame,
        target_column: str,
        forecast_periods: int,
        test_size: float,
        **kwargs,
    ) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æã®è¨ˆç®—"""
        try:
            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            feature_columns = [col for col in numeric_columns if col != target_column]

            # æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ
            df_features = self._create_time_features(df, target_column, feature_columns)

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            split_index = int(len(df_features) * (1 - test_size))
            train_data = df_features.iloc[:split_index]
            test_data = df_features.iloc[split_index:]

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            if LIGHTGBM_AVAILABLE:
                results = self._train_lightgbm_model(
                    train_data, test_data, target_column
>>>>>>> parent of 4e9bd04... 20250612_1355
                )
                feature_cols.append(ma_col)
                print(f"ç§»å‹•å¹³å‡ç‰¹å¾´é‡ä½œæˆ: {ma_col}")

        # å·®åˆ†ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ£ãƒ—ãƒãƒ£ã®ãŸã‚ï¼‰
        if len(df_features) > 1:
            diff_col = f"{target_column}_diff_1"
            df_features[diff_col] = df_features[target_column].diff()
            feature_cols.append(diff_col)
            print(f"å·®åˆ†ç‰¹å¾´é‡ä½œæˆ: {diff_col}")

        # æ—¥ä»˜ç‰¹å¾´é‡ã‚’ä½œæˆï¼ˆåˆ†æ•£ãŒ0ã§ãªã„å ´åˆã®ã¿ï¼‰
        if date_column in df_features.columns:
            month_vals = df_features[date_column].dt.month
            quarter_vals = df_features[date_column].dt.quarter
            dow_vals = df_features[date_column].dt.dayofweek

            # åˆ†æ•£ãƒã‚§ãƒƒã‚¯
            if month_vals.var() > 0:
                df_features["month"] = month_vals
                feature_cols.append("month")
                print(f"æ—¥ä»˜ç‰¹å¾´é‡è¿½åŠ : month (åˆ†æ•£={month_vals.var():.4f})")
            else:
                print("æœˆç‰¹å¾´é‡ã‚’ã‚¹ã‚­ãƒƒãƒ—: åˆ†æ•£ãŒ0")

            if quarter_vals.var() > 0:
                df_features["quarter"] = quarter_vals
                feature_cols.append("quarter")
                print(f"æ—¥ä»˜ç‰¹å¾´é‡è¿½åŠ : quarter (åˆ†æ•£={quarter_vals.var():.4f})")
            else:
                print("å››åŠæœŸç‰¹å¾´é‡ã‚’ã‚¹ã‚­ãƒƒãƒ—: åˆ†æ•£ãŒ0")

<<<<<<< HEAD
            if dow_vals.var() > 0:
                df_features["day_of_week"] = dow_vals
                feature_cols.append("day_of_week")
                print(f"æ—¥ä»˜ç‰¹å¾´é‡è¿½åŠ : day_of_week (åˆ†æ•£={dow_vals.var():.4f})")
            else:
                print("æ›œæ—¥ç‰¹å¾´é‡ã‚’ã‚¹ã‚­ãƒƒãƒ—: åˆ†æ•£ãŒ0")
=======
            # çµæœã®çµ±åˆ
            results.update(
                {
                    "forecast_periods": forecast_periods,
                    "test_size": test_size,
                    "data_info": {
                        "total_samples": len(df_features),
                        "train_samples": len(train_data),
                        "test_samples": len(test_data),
                        "feature_count": len(feature_columns),
                        "target_column": target_column,
                        "feature_columns": feature_columns,
                    },
                    "future_predictions": future_predictions,
                    "timeseries_info": {
                        "start_date": (
                            str(df.index[0])
                            if hasattr(df.index[0], "strftime")
                            else str(df.index[0])
                        ),
                        "end_date": (
                            str(df.index[-1])
                            if hasattr(df.index[-1], "strftime")
                            else str(df.index[-1])
                        ),
                        "frequency": self._infer_frequency(df.index),
                        "trend": self._analyze_trend(df[target_column]),
                    },
                }
            )
>>>>>>> parent of 4e9bd04... 20250612_1355

        # æ—¥ä»˜åˆ—ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ç‰¹å¾´é‡ã‹ã‚‰é™¤å¤–
        feature_cols = [
            col for col in feature_cols if col != date_column and col != target_column
        ]

        # å­˜åœ¨ã—ãªã„åˆ—ã‚’é™¤å¤–
        feature_cols = [col for col in feature_cols if col in df_features.columns]

        print(f"æœ€çµ‚çš„ãªç‰¹å¾´é‡: {feature_cols}")

<<<<<<< HEAD
        # NaNå€¤ã‚’å‰Šé™¤
        initial_rows = len(df_features)
        df_features = df_features.dropna()
        final_rows = len(df_features)
        print(f"NaNå‰Šé™¤å¾Œ: {initial_rows} -> {final_rows} è¡Œ")
=======
    def _create_time_features(
        self, df: pd.DataFrame, target_column: str, feature_columns: List[str]
    ) -> pd.DataFrame:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        df_features = df.copy()
>>>>>>> parent of 4e9bd04... 20250612_1355

        # æ•°å€¤å‹ã«å¤‰æ›
        for col in feature_cols:
            if col in df_features.columns:
                df_features[col] = pd.to_numeric(df_features[col], errors="coerce")

<<<<<<< HEAD
        df_features = df_features.dropna()

        # ç‰¹å¾´é‡ã®åˆ†æ•£ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç·©ã„é–¾å€¤ã«å¤‰æ›´ï¼‰
        print("=== ç‰¹å¾´é‡ã®åˆ†æ•£ãƒã‚§ãƒƒã‚¯ ===")
        filtered_features = []
        for col in feature_cols:
            if col in df_features.columns:
                variance = df_features[col].var()
                mean_val = df_features[col].mean()

                # åˆ†æ•£ã®é–¾å€¤ã‚’ç·©å’Œï¼ˆ1e-12æœªæº€ã®ã¿é™¤å¤–ï¼‰
                if variance > 1e-12:
                    filtered_features.append(col)
                    print(f"{col}: mean={mean_val:.4f}, var={variance:.8f} âœ“")
                else:
                    print(
                        f"{col}: mean={mean_val:.4f}, var={variance:.8f} âœ— (åˆ†æ•£ãŒå°ã•ã™ãã‚‹ãŸã‚é™¤å¤–)"
                    )
=======
        # æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ï¼ˆæ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆï¼‰
        if hasattr(df.index, "month"):
            df_features["month"] = df.index.month
            df_features["quarter"] = df.index.quarter
            df_features["day_of_week"] = df.index.dayofweek
            df_features["day_of_year"] = df.index.dayofyear

        # æ—¢å­˜ã®ç‰¹å¾´é‡ã‚’è¿½åŠ 
        for col in feature_columns:
            if col in df.columns:
                df_features[col] = df[col]

        # æ¬ æå€¤ã‚’é™¤å»
        df_features = df_features.dropna()

        return df_features

    def _train_lightgbm_model(
        self, train_data: pd.DataFrame, test_data: pd.DataFrame, target_column: str
    ) -> Dict[str, Any]:
        """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
        X_train = train_data.drop(columns=[target_column])
        y_train = train_data[target_column]
        X_test = test_data.drop(columns=[target_column])
        y_test = test_data[target_column]

        # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        train_dataset = lgb.Dataset(X_train, label=y_train)
        valid_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        params = {
            "objective": "regression",
            "metric": "rmse",
            "boosting_type": "gbdt",
            "num_leaves": 31,
            "learning_rate": 0.05,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1,
        }

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = lgb.train(
            params,
            train_dataset,
            valid_sets=[valid_dataset],
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )
>>>>>>> parent of 4e9bd04... 20250612_1355

        feature_cols = filtered_features
        print(f"åˆ†æ•£ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ç‰¹å¾´é‡: {feature_cols}")
        print(f"ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(feature_cols)}å€‹ã®ç‰¹å¾´é‡, {len(df_features)}è¡Œ")
        return df_features, feature_cols


@router.post("/timeseries/features")
async def extract_features_only(
    file: UploadFile = File(...),
    target_column: str = Form(...),
    date_column: str = Form(...),
    feature_columns: str = Form(None),
):
    """ç‰¹å¾´é‡ã®ã¿ã‚’æŠ½å‡ºã—ã¦è¿”ã™ï¼ˆãƒ¢ãƒ‡ãƒ«è¨“ç·´ãªã—ï¼‰"""
    try:
        print("=== ç‰¹å¾´é‡æŠ½å‡ºAPIé–‹å§‹ ===")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        content = await file.read()
        df = pd.read_csv(io.StringIO(content.decode("utf-8")))

        # å…ƒã®ç‰¹å¾´é‡åˆ—ã‚’è§£æ
        original_features = []
        if feature_columns:
            original_features = [col.strip() for col in feature_columns.split(",")]

        # æ™‚ç³»åˆ—åˆ†æå™¨ã‚’åˆæœŸåŒ–
        analyzer = SimpleTimeSeriesAnalyzer()

        # ç‰¹å¾´é‡ã‚’ä½œæˆ
        df_features, feature_cols = analyzer._create_features(
            df, target_column, date_column, original_features
        )

        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ï¼ˆNumPyå‹å¤‰æ›ä»˜ãï¼‰
        feature_stats = {}
        for col in feature_cols:
            if col in df_features.columns:
                values = df_features[col]
                feature_stats[col] = {
                    "mean": float(values.mean()),
                    "std": float(values.std()),
                    "min": float(values.min()),
                    "max": float(values.max()),
                    "variance": float(values.var()),
                    "non_zero_count": int((values != 0).sum()),
                    "total_count": int(len(values)),
                    "non_zero_ratio": float((values != 0).sum() / len(values)),
                }

        response = {
            "success": True,
            "data": {
                "original_shape": [int(x) for x in df.shape],
                "processed_shape": [int(x) for x in df_features.shape],
                "target_column": target_column,
                "date_column": date_column,
                "feature_columns": feature_cols,
                "feature_statistics": feature_stats,
                "data_info": {
                    "total_samples": int(len(df_features)),
                    "feature_count": int(len(feature_cols)),
                    "original_columns": list(df.columns),
                    "processed_columns": list(df_features.columns),
                    "rows_removed": int(len(df) - len(df_features)),
                },
                "sample_data": df_features[feature_cols + [target_column]]
                .head()
                .to_dict("records"),
            },
        }

        print("=== ç‰¹å¾´é‡æŠ½å‡ºAPIå®Œäº† ===")
        return response

    except Exception as e:
        print(f"ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        return {"success": False, "error": str(e)}


@router.post("/timeseries/analyze")
async def analyze_timeseries(
    file: UploadFile = File(...),
    session_name: str = Form(...),
    user_id: str = Form(None),
    target_column: str = Form(...),
    date_column: str = Form(...),
    feature_columns: str = Form(None),
    forecast_periods: int = Form(5),
    test_size: float = Form(0.2),
):
    """æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        print("=== æ™‚ç³»åˆ—åˆ†æAPIé–‹å§‹ ===")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        content = await file.read()
        df = pd.read_csv(io.StringIO(content.decode("utf-8")))

        # ç‰¹å¾´é‡åˆ—ã‚’è§£æ
        feature_list = []
        if feature_columns:
            feature_list = [col.strip() for col in feature_columns.split(",")]

        # åˆ†æå™¨ã‚’åˆæœŸåŒ–
        analyzer = SimpleTimeSeriesAnalyzer()

        # ç‰¹å¾´é‡ä½œæˆ
        df_features, feature_cols = analyzer._create_features(
            df, target_column, date_column, feature_list
        )

        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if len(df_features) < 10:
            return {
                "success": False,
                "error": f"ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {len(df_features)}è¡Œï¼ˆæœ€ä½10è¡Œå¿…è¦ï¼‰",
            }

        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
        X_features = df_features[feature_cols].copy()
        y_target = df_features[target_column].copy()

        print(f"=== ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ç¢ºèª ===")
        for col in feature_cols:
            values = X_features[col]
            print(
                f"{col}: mean={values.mean():.4f}, std={values.std():.4f}, var={values.var():.8f}"
            )

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        split_idx = int(len(X_features) * (1 - test_size))
        X_train = X_features.iloc[:split_idx]
        X_test = X_features.iloc[split_idx:]
        y_train = y_target.iloc[:split_idx]
        y_test = y_target.iloc[split_idx:]

        print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: è¨“ç·´={len(X_train)}è¡Œ, ãƒ†ã‚¹ãƒˆ={len(X_test)}è¡Œ")

        # ç‰¹å¾´é‡ã®æ­£è¦åŒ–ï¼ˆStandardScalerä½¿ç”¨ï¼‰
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )

        print("=== æ­£è¦åŒ–å¾Œã®ç‰¹å¾´é‡ç¢ºèª ===")
        for col in feature_cols:
            values = X_train_scaled[col]
            print(f"{col}: mean={values.mean():.4f}, std={values.std():.4f}")

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        if LIGHTGBM_AVAILABLE and len(X_train) > 5:
            print("=== LightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹ ===")

            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆã‚ˆã‚Šç©æ¥µçš„ãªè¨­å®šï¼‰
            data_size = len(X_train)

            if data_size < 20:
                # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨
                params = {
                    "objective": "regression",
                    "metric": "rmse",
                    "verbose": -1,
                    "random_state": 42,
                    "n_estimators": 100,  # å¢—åŠ 
                    "max_depth": 6,  # å¢—åŠ 
                    "learning_rate": 0.1,
                    "num_leaves": 31,  # å¢—åŠ 
                    "min_child_samples": 1,  # æ¸›å°‘
                    "feature_fraction": 1.0,  # ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨
                    "bagging_fraction": 1.0,  # ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨
                    "bagging_freq": 0,  # ãƒã‚®ãƒ³ã‚°ç„¡åŠ¹
                    "reg_alpha": 0.0,  # æ­£å‰‡åŒ–ã‚’ç„¡åŠ¹
                    "reg_lambda": 0.0,  # æ­£å‰‡åŒ–ã‚’ç„¡åŠ¹
                    "min_split_gain": 0.0,  # åˆ†å‰²åˆ¶é™ã‚’ç·©å’Œ
                    "force_col_wise": True,  # ã‚«ãƒ©ãƒ å˜ä½ã§å‡¦ç†
                }
            elif data_size < 100:
                # ä¸­ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨
                params = {
                    "objective": "regression",
                    "metric": "rmse",
                    "verbose": -1,
                    "random_state": 42,
                    "n_estimators": 150,
                    "max_depth": 8,
                    "learning_rate": 0.1,
                    "num_leaves": 63,
                    "min_child_samples": 2,
                    "feature_fraction": 0.9,
                    "bagging_fraction": 0.9,
                    "bagging_freq": 1,
                    "reg_alpha": 0.01,
                    "reg_lambda": 0.01,
                    "force_col_wise": True,
                }
            else:
                # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨
                params = {
                    "objective": "regression",
                    "metric": "rmse",
                    "verbose": -1,
                    "random_state": 42,
                    "n_estimators": 200,
                    "max_depth": 10,
                    "learning_rate": 0.1,
                    "num_leaves": 127,
                    "min_child_samples": 5,
                    "feature_fraction": 0.8,
                    "bagging_fraction": 0.8,
                    "bagging_freq": 1,
                    "reg_alpha": 0.05,
                    "reg_lambda": 0.05,
                    "force_col_wise": True,
                }

            print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data_size}, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé©ç”¨")

            model = lgb.LGBMRegressor(**params)
            model.fit(X_train_scaled, y_train)
            train_pred = model.predict(X_train_scaled)
            test_pred = model.predict(X_test_scaled)

            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆè¤‡æ•°ã®ç¨®é¡ã‚’å–å¾—ï¼‰
            importance_gain = model.feature_importances_

            print(f"=== ç‰¹å¾´é‡é‡è¦åº¦è©³ç´° ===")
            feature_importance = []
            for i, col in enumerate(feature_cols):
                importance = float(importance_gain[i])
                feature_importance.append((col, importance))
                print(f"{col}: {importance:.6f}")

            # é‡è¦åº¦ãŒ0ã®å ´åˆã®å¯¾å‡¦
            total_importance = sum(imp[1] for imp in feature_importance)
            if total_importance == 0:
                print("âš ï¸ ã™ã¹ã¦ã®ç‰¹å¾´é‡é‡è¦åº¦ãŒ0ã§ã™ã€‚ç·šå½¢å›å¸°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                # ç·šå½¢å›å¸°ã‚’ä½¿ç”¨
                model = LinearRegression()
                model.fit(X_train_scaled, y_train)
                train_pred = model.predict(X_train_scaled)
                test_pred = model.predict(X_test_scaled)

                # ä¿‚æ•°ã®çµ¶å¯¾å€¤ã‚’é‡è¦åº¦ã¨ã—ã¦ä½¿ç”¨
                coeffs = np.abs(model.coef_)
                feature_importance = []
                for i, col in enumerate(feature_cols):
                    importance = float(coeffs[i])
                    feature_importance.append((col, importance))
                    print(f"{col} (ä¿‚æ•°): {importance:.6f}")

                model_type = "linear_regression"
            else:
                feature_importance.sort(key=lambda x: x[1], reverse=True)
                model_type = "lightgbm"

        else:
            print("=== ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹ ===")
            model = LinearRegression()
            model.fit(X_train_scaled, y_train)
            train_pred = model.predict(X_train_scaled)
            test_pred = model.predict(X_test_scaled)

            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¿‚æ•°ã®çµ¶å¯¾å€¤ï¼‰
            coeffs = np.abs(model.coef_)

            print(f"=== ç‰¹å¾´é‡é‡è¦åº¦è©³ç´° ===")
            feature_importance = []
            for i, col in enumerate(feature_cols):
                importance = float(coeffs[i])
                feature_importance.append((col, importance))
                print(f"{col}: {importance:.6f}")

            feature_importance.sort(key=lambda x: x[1], reverse=True)
            model_type = "linear_regression"

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        def calculate_metrics(y_true, y_pred):
            y_true = np.array(y_true)
            y_pred = np.array(y_pred)

            rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
            mae = float(mean_absolute_error(y_true, y_pred))
            r2 = float(r2_score(y_true, y_pred))

            # MAPEè¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
            mask = y_true != 0
            if mask.sum() > 0:
                mape = float(
                    np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
                )
            else:
                mape = 0.0

            return {
                "rmse": rmse,
                "mae": mae,
                "r2": r2,
                "mape": mape,
            }

        train_metrics = calculate_metrics(y_train, train_pred)
        test_metrics = calculate_metrics(y_test, test_pred)

        print(f"=== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ===")
        print(
            f"è¨“ç·´ - RMSE: {train_metrics['rmse']:.4f}, RÂ²: {train_metrics['r2']:.4f}"
        )
        print(
            f"ãƒ†ã‚¹ãƒˆ - RMSE: {test_metrics['rmse']:.4f}, RÂ²: {test_metrics['r2']:.4f}"
        )

        # éå­¦ç¿’ãƒã‚§ãƒƒã‚¯
        overfitting_risk = "low"
        if train_metrics["r2"] > 0.9 and test_metrics["r2"] < 0.5:
            overfitting_risk = "high"
        elif train_metrics["r2"] > 0.8 and test_metrics["r2"] < 0.3:
            overfitting_risk = "medium"

        if overfitting_risk != "low":
            print(f"âš ï¸ éå­¦ç¿’ãƒªã‚¹ã‚¯: {overfitting_risk}")

        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        predictions = []
        for i, (idx, pred, actual) in enumerate(zip(X_test.index, test_pred, y_test)):
            predictions.append(
                {
                    "timestamp": str(idx),
                    "predicted_value": float(pred),
                    "actual_value": float(actual),
                    "residual": float(actual - pred),
                    "order_index": int(i),
                }
            )

        # å®Ÿæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        actual_values = []
        for i, (idx, value) in enumerate(y_train.items()):
            actual_values.append(
                {
                    "timestamp": str(idx),
                    "value": float(value),
                    "order_index": int(i),
                }
            )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        response = {
            "success": True,
            "session_id": None,  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã¯å¾Œã§å®Ÿè£…
            "analysis_type": "timeseries",
            "data": {
                "model_type": model_type,
                "target_column": target_column,
                "feature_columns": feature_cols,
                "forecast_periods": int(forecast_periods),
                "model_metrics": {
                    "train": train_metrics,
                    "test": test_metrics,
                    "r2_score": test_metrics["r2"],
                    "rmse": test_metrics["rmse"],
                    "mae": test_metrics["mae"],
                    "overfitting_risk": overfitting_risk,
                },
                "feature_importance": feature_importance,
                "predictions": predictions,
                "actual_values": actual_values,
                "future_predictions": [],
                "data_info": {
                    "total_samples": int(len(X_features)),
                    "train_samples": int(len(X_train)),
                    "test_samples": int(len(X_test)),
                    "feature_count": int(len(feature_cols)),
                    "target_column": target_column,
                    "feature_columns": feature_cols,
                    "normalization_applied": True,
                },
            },
            "metadata": {
                "session_name": session_name,
                "filename": file.filename,
                "rows": int(df.shape[0]),
                "columns": int(df.shape[1]),
                "target_column": target_column,
                "feature_columns": feature_cols,
            },
        }

        print("=== æ™‚ç³»åˆ—åˆ†æAPIå‡¦ç†å®Œäº† ===")
        return response

    except Exception as e:
        print(f"æ™‚ç³»åˆ—åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        return {"success": False, "error": str(e)}


@router.get("/timeseries/sessions")
async def get_timeseries_sessions(
    analysis_type: str = "timeseries",
    user_id: str = None,
    limit: int = 50,
    offset: int = 0,
    db: Session = Depends(get_db),
):
    """æ™‚ç³»åˆ—åˆ†æã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—"""
    try:
        query = db.query(AnalysisSession).filter(
            AnalysisSession.analysis_type == analysis_type
        )

        if user_id:
            query = query.filter(AnalysisSession.user_id == user_id)

        sessions = (
            query.order_by(AnalysisSession.analysis_timestamp.desc())
            .offset(offset)
            .limit(limit)
            .all()
        )

        session_list = []
        for session in sessions:
            session_data = {
                "id": int(session.id),
                "session_name": session.session_name,
                "analysis_type": session.analysis_type,
                "original_filename": session.original_filename,
                "row_count": int(session.row_count) if session.row_count else 0,
                "column_count": (
                    int(session.column_count) if session.column_count else 0
                ),
                "analysis_timestamp": (
                    session.analysis_timestamp.isoformat()
                    if session.analysis_timestamp
                    else None
                ),
                "user_id": session.user_id,
            }

            if session.analysis_parameters:
                session_data["parameters"] = session.analysis_parameters

            session_list.append(session_data)

        return {
            "success": True,
            "sessions": session_list,
            "total": int(len(session_list)),
        }

    except Exception as e:
        print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}
