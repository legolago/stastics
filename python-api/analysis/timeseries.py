from typing import Dict, Any, Optional, List, Tuple
import pandas as pd
import numpy as np
import matplotlib

matplotlib.use("Agg")  # GUIç„¡åŠ¹åŒ–

import matplotlib.pyplot as plt
import matplotlib.patheffects as pe
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sqlalchemy.orm import Session
from .base import BaseAnalyzer
import warnings
from datetime import datetime

warnings.filterwarnings("ignore")

# LightGBMã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import lightgbm as lgb

    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("Warning: LightGBM not available, using alternative methods")


class TimeSeriesAnalyzer(BaseAnalyzer):
    """æ™‚ç³»åˆ—åˆ†æã‚¯ãƒ©ã‚¹ï¼ˆLightGBMãƒ™ãƒ¼ã‚¹ï¼‰"""

    def get_analysis_type(self) -> str:
        return "timeseries"

    def save_to_database(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        try:
            print("=== æ™‚ç³»åˆ—åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–‹å§‹ ===")

            # ã¾ãšåŸºæœ¬ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
            session_id = self._save_session_directly(
                db,
                session_name,
                description,
                tags,
                user_id,
                file,
                csv_text,
                df,
                results,
                plot_base64,
            )

            print(f"åŸºæœ¬ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å®Œäº†: session_id = {session_id}")

            if session_id and session_id > 0:
                # æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¿å­˜
                self._save_timeseries_specific_data(db, session_id, results)

                # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                self._save_coordinates_data(db, session_id, df, results)
            else:
                print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ã«å¤±æ•—: session_id = {session_id}")

            return session_id

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return 0

    def _save_timeseries_specific_data(
        self, db: Session, session_id: int, results: Dict[str, Any]
    ):
        """æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from models import AnalysisMetadata

            print(f"=== æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹ ===")
            print(f"Session ID: {session_id}")

            ## ç‰¹å¾´é‡é‡è¦åº¦ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            feature_importance = results.get("feature_importance", [])
            print(f"ğŸ” ä¿å­˜ã™ã‚‹ç‰¹å¾´é‡é‡è¦åº¦æ•°: {len(feature_importance)}")
            if feature_importance:
                print(f"ğŸ” ä¸Šä½5å€‹ã®ç‰¹å¾´é‡é‡è¦åº¦:")
                for feat, imp in feature_importance[:5]:
                    print(f"  {feat}: {imp}")

            # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if "model_metrics" in results:
                metrics_metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="timeseries_metrics",
                    metadata_content={
                        "metrics": results["model_metrics"],
                        "feature_importance": results.get("feature_importance", []),
                        "forecast_parameters": results.get("forecast_parameters", {}),
                        "model_type": results.get("model_type", "lightgbm"),
                        "data_info": results.get("data_info", {}),
                    },
                )
                db.add(metrics_metadata)

            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿è©³ç´°
            if "timeseries_info" in results:
                ts_info_metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="timeseries_info",
                    metadata_content=results["timeseries_info"],
                )
                db.add(ts_info_metadata)

            # ã‚³ãƒŸãƒƒãƒˆå‰ã«flushã§ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            db.flush()
            db.commit()
            print(f"âœ… æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        except Exception as e:
            print(f"æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            try:
                db.rollback()
                print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã—ãŸ")
            except Exception as rollback_error:
                print(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {rollback_error}")

    def _save_session_directly(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        """åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ç›´æ¥ä¿å­˜ï¼ˆDBäº’æ›æ€§å¯¾å¿œç‰ˆï¼‰"""
        try:
            from models import AnalysisSession, VisualizationData

            # ğŸ†• æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from sqlalchemy import text

            print(f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³ç›´æ¥ä¿å­˜é–‹å§‹")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜ï¼ˆanalysis_typeã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
            session = AnalysisSession(
                session_name=session_name,
                description=description,
                analysis_type="timeseries",  # ğŸ†• æ˜ç¤ºçš„ã«æŒ‡å®š
                original_filename=file.filename,
                user_id=user_id,
                row_count=df.shape[0],
                column_count=df.shape[1],
                tags=tags,
                # æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                dimensions_count=1,  # äºˆæ¸¬å€¤
                dimension_1_contribution=(
                    results.get("model_metrics", {}).get("r2_score", 0)
                    if results.get("model_metrics")
                    else 0
                ),
            )

            db.add(session)
            db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚flush
            session_id = session.id
            print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å®Œäº†: session_id = {session_id}")

            # ã‚¿ã‚°ã‚’ä¿å­˜ï¼ˆæ—¢å­˜ã®ã‚«ãƒ©ãƒ å'tag'ã‚’ä½¿ç”¨ï¼‰
            if tags and session_id:
                try:
                    for tag in tags:
                        if tag.strip():
                            db.execute(
                                text(
                                    "INSERT INTO session_tags (session_id, tag, created_at) VALUES (:session_id, :tag, :created_at)"
                                ),
                                {
                                    "session_id": session_id,
                                    "tag": tag.strip(),
                                    "created_at": datetime.utcnow(),
                                },
                            )
                    print(f"âœ… session_tagsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚‚ã‚¿ã‚°ä¿å­˜å®Œäº†: {len(tags)}ä»¶")
                except Exception as tag_error:
                    print(
                        f"âš ï¸ session_tagsãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ã‚¿ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {tag_error}"
                    )
                    # ã‚¿ã‚°ä¿å­˜ã«å¤±æ•—ã—ã¦ã‚‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ä¿å­˜ã™ã‚‹

            # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ä¿å­˜
            if plot_base64:
                try:
                    visualization = VisualizationData(
                        session_id=session_id,
                        image_base64=plot_base64,
                        image_size=len(plot_base64),
                        width=1400,
                        height=1100,
                    )
                    db.add(visualization)
                    db.flush()
                    print("âœ… å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
                except Exception as viz_error:
                    print(f"âš ï¸ å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {viz_error}")

            db.commit()
            print(f"âœ… å…¨ä½“ã®ã‚³ãƒŸãƒƒãƒˆå®Œäº†: session_id = {session_id}")
            return session_id

        except Exception as e:
            print(f"âŒ ç›´æ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            try:
                db.rollback()
                print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã—ãŸ")
            except Exception as rollback_error:
                print(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {rollback_error}")
            return 0

    def _save_coordinates_data(
        self, db: Session, session_id: int, df: pd.DataFrame, results: Dict[str, Any]
    ):
        """æ™‚ç³»åˆ—åˆ†æã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆå®Ÿæ¸¬å€¤ãƒ»äºˆæ¸¬å€¤ãƒ»æ®‹å·®ï¼‰"""
        try:
            from models import CoordinatesData

            print(f"=== æ™‚ç³»åˆ—åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹ ===")
            print(f"Session ID: {session_id}")

            # session_idãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
            if not session_id or session_id == 0:
                print(f"âŒ ç„¡åŠ¹ãªsession_id: {session_id}")
                return

            # å®Ÿæ¸¬å€¤ã‚’ä¿å­˜ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ï¼‰
            actual_values = results.get("actual_values", [])
            if actual_values:
                print(f"å®Ÿæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(actual_values)}ä»¶")
                for i, (timestamp, value) in enumerate(actual_values):
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(timestamp),
                        point_type="train",  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
                        dimension_1=float(value),
                        dimension_2=0.0,  # å®Ÿæ¸¬å€¤ãªã®ã§äºˆæ¸¬èª¤å·®ã¯0
                        dimension_4=float(i),  # æ™‚ç³»åˆ—ã®é †åº
                    )
                    db.add(coord_data)

            # äºˆæ¸¬å€¤ã‚’ä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ï¼‰
            predictions = results.get("predictions", [])
            if predictions:
                print(f"äºˆæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(predictions)}ä»¶")
                for i, (timestamp, pred_value, actual_value) in enumerate(predictions):
                    residual = (
                        actual_value - pred_value if actual_value is not None else 0.0
                    )
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(timestamp),
                        point_type="test",  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
                        dimension_1=float(pred_value),
                        dimension_2=float(residual),
                        dimension_3=(
                            float(actual_value) if actual_value is not None else None
                        ),
                        dimension_4=float(i),  # æ™‚ç³»åˆ—ã®é †åº
                    )
                    db.add(coord_data)

            # æœªæ¥äºˆæ¸¬å€¤ã‚’ä¿å­˜ï¼ˆå¤‰æ•°ã¨ã—ã¦ï¼‰
            future_predictions = results.get("future_predictions", [])
            if future_predictions:
                print(f"æœªæ¥äºˆæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(future_predictions)}ä»¶")
                for i, (timestamp, pred_value) in enumerate(future_predictions):
                    coord_data = CoordinatesData(
                        session_id=session_id,
                        point_name=str(timestamp),
                        point_type="variable",  # å¤‰æ•°ã¨ã—ã¦ä¿å­˜
                        dimension_1=float(pred_value),
                        dimension_2=0.0,  # æœªæ¥ãªã®ã§æ®‹å·®ã¯ä¸æ˜
                        dimension_4=float(len(predictions) + i),  # æ™‚ç³»åˆ—ã®é †åº
                    )
                    db.add(coord_data)

            # ã‚³ãƒŸãƒƒãƒˆå‰ã«flushã§ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            db.flush()
            db.commit()
            print(f"âœ… æ™‚ç³»åˆ—åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            try:
                db.rollback()
                print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã—ãŸ")
            except Exception as rollback_error:
                print(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {rollback_error}")

    def analyze(
        self,
        df: pd.DataFrame,
        target_column: str,
        date_column: Optional[str] = None,
        feature_columns: Optional[List[str]] = None,
        forecast_periods: int = 30,
        test_size: float = 0.2,
        **kwargs,
    ) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æã‚’å®Ÿè¡Œ"""
        try:
            print(f"=== æ™‚ç³»åˆ—åˆ†æé–‹å§‹ ===")
            print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:\n{df}")
            print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
            print(f"ç›®çš„å¤‰æ•°: {target_column}, æ—¥ä»˜åˆ—: {date_column}")
            print(f"äºˆæ¸¬æœŸé–“: {forecast_periods}, ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {test_size}")

            # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å‰å‡¦ç†
            df_processed = self._preprocess_timeseries_data(
                df, target_column, date_column, feature_columns
            )
            print(f"å‰å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿:\n{df_processed}")

            # æ™‚ç³»åˆ—åˆ†æã®è¨ˆç®—
            results = self._compute_timeseries_analysis(
                df_processed, target_column, forecast_periods, test_size, **kwargs
            )

            print(f"åˆ†æçµæœ: {list(results.keys())}")
            return results

        except Exception as e:
            print(f"æ™‚ç³»åˆ—åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
            raise

    def _preprocess_timeseries_data(
        self,
        df: pd.DataFrame,
        target_column: str,
        date_column: Optional[str],
        feature_columns: Optional[List[str]],
    ) -> pd.DataFrame:
        """æ™‚ç³»åˆ—åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        df_clean = df.copy()

        # æ—¥ä»˜åˆ—ã®å‡¦ç†
        if date_column and date_column in df_clean.columns:
            try:
                df_clean[date_column] = pd.to_datetime(df_clean[date_column])
                df_clean = df_clean.sort_values(date_column)
                df_clean.set_index(date_column, inplace=True)
                print(f"âœ… æ—¥ä»˜åˆ— '{date_column}' ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š")
            except Exception as e:
                print(f"âš ï¸ æ—¥ä»˜åˆ—ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™")
        else:
            print("æ—¥ä»˜åˆ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ™‚ç³»åˆ—ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚")

        # ç›®çš„å¤‰æ•°ã®å­˜åœ¨ç¢ºèª
        if target_column not in df_clean.columns:
            raise ValueError(f"ç›®çš„å¤‰æ•° '{target_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
        numeric_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()

        if target_column not in numeric_columns:
            raise ValueError(f"ç›®çš„å¤‰æ•° '{target_column}' ã¯æ•°å€¤å‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

        # ğŸ†• ç‰¹å¾´é‡ã®é¸æŠï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
        if feature_columns:
            available_features = [
                col
                for col in feature_columns
                if col in numeric_columns and col != target_column
            ]
            print(f"ğŸ” æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã®ã†ã¡åˆ©ç”¨å¯èƒ½: {available_features}")

            # åˆ©ç”¨ã§ããªã„ç‰¹å¾´é‡ãŒã‚ã£ãŸå ´åˆã®è­¦å‘Š
            unavailable_features = [
                col
                for col in feature_columns
                if col not in numeric_columns or col not in df_clean.columns
            ]
            if unavailable_features:
                print(f"âš ï¸ åˆ©ç”¨ã§ããªã„ç‰¹å¾´é‡: {unavailable_features}")
        else:
            available_features = [
                col for col in numeric_columns if col != target_column
            ]
            print(f"ğŸ” è‡ªå‹•é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {available_features}")

        # æ¬ æå€¤ã®å‡¦ç†
        df_clean = df_clean.dropna(subset=[target_column])
        if df_clean.empty:
            raise ValueError("ç›®çš„å¤‰æ•°ã®æ¬ æå€¤ã‚’é™¤å»ã—ãŸçµæœã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸ")

        print(f"å‰å‡¦ç†å®Œäº†: {df.shape} -> {df_clean.shape}")
        print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {available_features}")

        # ğŸ†• åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜ï¼ˆå¾Œã§ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰
        df_clean._available_features = available_features

        return df_clean

    def _compute_timeseries_analysis(
        self,
        df: pd.DataFrame,
        target_column: str,
        forecast_periods: int,
        test_size: float,
        **kwargs,
    ) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æã®è¨ˆç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            # ğŸ†• å‰å‡¦ç†ã§ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’å–å¾—
            feature_columns = getattr(df, "_available_features", [])

            # æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ
            df_features = self._create_time_features(df, target_column, feature_columns)

            # ğŸ†• LightGBMç”¨ã®ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª
            print(f"ğŸ” LightGBMæŠ•å…¥å‰ã®ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª:")
            problematic_columns = []
            for col in df_features.columns:
                dtype = df_features[col].dtype
                print(f"  {col}: {dtype}")

                # LightGBMãŒå—ã‘ä»˜ã‘ãªã„ãƒ‡ãƒ¼ã‚¿å‹ã‚’ãƒã‚§ãƒƒã‚¯
                if dtype == "object" or dtype.name.startswith("datetime"):
                    if col != target_column:  # ç›®çš„å¤‰æ•°ä»¥å¤–
                        problematic_columns.append(col)

            # å•é¡Œã®ã‚ã‚‹åˆ—ã‚’å‰Šé™¤
            if problematic_columns:
                print(f"ğŸ—‘ï¸ LightGBMéå¯¾å¿œåˆ—ã‚’å‰Šé™¤: {problematic_columns}")
                df_features = df_features.drop(columns=problematic_columns)

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            split_index = int(len(df_features) * (1 - test_size))
            train_data = df_features.iloc[:split_index]
            test_data = df_features.iloc[split_index:]

            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: train={len(train_data)}, test={len(test_data)}")
            print(f"ğŸ“Š æœ€çµ‚ç‰¹å¾´é‡: {list(df_features.columns)}")

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            if LIGHTGBM_AVAILABLE:
                results = self._train_lightgbm_model(
                    train_data, test_data, target_column
                )
            else:
                results = self._train_alternative_model(
                    train_data, test_data, target_column
                )

            # æœªæ¥äºˆæ¸¬
            future_predictions = self._generate_future_predictions(
                df_features, results["model"], target_column, forecast_periods
            )

            # çµæœã®çµ±åˆ
            results.update(
                {
                    "forecast_periods": forecast_periods,
                    "test_size": test_size,
                    "data_info": {
                        "total_samples": len(df_features),
                        "train_samples": len(train_data),
                        "test_samples": len(test_data),
                        "feature_count": len(df_features.columns) - 1,  # ç›®çš„å¤‰æ•°ã‚’é™¤ã
                        "target_column": target_column,
                        "feature_columns": [
                            col for col in df_features.columns if col != target_column
                        ],
                    },
                    "future_predictions": future_predictions,
                    "timeseries_info": {
                        "start_date": (
                            str(df.index[0])
                            if hasattr(df.index[0], "strftime")
                            else str(df.index[0])
                        ),
                        "end_date": (
                            str(df.index[-1])
                            if hasattr(df.index[-1], "strftime")
                            else str(df.index[-1])
                        ),
                        "frequency": self._infer_frequency(df.index),
                        "trend": self._analyze_trend(df[target_column]),
                    },
                }
            )

            return results

        except Exception as e:
            print(f"æ™‚ç³»åˆ—åˆ†æè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            raise

    def _create_time_features(
        self, df: pd.DataFrame, target_column: str, feature_columns: List[str]
    ) -> pd.DataFrame:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿å‹ä¿®æ­£ç‰ˆï¼‰"""
        df_features = df.copy()

        # ãƒ©ã‚°ç‰¹å¾´é‡
        for lag in [1, 2, 3, 5, 7, 14, 21, 30]:
            if len(df) > lag:
                df_features[f"{target_column}_lag_{lag}"] = df_features[
                    target_column
                ].shift(lag)

        # ç§»å‹•å¹³å‡ç‰¹å¾´é‡
        for window in [3, 5, 7, 14, 21, 30]:
            if len(df) > window:
                df_features[f"{target_column}_ma_{window}"] = (
                    df_features[target_column].rolling(window=window).mean()
                )
        # å·®åˆ†ç‰¹å¾´é‡
        for diff in [1, 7, 30]:
            if len(df) > diff:
                df_features[f"{target_column}_diff_{diff}"] = df_features[
                    target_column
                ].diff(diff)

        # æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ï¼ˆæ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆï¼‰
        if hasattr(df.index, "month"):
            df_features["month"] = df.index.month
            df_features["quarter"] = df.index.quarter
            df_features["day_of_week"] = df.index.dayofweek
            df_features["day_of_year"] = df.index.dayofyear
            df_features["week_of_year"] = df.index.isocalendar().week
            df_features["is_weekend"] = (df.index.dayofweek >= 5).astype(int)
            df_features["is_month_start"] = df.index.is_month_start.astype(int)
            df_features["is_month_end"] = df.index.is_month_end.astype(int)

        # ğŸ†• æ—¢å­˜ã®æ•°å€¤ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆæ—¥ä»˜åˆ—ã¯é™¤å¤–ï¼‰
        for col in feature_columns:
            if col in df.columns:
                if df[col].dtype in ["int64", "float64", "int32", "float32"]:
                    df_features[col] = df[col]
                    # ç‰¹å¾´é‡ã®ãƒ©ã‚°ã‚‚ä½œæˆ
                    for lag in [1, 3, 7]:
                        if len(df) > lag:
                            df_features[f"{col}_lag_{lag}"] = df_features[col].shift(
                                lag
                            )

        # ğŸ†• å…ƒã®æ—¥ä»˜åˆ—ã‚’å‰Šé™¤ï¼ˆã‚‚ã—å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        date_columns_to_remove = []
        for col in df_features.columns:
            if df_features[col].dtype == "object" or df_features[
                col
            ].dtype.name.startswith("datetime"):
                if col != target_column:  # ç›®çš„å¤‰æ•°ã¯ä¿æŒ
                    date_columns_to_remove.append(col)

        if date_columns_to_remove:
            print(f"ğŸ—‘ï¸ æ—¥ä»˜/æ–‡å­—åˆ—åˆ—ã‚’å‰Šé™¤: {date_columns_to_remove}")
            df_features = df_features.drop(columns=date_columns_to_remove)

        # ğŸ†• ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€çµ‚ç¢ºèªã¨å¤‰æ›
        for col in df_features.columns:
            if col != target_column:  # ç›®çš„å¤‰æ•°ã¯é™¤ã
                if df_features[col].dtype == "object":
                    try:
                        # æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ã‚’è©¦è¡Œ
                        df_features[col] = pd.to_numeric(
                            df_features[col], errors="coerce"
                        )
                        print(f"âœ… åˆ— {col} ã‚’æ•°å€¤ã«å¤‰æ›")
                    except:
                        # å¤‰æ›ã§ããªã„å ´åˆã¯å‰Šé™¤
                        print(f"âš ï¸ åˆ— {col} ã‚’å‰Šé™¤ï¼ˆæ•°å€¤å¤‰æ›ä¸å¯ï¼‰")
                        df_features = df_features.drop(columns=[col])

        # æ¬ æå€¤ã‚’é™¤å»
        df_features = df_features.dropna()

        print(f"ğŸ” æœ€çµ‚çš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿å‹:")
        for col in df_features.columns:
            print(f"  {col}: {df_features[col].dtype}")

        return df_features

    def _train_lightgbm_model(
        self, train_data: pd.DataFrame, test_data: pd.DataFrame, target_column: str
    ) -> Dict[str, Any]:
        """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
        X_train = train_data.drop(columns=[target_column])
        y_train = train_data[target_column]
        X_test = test_data.drop(columns=[target_column])
        y_test = test_data[target_column]

        # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        train_dataset = lgb.Dataset(X_train, label=y_train)
        valid_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        params = {
            "objective": "regression",
            "metric": "rmse",
            "boosting_type": "gbdt",
            "num_leaves": 31,
            "learning_rate": 0.05,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1,
            "seed": 42,  # å†ç¾æ€§ã®ãŸã‚è¿½åŠ 
            "min_child_samples": 20,  # éå­¦ç¿’é˜²æ­¢
        }

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = lgb.train(
            params,
            train_dataset,
            valid_sets=[valid_dataset],
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )

        # äºˆæ¸¬
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        train_metrics = self._calculate_metrics(y_train, y_pred_train)
        test_metrics = self._calculate_metrics(y_test, y_pred_test)

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã§å–å¾—ï¼‰
        importance_gain = model.feature_importance(importance_type="gain")
        importance_split = model.feature_importance(importance_type="split")

        # ã‚¼ãƒ­ã§ãªã„é‡è¦åº¦ã‚’ç¢ºä¿
        feature_importance = []
        for i, col in enumerate(X_train.columns):
            # gainãŒã‚¼ãƒ­ã®å ´åˆã¯splitã‚’ä½¿ç”¨
            importance = (
                importance_gain[i] if importance_gain[i] > 0 else importance_split[i]
            )
            feature_importance.append((col, float(importance)))
            feature_importance.sort(key=lambda x: x[1], reverse=True)

        return {
            "model": model,
            "model_type": "lightgbm",
            "model_metrics": {
                "train": train_metrics,
                "test": test_metrics,
                "r2_score": test_metrics["r2"],
                "rmse": test_metrics["rmse"],
                "mae": test_metrics["mae"],
            },
            "feature_importance": feature_importance,
            "predictions": [
                (str(idx), pred, actual)
                for idx, pred, actual in zip(test_data.index, y_pred_test, y_test)
            ],
            "actual_values": [
                (str(idx), val) for idx, val in zip(train_data.index, y_train)
            ],
        }

    def _train_alternative_model(
        self, train_data: pd.DataFrame, test_data: pd.DataFrame, target_column: str
    ) -> Dict[str, Any]:
        """LightGBMãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ï¼ˆç·šå½¢å›å¸°ï¼‰"""
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import StandardScaler

        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
        X_train = train_data.drop(columns=[target_column])
        y_train = train_data[target_column]
        X_test = test_data.drop(columns=[target_column])
        y_test = test_data[target_column]

        # ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ï¼ˆä¿‚æ•°ã®æ¯”è¼ƒã®ãŸã‚ï¼‰
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = LinearRegression()
        model.fit(X_train, y_train)

        # äºˆæ¸¬
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        train_metrics = self._calculate_metrics(y_train, y_pred_train)
        test_metrics = self._calculate_metrics(y_test, y_pred_test)

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆæ¨™æº–åŒ–ã•ã‚ŒãŸä¿‚æ•°ã®çµ¶å¯¾å€¤ï¼‰
        feature_importance = []
        for i, col in enumerate(X_train.columns):
            importance = abs(model.coef_[i])
            feature_importance.append((col, float(importance)))

        feature_importance.sort(key=lambda x: x[1], reverse=True)

        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        print(f"ğŸ” ç·šå½¢å›å¸°ã®ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½5å€‹ï¼‰:")
        for feat, imp in feature_importance[:5]:
            print(f"  {feat}: {imp:.4f}")

        return {
            "model": model,
            "model_type": "linear_regression",
            "model_metrics": {
                "train": train_metrics,
                "test": test_metrics,
                "r2_score": test_metrics["r2"],
                "rmse": test_metrics["rmse"],
                "mae": test_metrics["mae"],
            },
            "feature_importance": feature_importance,
            "predictions": [
                (str(idx), pred, actual)
                for idx, pred, actual in zip(test_data.index, y_pred_test, y_test)
            ],
            "actual_values": [
                (str(idx), val) for idx, val in zip(train_data.index, y_train)
            ],
        }

    def _calculate_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
        return {
            "rmse": float(np.sqrt(mean_squared_error(y_true, y_pred))),
            "mae": float(mean_absolute_error(y_true, y_pred)),
            "r2": float(r2_score(y_true, y_pred)),
            "mape": (
                float(np.mean(np.abs((y_true - y_pred) / y_true)) * 100)
                if np.all(y_true != 0)
                else 0.0
            ),
        }

    def _generate_future_predictions(
        self, df: pd.DataFrame, model, target_column: str, forecast_periods: int
    ) -> List[Tuple[str, float]]:
        """æœªæ¥äºˆæ¸¬ã‚’ç”Ÿæˆ"""
        future_predictions = []

        try:
            # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬
            last_data = df.iloc[-1:].drop(columns=[target_column])

            # æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‡¦ç†
            if hasattr(df.index, "freq") and df.index.freq is not None:
                freq = df.index.freq
            else:
                freq = pd.infer_freq(df.index) or "D"

            last_date = df.index[-1]

            for i in range(1, forecast_periods + 1):
                # æœªæ¥ã®æ—¥ä»˜ã‚’ç”Ÿæˆ
                if hasattr(last_date, "strftime"):
                    future_date = last_date + pd.Timedelta(days=i)
                else:
                    future_date = len(df) + i

                # ç‰¹å¾´é‡ã‚’æ›´æ–°ï¼ˆç°¡å˜ãªä¾‹ï¼‰
                if LIGHTGBM_AVAILABLE and hasattr(model, "predict"):
                    pred_value = model.predict(last_data)[0]
                else:
                    pred_value = model.predict(last_data)[0]

                future_predictions.append((str(future_date), float(pred_value)))

                # æ¬¡ã®äºˆæ¸¬ã®ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ãªã©ï¼‰
                # ç°¡ç•¥åŒ–ã®ãŸã‚ã€åŒã˜ç‰¹å¾´é‡ã‚’ä½¿ç”¨

        except Exception as e:
            print(f"æœªæ¥äºˆæ¸¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™

        return future_predictions

    def _infer_frequency(self, index) -> str:
        """æ™‚ç³»åˆ—ã®é »åº¦ã‚’æ¨å®š"""
        try:
            if hasattr(index, "freq") and index.freq:
                return str(index.freq)
            return pd.infer_freq(index) or "Unknown"
        except:
            return "Unknown"

    def _analyze_trend(self, series: pd.Series) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
        try:
            # ç·šå½¢å›å¸°ã§ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ¤å®š
            x = np.arange(len(series))
            slope = np.polyfit(x, series.values, 1)[0]

            if slope > 0.01:
                return "ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰"
            elif slope < -0.01:
                return "ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰"
            else:
                return "æ¨ªã°ã„"
        except:
            return "ä¸æ˜"

    def create_plot(self, results: Dict[str, Any], df: pd.DataFrame) -> str:
        """æ™‚ç³»åˆ—åˆ†æã®å¯è¦–åŒ–ã‚’ä½œæˆ"""
        try:
            print("=== ãƒ—ãƒ­ãƒƒãƒˆä½œæˆé–‹å§‹ ===")

            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
            self.setup_japanese_font()

            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            model_metrics = results["model_metrics"]
            feature_importance = results["feature_importance"]
            predictions = results["predictions"]
            actual_values = results["actual_values"]
            future_predictions = results.get("future_predictions", [])

            # å›³ã®ã‚µã‚¤ã‚ºã¨é…ç½®
            fig = plt.figure(figsize=(16, 12))
            fig.patch.set_facecolor("white")

            # 1. æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå®Ÿæ¸¬å€¤ãƒ»äºˆæ¸¬å€¤ãƒ»æœªæ¥äºˆæ¸¬ï¼‰
            ax1 = plt.subplot(2, 3, 1)

            # å®Ÿæ¸¬å€¤
            if actual_values:
                dates_actual = [
                    pd.to_datetime(x[0]) if isinstance(x[0], str) else x[0]
                    for x in actual_values
                ]
                values_actual = [x[1] for x in actual_values]
                plt.plot(dates_actual, values_actual, "b-", label="å®Ÿæ¸¬å€¤", linewidth=2)

            # äºˆæ¸¬å€¤
            if predictions:
                dates_pred = [
                    pd.to_datetime(x[0]) if isinstance(x[0], str) else x[0]
                    for x in predictions
                ]
                values_pred = [x[1] for x in predictions]
                plt.plot(dates_pred, values_pred, "r--", label="äºˆæ¸¬å€¤", linewidth=2)

            # æœªæ¥äºˆæ¸¬
            if future_predictions:
                dates_future = [
                    pd.to_datetime(x[0]) if isinstance(x[0], str) else x[0]
                    for x in future_predictions
                ]
                values_future = [x[1] for x in future_predictions]
                plt.plot(
                    dates_future, values_future, "g:", label="æœªæ¥äºˆæ¸¬", linewidth=2
                )

            plt.xlabel("æ™‚é–“")
            plt.ylabel("å€¤")
            plt.title("æ™‚ç³»åˆ—äºˆæ¸¬çµæœ")
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.xticks(rotation=45)

            # 2. äºˆæ¸¬ç²¾åº¦ã®ãƒ—ãƒ­ãƒƒãƒˆ
            ax2 = plt.subplot(2, 3, 2)
            if predictions:
                actual_test = [x[2] for x in predictions]
                pred_test = [x[1] for x in predictions]
                plt.scatter(actual_test, pred_test, alpha=0.6)

                # å¯¾è§’ç·š
                min_val = min(min(actual_test), min(pred_test))
                max_val = max(max(actual_test), max(pred_test))
                plt.plot([min_val, max_val], [min_val, max_val], "r--", alpha=0.8)

                plt.xlabel("å®Ÿæ¸¬å€¤")
                plt.ylabel("äºˆæ¸¬å€¤")
                plt.title(f'äºˆæ¸¬ç²¾åº¦\nRÂ² = {model_metrics["r2_score"]:.3f}')
                plt.grid(True, alpha=0.3)

            # 3. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
            ax3 = plt.subplot(2, 3, 3)
            if predictions:
                residuals = [x[2] - x[1] for x in predictions]
                pred_values = [x[1] for x in predictions]
                plt.scatter(pred_values, residuals, alpha=0.6)
                plt.axhline(y=0, color="r", linestyle="--", alpha=0.8)
                plt.xlabel("äºˆæ¸¬å€¤")
                plt.ylabel("æ®‹å·®")
                plt.title("æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ")
                plt.grid(True, alpha=0.3)

            # 4. ç‰¹å¾´é‡é‡è¦åº¦
            ax4 = plt.subplot(2, 3, 4)
            if feature_importance:
                top_features = feature_importance[:10]  # ä¸Šä½10å€‹
                feature_names = [x[0] for x in top_features]
                importance_values = [x[1] for x in top_features]

                y_pos = np.arange(len(feature_names))
                plt.barh(y_pos, importance_values, alpha=0.7)
                plt.yticks(y_pos, feature_names)
                plt.xlabel("é‡è¦åº¦")
                plt.title("ç‰¹å¾´é‡é‡è¦åº¦")
                plt.gca().invert_yaxis()

            # 5. è©•ä¾¡æŒ‡æ¨™
            ax5 = plt.subplot(2, 3, 5)
            metrics_names = ["RMSE", "MAE", "RÂ²", "MAPE"]
            test_metrics = model_metrics["test"]
            metrics_values = [
                test_metrics["rmse"],
                test_metrics["mae"],
                test_metrics["r2"],
                test_metrics.get("mape", 0),
            ]

            bars = plt.bar(
                metrics_names,
                metrics_values,
                alpha=0.7,
                color=["skyblue", "lightgreen", "orange", "pink"],
            )
            plt.ylabel("å€¤")
            plt.title("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™")
            plt.xticks(rotation=45)

            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, value in zip(bars, metrics_values):
                plt.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + 0.01,
                    f"{value:.3f}",
                    ha="center",
                    va="bottom",
                )

            # 6. ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã¨ã‚µãƒãƒªãƒ¼
            ax6 = plt.subplot(2, 3, 6)
            plt.axis("off")

            data_info = results.get("data_info", {})
            timeseries_info = results.get("timeseries_info", {})

            info_text = f"""
ãƒ‡ãƒ¼ã‚¿æƒ…å ±:
â€¢ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {data_info.get('total_samples', 0)}
â€¢ è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {data_info.get('train_samples', 0)}
â€¢ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {data_info.get('test_samples', 0)}
â€¢ ç‰¹å¾´é‡æ•°: {data_info.get('feature_count', 0)}

æ™‚ç³»åˆ—æƒ…å ±:
â€¢ é–‹å§‹æ—¥: {timeseries_info.get('start_date', 'N/A')}
â€¢ çµ‚äº†æ—¥: {timeseries_info.get('end_date', 'N/A')}
â€¢ é »åº¦: {timeseries_info.get('frequency', 'N/A')}
â€¢ ãƒˆãƒ¬ãƒ³ãƒ‰: {timeseries_info.get('trend', 'N/A')}

ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:
â€¢ RÂ²ã‚¹ã‚³ã‚¢: {model_metrics['r2_score']:.3f}
â€¢ RMSE: {model_metrics['rmse']:.3f}
â€¢ MAE: {model_metrics['mae']:.3f}
â€¢ ãƒ¢ãƒ‡ãƒ«: {results.get('model_type', 'N/A')}
            """

            plt.text(
                0.1,
                0.9,
                info_text,
                fontsize=10,
                verticalalignment="top",
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5),
            )

            # å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«
            model_type = results.get("model_type", "unknown")
            target_col = data_info.get("target_column", "unknown")

            fig.suptitle(
                f"æ™‚ç³»åˆ—åˆ†æçµæœ - {model_type.upper()}\n"
                f"ç›®çš„å¤‰æ•°: {target_col}, "
                f"RÂ² = {model_metrics['r2_score']:.3f}, "
                f"äºˆæ¸¬æœŸé–“: {results.get('forecast_periods', 0)}æœŸé–“",
                fontsize=14,
                y=0.98,
            )

            plt.tight_layout()
            plt.subplots_adjust(top=0.92)

            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            plot_base64 = self.save_plot_as_base64(fig)
            print(f"ãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
            return plot_base64

        except Exception as e:
            print(f"ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return ""

    def create_response(
        self,
        results: Dict[str, Any],
        df: pd.DataFrame,
        session_id: int,
        session_name: str,
        file,
        plot_base64: str,
    ) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        try:
            predictions = results["predictions"]
            actual_values = results["actual_values"]
            future_predictions = results.get("future_predictions", [])

            # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆ
            prediction_data = []
            for i, (timestamp, pred_value, actual_value) in enumerate(predictions):
                prediction_data.append(
                    {
                        "timestamp": str(timestamp),
                        "predicted_value": float(pred_value),
                        "actual_value": float(actual_value),
                        "residual": float(actual_value - pred_value),
                        "order_index": i,
                    }
                )

            # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆ
            actual_data = []
            for i, (timestamp, value) in enumerate(actual_values):
                actual_data.append(
                    {
                        "timestamp": str(timestamp),
                        "value": float(value),
                        "order_index": i,
                    }
                )

            # æœªæ¥äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆ
            forecast_data = []
            for i, (timestamp, pred_value) in enumerate(future_predictions):
                forecast_data.append(
                    {
                        "timestamp": str(timestamp),
                        "predicted_value": float(pred_value),
                        "order_index": len(predictions) + i,
                    }
                )

            return {
                "success": True,
                "session_id": session_id,
                "analysis_type": self.get_analysis_type(),
                "data": {
                    "model_type": results["model_type"],
                    "target_column": results["data_info"]["target_column"],
                    "feature_columns": results["data_info"]["feature_columns"],
                    "forecast_periods": results["forecast_periods"],
                    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½
                    "model_metrics": results["model_metrics"],
                    "feature_importance": results["feature_importance"],
                    # äºˆæ¸¬çµæœ
                    "predictions": prediction_data,
                    "actual_values": actual_data,
                    "future_predictions": forecast_data,
                    # æ™‚ç³»åˆ—æƒ…å ±
                    "timeseries_info": results["timeseries_info"],
                    "data_info": results["data_info"],
                    # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒ
                    "plot_image": plot_base64,
                    # å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®åº§æ¨™å½¢å¼
                    "coordinates": {
                        "actual": [
                            {
                                "timestamp": str(timestamp),
                                "value": float(value),
                            }
                            for timestamp, value in actual_values
                        ],
                        "predictions": [
                            {
                                "timestamp": str(timestamp),
                                "predicted": float(pred_value),
                                "actual": float(actual_value),
                                "residual": float(actual_value - pred_value),
                            }
                            for timestamp, pred_value, actual_value in predictions
                        ],
                        "forecast": [
                            {
                                "timestamp": str(timestamp),
                                "predicted": float(pred_value),
                            }
                            for timestamp, pred_value in future_predictions
                        ],
                    },
                },
                "metadata": {
                    "session_name": session_name,
                    "filename": file.filename,
                    "rows": df.shape[0],
                    "columns": df.shape[1],
                    "target_column": results["data_info"]["target_column"],
                    "feature_columns": results["data_info"]["feature_columns"],
                },
            }

        except Exception as e:
            print(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            raise

    def get_session_detail(self, db: Session, session_id: int) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—"""
        try:
            print(f"ğŸ“Š æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

            # åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if hasattr(super(), "get_session_detail"):
                try:
                    base_detail = super().get_session_detail(db, session_id)
                except Exception as e:
                    print(f"âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã®get_session_detailã‚¨ãƒ©ãƒ¼: {e}")
                    base_detail = self._get_session_detail_directly(db, session_id)
            else:
                print("âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã«get_session_detailãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                base_detail = self._get_session_detail_directly(db, session_id)

            if not base_detail or not base_detail.get("success"):
                return base_detail

            # æ™‚ç³»åˆ—ç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            timeseries_data = self._get_timeseries_data(db, session_id)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’æ§‹ç¯‰
            response_data = {
                "success": True,
                "data": {
                    "session_info": base_detail["data"]["session_info"],
                    "metadata": {
                        "filename": base_detail["data"]["metadata"]["filename"],
                        "rows": base_detail["data"]["metadata"]["rows"],
                        "columns": base_detail["data"]["metadata"]["columns"],
                        "target_column": timeseries_data.get("target_column", ""),
                        "feature_columns": timeseries_data.get("feature_columns", []),
                    },
                    "analysis_data": {
                        "predictions": timeseries_data.get("predictions", []),
                        "actual_values": timeseries_data.get("actual_values", []),
                        "future_predictions": timeseries_data.get(
                            "future_predictions", []
                        ),
                        "model_metrics": timeseries_data.get("model_metrics", {}),
                        "feature_importance": timeseries_data.get(
                            "feature_importance", []
                        ),
                    },
                    "visualization": base_detail["data"].get("visualization", {}),
                },
            }

            print(f"âœ… æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—å®Œäº†")
            return response_data

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return {"success": False, "error": str(e)}

    def _get_timeseries_data(self, db: Session, session_id: int) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            from models import CoordinatesData, AnalysisMetadata

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            coordinates = (
                db.query(CoordinatesData)
                .filter(CoordinatesData.session_id == session_id)
                .all()
            )

            # å®Ÿæ¸¬å€¤ãƒ‡ãƒ¼ã‚¿
            actual_values = []
            predictions = []
            future_predictions = []

            for coord in coordinates:
                coord_data = {
                    "timestamp": coord.point_name,
                    "order_index": int(coord.dimension_4) if coord.dimension_4 else 0,
                }

                if coord.point_type == "train":  # å®Ÿæ¸¬å€¤ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
                    coord_data["value"] = (
                        float(coord.dimension_1) if coord.dimension_1 else 0.0
                    )
                    actual_values.append(coord_data)

                elif coord.point_type == "test":  # äºˆæ¸¬å€¤ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
                    coord_data.update(
                        {
                            "predicted_value": (
                                float(coord.dimension_1) if coord.dimension_1 else 0.0
                            ),
                            "actual_value": (
                                float(coord.dimension_3) if coord.dimension_3 else 0.0
                            ),
                            "residual": (
                                float(coord.dimension_2) if coord.dimension_2 else 0.0
                            ),
                        }
                    )
                    predictions.append(coord_data)

                elif coord.point_type == "variable":  # æœªæ¥äºˆæ¸¬
                    coord_data["predicted_value"] = (
                        float(coord.dimension_1) if coord.dimension_1 else 0.0
                    )
                    future_predictions.append(coord_data)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            metadata_entries = (
                db.query(AnalysisMetadata)
                .filter(AnalysisMetadata.session_id == session_id)
                .all()
            )

            model_metrics = {}
            feature_importance = []
            target_column = ""
            feature_columns = []

            for meta in metadata_entries:
                if meta.metadata_type == "timeseries_metrics":
                    content = meta.metadata_content
                    model_metrics = content.get("metrics", {})
                    feature_importance = content.get("feature_importance", [])
                    print(f"ğŸ” ä¿å­˜ã™ã‚‹ç‰¹å¾´é‡é‡è¦åº¦: {feature_importance}")
                    target_column = content.get("forecast_parameters", {}).get(
                        "target_column", ""
                    )
                    feature_columns = content.get("data_info", {}).get(
                        "feature_columns", []
                    )

            print(f"ğŸ” æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ:")
            print(f"  - å®Ÿæ¸¬å€¤: {len(actual_values)}ä»¶")
            print(f"  - äºˆæ¸¬å€¤: {len(predictions)}ä»¶")
            print(f"  - æœªæ¥äºˆæ¸¬: {len(future_predictions)}ä»¶")

            return {
                "actual_values": actual_values,
                "predictions": predictions,
                "future_predictions": future_predictions,
                "model_metrics": model_metrics,
                "feature_importance": feature_importance,
                "target_column": target_column,
                "feature_columns": feature_columns,
            }

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "actual_values": [],
                "predictions": [],
                "future_predictions": [],
                "model_metrics": {},
                "feature_importance": [],
                "target_column": "",
                "feature_columns": [],
            }

    def _get_session_detail_directly(self, db: Session, session_id: int):
        """æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’ç›´æ¥å–å¾—"""
        try:
            from models import AnalysisSession, VisualizationData

            print(f"ğŸ“Š æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’å–å¾—
            session = (
                db.query(AnalysisSession)
                .filter(AnalysisSession.id == session_id)
                .first()
            )

            if not session:
                return {
                    "success": False,
                    "error": f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                }

            print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±å–å¾—: {session.session_name}")

            # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            visualization = (
                db.query(VisualizationData)
                .filter(VisualizationData.session_id == session_id)
                .first()
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’æ§‹ç¯‰
            response_data = {
                "success": True,
                "data": {
                    "session_info": {
                        "session_id": session.id,
                        "session_name": session.session_name,
                        "description": getattr(session, "description", "") or "",
                        "filename": session.original_filename,
                        "row_count": getattr(session, "row_count", 0) or 0,
                        "column_count": getattr(session, "column_count", 0) or 0,
                        "analysis_timestamp": (
                            session.analysis_timestamp.isoformat()
                            if hasattr(session, "analysis_timestamp")
                            and session.analysis_timestamp
                            else None
                        ),
                    },
                    "metadata": {
                        "filename": session.original_filename,
                        "rows": getattr(session, "row_count", 0) or 0,
                        "columns": getattr(session, "column_count", 0) or 0,
                    },
                    "visualization": {
                        "plot_image": (
                            visualization.image_base64 if visualization else None
                        ),
                        "width": visualization.width if visualization else 1400,
                        "height": visualization.height if visualization else 1100,
                    },
                },
            }

            print(f"âœ… æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—å®Œäº†")
            return response_data

        except Exception as e:
            print(f"âŒ æ™‚ç³»åˆ—åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return {"success": False, "error": str(e)}
