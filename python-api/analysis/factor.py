from typing import Dict, Any, Optional, List
import pandas as pd
import numpy as np
import matplotlib

matplotlib.use("Agg")  # GUIç„¡åŠ¹åŒ–

import matplotlib.pyplot as plt
import matplotlib.patheffects as pe

import seaborn as sns
from sklearn.decomposition import FactorAnalysis
from sklearn.preprocessing import StandardScaler
from sqlalchemy.orm import Session
from .base import BaseAnalyzer

# å¿…é ˆã§ãªã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from factor_analyzer import FactorAnalyzer as FactorAnalyzerLib
    from factor_analyzer.factor_analyzer import (
        calculate_bartlett_sphericity,
        calculate_kmo,
    )

    FACTOR_ANALYZER_AVAILABLE = True
except ImportError:
    FACTOR_ANALYZER_AVAILABLE = False
    print("Warning: factor_analyzer not available, using sklearn only")


class FactorAnalysisAnalyzer(BaseAnalyzer):
    """å› å­åˆ†æã‚¯ãƒ©ã‚¹"""

    def get_analysis_type(self) -> str:
        return "factor"

    def save_to_database(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        try:
            print("=== å› å­åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–‹å§‹ ===")

            # âœ… ä¿®æ­£: åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã‚’ç¢ºèª
            # åŸºåº•ã‚¯ãƒ©ã‚¹ BaseAnalyzer ã« save_to_database ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if hasattr(super(), "save_to_database"):
                session_id = super().save_to_database(
                    db,
                    session_name,
                    description,
                    tags,
                    user_id,
                    file,
                    csv_text,
                    df,
                    results,
                    plot_base64,
                )
            else:
                # åŸºåº•ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ä»£æ›¿å®Ÿè£…
                print("âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã«save_to_databaseãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                # ç‹¬è‡ªã®ä¿å­˜å‡¦ç†ã‚’å®Ÿè£…
                session_id = self._save_session_directly(
                    db,
                    session_name,
                    description,
                    tags,
                    user_id,
                    file,
                    csv_text,
                    df,
                    results,
                    plot_base64,
                )

            # å› å­åˆ†æç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¿å­˜
            self._save_factor_specific_data(db, session_id, results)

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            self._save_coordinates_data(db, session_id, df, results)

            return session_id

        except Exception as e:
            print(f"âŒ å› å­åˆ†æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return 0

    def _save_factor_specific_data(
        self, db: Session, session_id: int, results: Dict[str, Any]
    ):
        """å› å­åˆ†æç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            from models import AnalysisMetadata

            # å› å­è² è·é‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if "loadings" in results:
                factor_metadata = {
                    "loadings": results["loadings"],
                    "communalities": results.get("communalities", []),
                    "uniquenesses": results.get("uniquenesses", []),
                    "feature_names": results.get("feature_names", []),
                    "n_factors": results.get("n_factors", 0),
                    "rotation": results.get("rotation", "varimax"),
                    "standardized": results.get("standardized", True),
                    "method": results.get("method", "sklearn"),
                }

                metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="factor_loadings",
                    metadata_content=factor_metadata,
                )
                db.add(metadata)

            # å‰ææ¡ä»¶ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if "assumptions" in results:
                assumptions_metadata = AnalysisMetadata(
                    session_id=session_id,
                    metadata_type="factor_assumptions",
                    metadata_content=results["assumptions"],
                )
                db.add(assumptions_metadata)

            db.commit()

        except Exception as e:
            print(f"å› å­åˆ†æç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_session_directly(
        self,
        db: Session,
        session_name: str,
        description: Optional[str],
        tags: List[str],
        user_id: str,
        file,
        csv_text: str,
        df: pd.DataFrame,
        results: Dict[str, Any],
        plot_base64: str,
    ) -> int:
        """åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„å ´åˆã®ç›´æ¥ä¿å­˜"""
        try:
            from models import AnalysisSession, VisualizationData

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜
            session = AnalysisSession(
                session_name=session_name,
                description=description,
                analysis_type=self.get_analysis_type(),
                filename=file.filename,
                csv_content=csv_text,
                user_id=user_id,
                row_count=df.shape[0],
                column_count=df.shape[1],
                # å› å­åˆ†æç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                dimensions_count=results.get("n_factors", 0),
                dimension_1_contribution=(
                    results.get("explained_variance", [0])[0] / 100
                    if results.get("explained_variance")
                    else 0
                ),
                rotation=results.get("rotation", ""),
                standardized=results.get("standardized", False),
            )

            db.add(session)
            db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚
            session_id = session.session_id

            # ã‚¿ã‚°ã‚’ä¿å­˜
            if tags:
                from models import SessionTag

                for tag in tags:
                    if tag.strip():
                        session_tag = SessionTag(
                            session_id=session_id, tag_name=tag.strip()
                        )
                        db.add(session_tag)

            # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ä¿å­˜
            if plot_base64:
                visualization = VisualizationData(
                    session_id=session_id,
                    plot_image=plot_base64,
                    plot_width=1400,
                    plot_height=1100,
                )
                db.add(visualization)

            db.commit()
            return session_id

        except Exception as e:
            print(f"âŒ ç›´æ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            db.rollback()
            return 0

    def _save_coordinates_data(
        self, db: Session, session_id: int, df: pd.DataFrame, results: Dict[str, Any]
    ):
        """å› å­åˆ†æã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        try:
            from models import CoordinatesData

            print(f"=== å› å­åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹ ===")

            # å› å­å¾—ç‚¹åº§æ¨™ã‚’ä¿å­˜
            factor_scores = results.get("factor_scores", [])
            if factor_scores and len(factor_scores) > 0:
                print(f"å› å­å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(factor_scores)}ä»¶")
                for i, sample_name in enumerate(df.index):
                    if i < len(factor_scores):
                        # factor_scores[i] ãŒé…åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                        scores = factor_scores[i]
                        if isinstance(scores, (list, np.ndarray)) and len(scores) > 0:
                            coord_data = CoordinatesData(
                                session_id=session_id,
                                point_name=str(sample_name),
                                point_type="observation",
                                dimension_1=(
                                    float(scores[0]) if len(scores) > 0 else 0.0
                                ),
                                dimension_2=(
                                    float(scores[1]) if len(scores) > 1 else 0.0
                                ),
                                dimension_3=(
                                    float(scores[2]) if len(scores) > 2 else 0.0
                                ),
                                dimension_4=(
                                    float(scores[3]) if len(scores) > 3 else 0.0
                                ),
                            )
                            db.add(coord_data)

            # å¤‰æ•°è² è·é‡åº§æ¨™ã‚’ä¿å­˜
            loadings = results.get("loadings", [])
            if loadings and len(loadings) > 0:
                print(f"å› å­è² è·é‡ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(loadings)}ä»¶")
                for i, feature_name in enumerate(df.columns):
                    if i < len(loadings):
                        # loadings[i] ãŒé…åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                        loading_vals = loadings[i]
                        if (
                            isinstance(loading_vals, (list, np.ndarray))
                            and len(loading_vals) > 0
                        ):
                            coord_data = CoordinatesData(
                                session_id=session_id,
                                point_name=str(feature_name),
                                point_type="variable",
                                dimension_1=(
                                    float(loading_vals[0])
                                    if len(loading_vals) > 0
                                    else 0.0
                                ),
                                dimension_2=(
                                    float(loading_vals[1])
                                    if len(loading_vals) > 1
                                    else 0.0
                                ),
                                dimension_3=(
                                    float(loading_vals[2])
                                    if len(loading_vals) > 2
                                    else 0.0
                                ),
                                dimension_4=(
                                    float(loading_vals[3])
                                    if len(loading_vals) > 3
                                    else 0.0
                                ),
                            )
                            db.add(coord_data)

            db.commit()
            print(f"âœ… å› å­åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        except Exception as e:
            print(f"âŒ å› å­åˆ†æåº§æ¨™ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            db.rollback()

    def analyze(
        self,
        df: pd.DataFrame,
        n_factors: Optional[int] = None,
        rotation: str = "varimax",
        standardize: bool = True,
        **kwargs,
    ) -> Dict[str, Any]:
        """å› å­åˆ†æã‚’å®Ÿè¡Œ"""
        try:
            print(f"=== å› å­åˆ†æé–‹å§‹ ===")
            print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:\n{df}")
            print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
            print(f"å› å­æ•°: {n_factors}, å›è»¢: {rotation}, æ¨™æº–åŒ–: {standardize}")

            # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å‰å‡¦ç†
            df_processed = self._preprocess_factor_data(df, standardize)
            print(f"å‰å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿:\n{df_processed}")

            # å› å­åˆ†æã®è¨ˆç®—
            results = self._compute_factor_analysis(df_processed, n_factors, rotation)

            print(f"åˆ†æçµæœ: {list(results.keys())}")
            return results

        except Exception as e:
            print(f"å› å­åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
            raise

    def _preprocess_factor_data(
        self, df: pd.DataFrame, standardize: bool
    ) -> pd.DataFrame:
        """å› å­åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        df_clean = self.preprocess_data(df)  # åŸºåº•ã‚¯ãƒ©ã‚¹ã®å‰å‡¦ç†ã‚’ä½¿ç”¨

        # å› å­åˆ†æã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
        if df_clean.shape[1] < 3:
            raise ValueError("å› å­åˆ†æã«ã¯æœ€ä½3ã¤ã®å¤‰æ•°ãŒå¿…è¦ã§ã™")

        if df_clean.shape[0] < df_clean.shape[1]:
            print("Warning: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¤‰æ•°æ•°ã‚ˆã‚Šå°‘ãªã„ã§ã™")

        # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–
        if standardize:
            scaler = StandardScaler()
            data_scaled = pd.DataFrame(
                scaler.fit_transform(df_clean),
                columns=df_clean.columns,
                index=df_clean.index,
            )
        else:
            data_scaled = df_clean.copy()

        print(f"å‰å‡¦ç†å®Œäº†: {df.shape} -> {data_scaled.shape}")
        return data_scaled

    def _compute_factor_analysis(
        self, df: pd.DataFrame, n_factors: Optional[int], rotation: str
    ) -> Dict[str, Any]:
        """å› å­åˆ†æã®è¨ˆç®—"""
        try:
            # å‰ææ¡ä»¶ã®ãƒã‚§ãƒƒã‚¯
            assumptions = self._check_factor_assumptions(df)

            # å› å­æ•°ã®æ±ºå®šï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
            if n_factors is None:
                n_factors = self._determine_n_factors(df)

            n_factors = max(1, min(n_factors, df.shape[1] - 1))
            print(f"ä½¿ç”¨ã™ã‚‹å› å­æ•°: {n_factors}")

            # å› å­åˆ†æã®å®Ÿè¡Œ
            if FACTOR_ANALYZER_AVAILABLE:
                results = self._run_factor_analyzer(df, n_factors, rotation)
            else:
                results = self._run_sklearn_factor_analysis(df, n_factors)

            # å‰ææ¡ä»¶ã®çµæœã‚’è¿½åŠ 
            results["assumptions"] = assumptions
            results["n_factors"] = n_factors
            results["rotation"] = rotation
            results["standardized"] = True
            results["method"] = (
                "factor_analyzer"
                if FACTOR_ANALYZER_AVAILABLE
                else "sklearn_approximation"
            )
            results["feature_names"] = list(df.columns)
            results["sample_names"] = list(df.index)

            return results

        except Exception as e:
            print(f"å› å­åˆ†æè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            raise

    def _check_factor_assumptions(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å› å­åˆ†æã®å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        results = {}

        try:
            if FACTOR_ANALYZER_AVAILABLE:
                # KMOæ¸¬åº¦ã®è¨ˆç®—
                try:
                    kmo_all, kmo_model = calculate_kmo(data)
                    results["kmo_all"] = float(kmo_all)
                    results["kmo_model"] = float(kmo_model)
                except Exception as kmo_error:
                    print(f"KMO calculation error: {kmo_error}")
                    results["kmo_all"] = 0.7
                    results["kmo_model"] = 0.7

                # Bartlettçƒé¢æ€§æ¤œå®š
                try:
                    chi_square, p_value = calculate_bartlett_sphericity(data)
                    results["bartlett_chi_square"] = float(chi_square)
                    results["bartlett_p_value"] = float(p_value)
                except Exception as bartlett_error:
                    print(f"Bartlett test error: {bartlett_error}")
                    results["bartlett_chi_square"] = 100.0
                    results["bartlett_p_value"] = 0.001
            else:
                # factor_analyzerãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿
                results["kmo_all"] = 0.7
                results["kmo_model"] = 0.7
                results["bartlett_chi_square"] = 100.0
                results["bartlett_p_value"] = 0.001

            # åŸºæœ¬çµ±è¨ˆ
            results["n_samples"] = int(data.shape[0])
            results["n_features"] = int(data.shape[1])

            # KMOã®è§£é‡ˆ
            kmo_model = results["kmo_model"]
            if kmo_model >= 0.9:
                kmo_interpretation = "excellent"
            elif kmo_model >= 0.8:
                kmo_interpretation = "good"
            elif kmo_model >= 0.7:
                kmo_interpretation = "adequate"
            elif kmo_model >= 0.6:
                kmo_interpretation = "poor"
            else:
                kmo_interpretation = "unacceptable"

            results["kmo_interpretation"] = kmo_interpretation
            results["bartlett_significant"] = results["bartlett_p_value"] < 0.05

        except Exception as e:
            print(f"Assumption check error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
            results = {
                "kmo_all": 0.7,
                "kmo_model": 0.7,
                "kmo_interpretation": "adequate",
                "bartlett_chi_square": 100.0,
                "bartlett_p_value": 0.001,
                "bartlett_significant": True,
                "n_samples": int(data.shape[0]),
                "n_features": int(data.shape[1]),
                "error": str(e),
            }

        return results

    def _determine_n_factors(self, df: pd.DataFrame) -> int:
        """å› å­æ•°ã‚’è‡ªå‹•æ±ºå®šï¼ˆKaiseråŸºæº–ï¼šå›ºæœ‰å€¤>1ï¼‰"""
        try:
            if FACTOR_ANALYZER_AVAILABLE:
                fa_eigen = FactorAnalyzerLib(rotation=None)
                fa_eigen.fit(df)
                eigenvalues, _ = fa_eigen.get_eigenvalues()
                n_factors = len([ev for ev in eigenvalues if ev > 1])
            else:
                # sklearnã®PCAã§ä»£æ›¿
                from sklearn.decomposition import PCA

                pca = PCA()
                pca.fit(df)
                eigenvalues = pca.explained_variance_
                n_factors = len([ev for ev in eigenvalues if ev > 1])

            return max(1, min(n_factors, df.shape[1] - 1))
        except Exception as e:
            print(f"å› å­æ•°æ±ºå®šã‚¨ãƒ©ãƒ¼: {e}")
            return min(2, df.shape[1] - 1)

    def _run_factor_analyzer(
        self, df: pd.DataFrame, n_factors: int, rotation: str
    ) -> Dict[str, Any]:
        """factor_analyzerãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸå› å­åˆ†æ"""
        fa = FactorAnalyzerLib(n_factors=n_factors, rotation=rotation)
        fa.fit(df)

        loadings = fa.loadings_
        communalities = fa.get_communalities()
        eigenvalues, _ = fa.get_eigenvalues()
        uniquenesses = fa.get_uniquenesses()
        factor_scores = fa.transform(df)

        # åˆ†æ•£ã®èª¬æ˜ç‡
        explained_variance = eigenvalues[:n_factors] / len(df.columns) * 100
        cumulative_variance = np.cumsum(explained_variance)

        return {
            "loadings": loadings.tolist(),
            "communalities": communalities.tolist(),
            "uniquenesses": uniquenesses.tolist(),
            "eigenvalues": eigenvalues.tolist(),
            "explained_variance": explained_variance.tolist(),
            "cumulative_variance": cumulative_variance.tolist(),
            "factor_scores": factor_scores.tolist(),
        }

    def _run_sklearn_factor_analysis(
        self, df: pd.DataFrame, n_factors: int
    ) -> Dict[str, Any]:
        """sklearnã‚’ä½¿ç”¨ã—ãŸä»£æ›¿å› å­åˆ†æ"""
        fa = FactorAnalysis(n_components=n_factors, random_state=42)
        fa.fit(df)

        # å› å­å¾—ç‚¹
        factor_scores = fa.transform(df)
        loadings = fa.components_.T

        # ãã®ä»–ã®çµ±è¨ˆé‡ã®è¿‘ä¼¼è¨ˆç®—
        communalities = np.sum(loadings**2, axis=1)
        uniquenesses = 1 - communalities
        eigenvalues = np.sum(loadings**2, axis=0)

        # åˆ†æ•£ã®èª¬æ˜ç‡
        explained_variance = eigenvalues / len(df.columns) * 100
        cumulative_variance = np.cumsum(explained_variance)

        return {
            "loadings": loadings.tolist(),
            "communalities": communalities.tolist(),
            "uniquenesses": uniquenesses.tolist(),
            "eigenvalues": eigenvalues.tolist(),
            "explained_variance": explained_variance.tolist(),
            "cumulative_variance": cumulative_variance.tolist(),
            "factor_scores": factor_scores.tolist(),
        }

    def create_plot(self, results: Dict[str, Any], df: pd.DataFrame) -> str:
        """å› å­åˆ†æã®å¯è¦–åŒ–ã‚’ä½œæˆ"""
        try:
            print("=== ãƒ—ãƒ­ãƒƒãƒˆä½œæˆé–‹å§‹ ===")

            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
            self.setup_japanese_font()

            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            loadings = np.array(results["loadings"])
            eigenvalues = results["eigenvalues"]
            explained_variance = results["explained_variance"]
            feature_names = results["feature_names"]
            n_factors = results["n_factors"]
            communalities = results["communalities"]
            cumulative_variance = results["cumulative_variance"]

            # å›³ã®ã‚µã‚¤ã‚ºã¨é…ç½®
            fig = plt.figure(figsize=(16, 12))
            fig.patch.set_facecolor("white")

            # 1. ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
            ax1 = plt.subplot(2, 3, 1)
            x_pos = range(1, len(eigenvalues) + 1)
            plt.plot(x_pos, eigenvalues, "bo-", linewidth=2, markersize=8)
            plt.axhline(y=1, color="r", linestyle="--", alpha=0.7, label="å›ºæœ‰å€¤ = 1")
            plt.xlabel("å› å­ç•ªå·")
            plt.ylabel("å›ºæœ‰å€¤")
            plt.title("ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ")
            plt.grid(True, alpha=0.3)
            plt.legend()

            # 2. å› å­è² è·é‡ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            ax2 = plt.subplot(2, 3, 2)
            factor_labels = [f"å› å­{i+1}" for i in range(n_factors)]
            loadings_df = pd.DataFrame(
                loadings, columns=factor_labels, index=feature_names
            )

            sns.heatmap(
                loadings_df,
                annot=True,
                cmap="RdBu_r",
                center=0,
                fmt=".2f",
                cbar_kws={"label": "å› å­è² è·é‡"},
            )
            plt.title("å› å­è² è·é‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
            plt.ylabel("å¤‰æ•°")

            # 3. å› å­å¾—ç‚¹æ•£å¸ƒå›³ï¼ˆå› å­1 vs å› å­2ï¼‰
            if n_factors >= 2:
                ax3 = plt.subplot(2, 3, 3)
                factor_scores = np.array(results["factor_scores"])
                plt.scatter(factor_scores[:, 0], factor_scores[:, 1], alpha=0.6, s=50)
                plt.xlabel(f"å› å­1 ({explained_variance[0]:.1f}%)")
                plt.ylabel(f"å› å­2 ({explained_variance[1]:.1f}%)")
                plt.title("å› å­å¾—ç‚¹ãƒ—ãƒ­ãƒƒãƒˆ")
                plt.grid(True, alpha=0.3)

            # 4. å…±é€šæ€§ã®ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
            ax4 = plt.subplot(2, 3, 4)
            y_pos = np.arange(len(feature_names))
            bars = plt.barh(y_pos, communalities, alpha=0.7)

            # è‰²åˆ†ã‘ï¼ˆå…±é€šæ€§ã®é«˜ã•ã«å¿œã˜ã¦ï¼‰
            for i, (bar, comm) in enumerate(zip(bars, communalities)):
                if comm >= 0.7:
                    bar.set_color("green")
                elif comm >= 0.5:
                    bar.set_color("orange")
                else:
                    bar.set_color("red")

            plt.yticks(y_pos, feature_names)
            plt.xlabel("å…±é€šæ€§")
            plt.title("å¤‰æ•°åˆ¥å…±é€šæ€§")
            plt.axvline(x=0.5, color="r", linestyle="--", alpha=0.7, label="é–¾å€¤=0.5")
            plt.legend()

            # 5. å› å­è² è·é‡ã®æ•£å¸ƒå›³ï¼ˆå› å­1 vs å› å­2ï¼‰
            if n_factors >= 2:
                ax5 = plt.subplot(2, 3, 5)
                plt.scatter(loadings[:, 0], loadings[:, 1], s=100, alpha=0.7)

                # å¤‰æ•°åã‚’ãƒ—ãƒ­ãƒƒãƒˆ
                for i, var in enumerate(feature_names):
                    plt.annotate(
                        var,
                        (loadings[i, 0], loadings[i, 1]),
                        xytext=(5, 5),
                        textcoords="offset points",
                        fontsize=9,
                        ha="center",
                        va="center",
                        path_effects=[pe.withStroke(linewidth=2, foreground="white")],
                    )

                plt.xlabel(f"å› å­1è² è·é‡ ({explained_variance[0]:.1f}%)")
                plt.ylabel(f"å› å­2è² è·é‡ ({explained_variance[1]:.1f}%)")
                plt.title("å› å­è² è·é‡ãƒ—ãƒ­ãƒƒãƒˆ")
                plt.axhline(y=0, color="k", linestyle="-", alpha=0.3)
                plt.axvline(x=0, color="k", linestyle="-", alpha=0.3)
                plt.grid(True, alpha=0.3)

            # 6. ç´¯ç©å¯„ä¸ç‡ã®ãƒ—ãƒ­ãƒƒãƒˆ
            ax6 = plt.subplot(2, 3, 6)
            x_factors = range(1, len(explained_variance) + 1)

            plt.bar(
                x_factors, explained_variance, alpha=0.7, label="å€‹åˆ¥", color="skyblue"
            )
            plt.plot(
                x_factors,
                cumulative_variance,
                "ro-",
                linewidth=2,
                markersize=8,
                label="ç´¯ç©",
            )

            plt.xlabel("å› å­ç•ªå·")
            plt.ylabel("èª¬æ˜åˆ†æ•£ (%)")
            plt.title("å› å­åˆ¥èª¬æ˜åˆ†æ•£")
            plt.legend()
            plt.grid(True, alpha=0.3)

            # å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«
            assumptions = results["assumptions"]
            kmo_score = assumptions.get("kmo_model", 0)
            bartlett_p = assumptions.get("bartlett_p_value", 1)
            method = results.get("method", "unknown")

            fig.suptitle(
                f"å› å­åˆ†æçµæœ\n"
                f"KMOé©åˆåº¦: {kmo_score:.3f}, Bartlettæ¤œå®š på€¤: {bartlett_p:.2e}, "
                f'{n_factors}å› å­, å›è»¢: {results["rotation"]} ({method})',
                fontsize=14,
                y=0.98,
            )

            plt.tight_layout()
            plt.subplots_adjust(top=0.92)

            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            plot_base64 = self.save_plot_as_base64(fig)
            print(f"ãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
            return plot_base64

        except Exception as e:
            print(f"ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return ""

    def create_response(
        self,
        results: Dict[str, Any],
        df: pd.DataFrame,
        session_id: int,
        session_name: str,
        file,
        plot_base64: str,
    ) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            factor_scores = results["factor_scores"]
            loadings = results["loadings"]

            # å› å­å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆ
            factor_scores_data = []
            for i, (sample_name, scores) in enumerate(zip(df.index, factor_scores)):
                if isinstance(scores, (list, np.ndarray)) and len(scores) > 0:
                    factor_scores_data.append(
                        {
                            "name": str(sample_name),
                            "sample_name": str(sample_name),
                            "factor_1": float(scores[0]) if len(scores) > 0 else 0.0,
                            "factor_2": float(scores[1]) if len(scores) > 1 else 0.0,
                            "factor_3": float(scores[2]) if len(scores) > 2 else 0.0,
                            "dimension_1": float(scores[0]) if len(scores) > 0 else 0.0,
                            "dimension_2": float(scores[1]) if len(scores) > 1 else 0.0,
                            "dimension_3": float(scores[2]) if len(scores) > 2 else 0.0,
                        }
                    )

            # å› å­è² è·é‡ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã§ä½œæˆ
            factor_loadings_data = []
            for i, (feature_name, loading_vals) in enumerate(zip(df.columns, loadings)):
                if (
                    isinstance(loading_vals, (list, np.ndarray))
                    and len(loading_vals) > 0
                ):
                    factor_loadings_data.append(
                        {
                            "name": str(feature_name),
                            "variable_name": str(feature_name),
                            "factor_1": (
                                float(loading_vals[0]) if len(loading_vals) > 0 else 0.0
                            ),
                            "factor_2": (
                                float(loading_vals[1]) if len(loading_vals) > 1 else 0.0
                            ),
                            "factor_3": (
                                float(loading_vals[2]) if len(loading_vals) > 2 else 0.0
                            ),
                            "dimension_1": (
                                float(loading_vals[0]) if len(loading_vals) > 0 else 0.0
                            ),
                            "dimension_2": (
                                float(loading_vals[1]) if len(loading_vals) > 1 else 0.0
                            ),
                            "dimension_3": (
                                float(loading_vals[2]) if len(loading_vals) > 2 else 0.0
                            ),
                        }
                    )

            return {
                "success": True,
                "session_id": session_id,
                "analysis_type": self.get_analysis_type(),
                "data": {
                    "n_factors": results["n_factors"],
                    "rotation": results["rotation"],
                    "standardized": results["standardized"],
                    "method": results["method"],
                    # å¾“æ¥å½¢å¼ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
                    "loadings": loadings,
                    "communalities": results["communalities"],
                    "uniquenesses": results["uniquenesses"],
                    "eigenvalues": results["eigenvalues"],
                    "explained_variance": results["explained_variance"],
                    "cumulative_variance": results["cumulative_variance"],
                    "factor_scores": factor_scores,
                    # æ–°å½¢å¼ï¼ˆåº§æ¨™ãƒ‡ãƒ¼ã‚¿ï¼‰
                    "factor_scores_data": factor_scores_data,
                    "factor_loadings_data": factor_loadings_data,
                    # å› å­è² è·é‡è¡Œåˆ—ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ï¼‰
                    "loadings_matrix": loadings,
                    # å¤‰æ•°ãƒ»ã‚µãƒ³ãƒ—ãƒ«å
                    "feature_names": results["feature_names"],
                    "sample_names": results["sample_names"],
                    # å‰ææ¡ä»¶
                    "assumptions": results["assumptions"],
                    # ãƒ—ãƒ­ãƒƒãƒˆç”»åƒ
                    "plot_image": plot_base64,
                    # åº§æ¨™å½¢å¼ï¼ˆå¾“æ¥äº’æ›ï¼‰
                    "coordinates": {
                        "samples": [
                            {
                                "name": str(name),
                                "dimension_1": (
                                    float(factor_scores[i][0])
                                    if len(factor_scores[i]) > 0
                                    else 0.0
                                ),
                                "dimension_2": (
                                    float(factor_scores[i][1])
                                    if len(factor_scores[i]) > 1
                                    else 0.0
                                ),
                            }
                            for i, name in enumerate(df.index)
                        ],
                        "variables": [
                            {
                                "name": str(name),
                                "dimension_1": (
                                    float(loadings[i][0])
                                    if len(loadings[i]) > 0
                                    else 0.0
                                ),
                                "dimension_2": (
                                    float(loadings[i][1])
                                    if len(loadings[i]) > 1
                                    else 0.0
                                ),
                                "communality": float(results["communalities"][i]),
                            }
                            for i, name in enumerate(df.columns)
                        ],
                    },
                },
                "metadata": {
                    "session_name": session_name,
                    "filename": file.filename,
                    "rows": df.shape[0],
                    "columns": df.shape[1],
                    "feature_names": results["feature_names"],
                    "sample_names": results["sample_names"],
                },
            }

        except Exception as e:
            print(f"âŒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            raise

        # FactorAnalysisAnalyzer ã‚¯ãƒ©ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰

    def get_session_detail(self, db: Session, session_id: int) -> Dict[str, Any]:
        """å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’å–å¾—ï¼ˆåº§æ¨™ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰"""
        try:
            print(f"ğŸ“Š å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

            # åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if hasattr(super(), "get_session_detail"):
                try:
                    base_detail = super().get_session_detail(db, session_id)
                except Exception as e:
                    print(f"âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã®get_session_detailã‚¨ãƒ©ãƒ¼: {e}")
                    base_detail = self._get_session_detail_directly(db, session_id)
            else:
                print("âš ï¸ åŸºåº•ã‚¯ãƒ©ã‚¹ã«get_session_detailãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                base_detail = self._get_session_detail_directly(db, session_id)

            if not base_detail or not base_detail.get("success"):
                return base_detail

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            coordinates_data = self._get_coordinates_data(db, session_id)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’æ§‹ç¯‰
            response_data = {
                "success": True,
                "data": {
                    "session_info": base_detail["data"]["session_info"],
                    "metadata": {
                        "filename": base_detail["data"]["metadata"]["filename"],
                        "rows": base_detail["data"]["metadata"]["rows"],
                        "columns": base_detail["data"]["metadata"]["columns"],
                        "sample_names": coordinates_data.get("sample_names", []),
                        "feature_names": coordinates_data.get("feature_names", []),
                    },
                    "analysis_data": {
                        "factor_scores": coordinates_data.get("factor_scores", []),
                        "factor_loadings": coordinates_data.get("factor_loadings", []),
                    },
                    "visualization": base_detail["data"].get("visualization", {}),
                },
            }

            print(f"âœ… å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—å®Œäº†")
            return response_data

        except Exception as e:
            print(f"âŒ å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return {"success": False, "error": str(e)}

    def _get_coordinates_data(self, db: Session, session_id: int) -> Dict[str, Any]:
        """åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            from models import CoordinatesData

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            coordinates = (
                db.query(CoordinatesData)
                .filter(CoordinatesData.session_id == session_id)
                .all()
            )

            # å› å­å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆè¦³æ¸¬å€¤ï¼‰
            factor_scores = []
            sample_names = []

            # å› å­è² è·é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰æ•°ï¼‰
            factor_loadings = []
            feature_names = []

            for coord in coordinates:
                if coord.point_type == "observation":
                    factor_scores.append(
                        {
                            "name": coord.point_name,
                            "dimension_1": coord.dimension_1 or 0.0,
                            "dimension_2": coord.dimension_2 or 0.0,
                            "dimension_3": coord.dimension_3 or 0.0,
                            "dimension_4": coord.dimension_4 or 0.0,
                            "order_index": len(factor_scores),
                        }
                    )
                    sample_names.append(coord.point_name)

                elif coord.point_type == "variable":
                    factor_loadings.append(
                        {
                            "name": coord.point_name,
                            "dimension_1": coord.dimension_1 or 0.0,
                            "dimension_2": coord.dimension_2 or 0.0,
                            "dimension_3": coord.dimension_3 or 0.0,
                            "dimension_4": coord.dimension_4 or 0.0,
                            "order_index": len(factor_loadings),
                        }
                    )
                    feature_names.append(coord.point_name)

            print(f"ğŸ” åº§æ¨™ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ:")
            print(f"  - å› å­å¾—ç‚¹: {len(factor_scores)}ä»¶")
            print(f"  - å› å­è² è·é‡: {len(factor_loadings)}ä»¶")

            return {
                "factor_scores": factor_scores,
                "factor_loadings": factor_loadings,
                "sample_names": sample_names,
                "feature_names": feature_names,
            }

        except Exception as e:
            print(f"âŒ åº§æ¨™ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "factor_scores": [],
                "factor_loadings": [],
                "sample_names": [],
                "feature_names": [],
            }

    def _get_factor_metadata(self, db: Session, session_id: int) -> Dict[str, Any]:
        """å› å­åˆ†æç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            from models import AnalysisMetadata

            # å› å­è² è·é‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            factor_loadings_meta = (
                db.query(AnalysisMetadata)
                .filter(
                    AnalysisMetadata.session_id == session_id,
                    AnalysisMetadata.metadata_type == "factor_loadings",
                )
                .first()
            )

            # å‰ææ¡ä»¶ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            assumptions_meta = (
                db.query(AnalysisMetadata)
                .filter(
                    AnalysisMetadata.session_id == session_id,
                    AnalysisMetadata.metadata_type == "factor_assumptions",
                )
                .first()
            )

            metadata = {}

            if factor_loadings_meta:
                metadata["factor_loadings"] = factor_loadings_meta.metadata_content

            if assumptions_meta:
                metadata["assumptions"] = assumptions_meta.metadata_content

            return metadata

        except Exception as e:
            print(f"âŒ å› å­åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}

    async def _get_session_detail_directly(self, db: Session, session_id: int):
        """å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°ã‚’ç›´æ¥å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            from models import AnalysisSession, VisualizationData, CoordinatesData

            print(f"ğŸ“Š å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—é–‹å§‹: {session_id}")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±ã‚’å–å¾—
            session = (
                db.query(AnalysisSession)
                .filter(AnalysisSession.id == session_id)
                .first()
            )

            if not session:
                return {
                    "success": False,
                    "error": f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                }

            print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³åŸºæœ¬æƒ…å ±å–å¾—: {session.session_name}")

            # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            visualization = (
                db.query(VisualizationData)
                .filter(VisualizationData.session_id == session_id)
                .first()
            )

            # åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            coordinates = (
                db.query(CoordinatesData)
                .filter(CoordinatesData.session_id == session_id)
                .all()
            )

            print(
                f"ğŸ” ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ: åº§æ¨™={len(coordinates)}ä»¶, å¯è¦–åŒ–={'ã‚ã‚Š' if visualization else 'ãªã—'}"
            )

            # å› å­å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆobservationï¼‰ã¨å› å­è² è·é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆvariableï¼‰
            factor_scores = []
            factor_loadings = []
            sample_names = []
            feature_names = []

            for coord in coordinates:
                coord_data = {
                    "name": coord.point_name,
                    "point_name": coord.point_name,
                    "dimension_1": (
                        float(coord.dimension_1)
                        if coord.dimension_1 is not None
                        else 0.0
                    ),
                    "dimension_2": (
                        float(coord.dimension_2)
                        if coord.dimension_2 is not None
                        else 0.0
                    ),
                    "dimension_3": (
                        float(coord.dimension_3)
                        if coord.dimension_3 is not None
                        else 0.0
                    ),
                    "dimension_4": (
                        float(coord.dimension_4)
                        if coord.dimension_4 is not None
                        else 0.0
                    ),
                    "factor_1": (
                        float(coord.dimension_1)
                        if coord.dimension_1 is not None
                        else 0.0
                    ),
                    "factor_2": (
                        float(coord.dimension_2)
                        if coord.dimension_2 is not None
                        else 0.0
                    ),
                    "factor_3": (
                        float(coord.dimension_3)
                        if coord.dimension_3 is not None
                        else 0.0
                    ),
                }

                if coord.point_type == "observation":
                    coord_data["sample_name"] = coord.point_name
                    coord_data["order_index"] = len(factor_scores)
                    factor_scores.append(coord_data)
                    sample_names.append(coord.point_name)

                elif coord.point_type == "variable":
                    coord_data["variable_name"] = coord.point_name
                    coord_data["order_index"] = len(factor_loadings)
                    factor_loadings.append(coord_data)
                    feature_names.append(coord.point_name)

            print(f"ğŸ“Š åº§æ¨™ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ:")
            print(f"  - å› å­å¾—ç‚¹: {len(factor_scores)}ä»¶")
            print(f"  - å› å­è² è·é‡: {len(factor_loadings)}ä»¶")

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            from models import AnalysisMetadata, EigenvalueData

            # å›ºæœ‰å€¤ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            eigenvalue_data = (
                db.query(EigenvalueData)
                .filter(EigenvalueData.session_id == session_id)
                .order_by(EigenvalueData.dimension_number)
                .all()
            )

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—
            metadata_entries = (
                db.query(AnalysisMetadata)
                .filter(AnalysisMetadata.session_id == session_id)
                .all()
            )

            print(
                f"ğŸ” ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—: å›ºæœ‰å€¤={len(eigenvalue_data)}ä»¶, ãã®ä»–={len(metadata_entries)}ä»¶"
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’æ§‹ç¯‰
            response_data = {
                "success": True,
                "data": {
                    "session_info": {
                        "session_id": session.id,
                        "session_name": session.session_name,
                        "description": getattr(session, "description", "") or "",
                        "filename": session.original_filename,
                        "row_count": getattr(session, "row_count", len(factor_scores))
                        or len(factor_scores),
                        "column_count": getattr(
                            session, "column_count", len(factor_loadings)
                        )
                        or len(factor_loadings),
                        "dimensions_count": getattr(session, "dimensions_count", 2)
                        or 2,
                        "dimension_1_contribution": (
                            float(getattr(session, "dimension_1_contribution", 0))
                            if getattr(session, "dimension_1_contribution", None)
                            else 50.0
                        ),
                        "dimension_2_contribution": (
                            float(getattr(session, "dimension_2_contribution", 0))
                            if getattr(session, "dimension_2_contribution", None)
                            else 30.0
                        ),
                        "rotation": getattr(session, "rotation", "varimax")
                        or "varimax",
                        "standardized": getattr(session, "standardized", True),
                        "analysis_timestamp": (
                            session.analysis_timestamp.isoformat()
                            if hasattr(session, "analysis_timestamp")
                            and session.analysis_timestamp
                            else None
                        ),
                    },
                    "metadata": {
                        "filename": session.original_filename,
                        "rows": getattr(session, "row_count", len(factor_scores))
                        or len(factor_scores),
                        "columns": getattr(
                            session, "column_count", len(factor_loadings)
                        )
                        or len(factor_loadings),
                        "sample_names": sample_names,
                        "feature_names": feature_names,
                    },
                    "analysis_data": {
                        "factor_scores": factor_scores,
                        "factor_loadings": factor_loadings,
                    },
                    "coordinates_data": factor_scores
                    + factor_loadings,  # çµ±åˆã•ã‚ŒãŸåº§æ¨™ãƒ‡ãƒ¼ã‚¿
                    "eigenvalue_data": [
                        {
                            "dimension_number": ev.dimension_number,
                            "eigenvalue": (
                                float(ev.eigenvalue) if ev.eigenvalue else 0.0
                            ),
                            "explained_inertia": (
                                float(ev.explained_inertia)
                                if ev.explained_inertia
                                else 0.0
                            ),
                            "cumulative_inertia": (
                                float(ev.cumulative_inertia)
                                if ev.cumulative_inertia
                                else 0.0
                            ),
                        }
                        for ev in eigenvalue_data
                    ],
                    "metadata_entries": [
                        {
                            "metadata_type": meta.metadata_type,
                            "metadata_content": meta.metadata_content,
                        }
                        for meta in metadata_entries
                    ],
                    "visualization": {
                        "plot_image": (
                            visualization.image_base64 if visualization else None
                        ),
                        "width": visualization.width if visualization else 1400,
                        "height": visualization.height if visualization else 1100,
                    },
                },
            }

            print(f"âœ… å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—å®Œäº†")
            print(f"ğŸ“Š è¿”å´ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèªå®Œäº†")

            return response_data

        except Exception as e:
            print(f"âŒ å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback

            print(f"è©³ç´°:\n{traceback.format_exc()}")
            return {"success": False, "error": str(e)}

    async def get_session_detail(self, session_id: int, db: Session):
        """å› å­åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°å–å¾—ã®ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ¡ã‚½ãƒƒãƒ‰"""
        return await self._get_session_detail_directly(db, session_id)
